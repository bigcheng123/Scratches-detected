Index: streams.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>0\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/streams.txt b/streams.txt
--- a/streams.txt	(revision 38763aecd12af542eecef5d90e69df6fc12d9b25)
+++ b/streams.txt	(date 1708316090131)
@@ -1,3 +1,2 @@
 0
-
-
+1
\ No newline at end of file
Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from PyQt5.QtWidgets import QApplication, QMainWindow, QFileDialog, QMenu, QAction\r\nfrom ui_files.main_win import Ui_mainWindow\r\nfrom ui_files.dialog.rtsp_win import Window\r\n\r\nfrom PyQt5.QtCore import Qt, QPoint, QTimer, QThread, pyqtSignal\r\nfrom PyQt5.QtGui import QImage, QPixmap, QPainter, QIcon\r\nfrom pathlib import Path\r\nimport sys\r\nimport json\r\nimport numpy as np\r\nimport torch\r\nimport torch.backends.cudnn as cudnn\r\nimport os\r\nimport time\r\nimport cv2\r\nimport modbus_rtu\r\nimport _thread\r\n\r\nfrom shutil import copy\r\n\r\nfrom models.experimental import attempt_load\r\nfrom utils.datasets import LoadImages, LoadWebcam, LoadStreams\r\nfrom utils.CustomMessageBox import MessageBox\r\nfrom utils.general import check_img_size, check_requirements, check_imshow, colorstr, non_max_suppression, \\\r\n    apply_classifier, scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\r\n# from utils.plots import colors, plot_one_box, plot_one_box_PIL\r\nfrom utils.plots import Annotator, colors, save_one_box,plot_one_box\r\n\r\nfrom utils.torch_utils import select_device,time_sync,load_classifier\r\nfrom utils.capnums import Camera\r\n\r\n## 设置全局变量\r\nmodbus_flag = False\r\n\r\n\r\nclass DetThread(QThread): ###继承 QThread\r\n    send_img_ch0 = pyqtSignal(np.ndarray)  ### CH0 output image\r\n    send_img_ch1 = pyqtSignal(np.ndarray)  ### CH1 output image\r\n    send_img_ch2 = pyqtSignal(np.ndarray)  ### CH2 output image\r\n    send_img_ch3 = pyqtSignal(np.ndarray)  ### CH3 output image\r\n    send_img_ch4 = pyqtSignal(np.ndarray)  ### CH4 output image\r\n    send_img_ch5 = pyqtSignal(np.ndarray)  ### CH5 output image\r\n    send_statistic = pyqtSignal(dict)  ###\r\n    # emit：detecting/pause/stop/finished/error msg\r\n    send_msg = pyqtSignal(str)\r\n    send_percent = pyqtSignal(int)\r\n    send_fps = pyqtSignal(str)\r\n\r\n    def __init__(self):\r\n        super(DetThread, self).__init__()\r\n        self.weights = './yolov5s.pt'\r\n        self.current_weight = './yolov5s.pt'\r\n        self.source = '0'\r\n        self.device = '0'\r\n        self.conf_thres = 0.25\r\n        self.iou_thres = 0.45\r\n        self.jump_out = False                   # jump out of the loop\r\n        self.is_continue = False                # continue/pause\r\n        self.percent_length = 1000              # progress bar\r\n        self.rate_check = True                  # Whether to enable delay\r\n        self.rate = 100\r\n        self.save_fold = None  ####'./auto_save/mp4'\r\n\r\n    @torch.no_grad()\r\n    def run(self,\r\n            imgsz=320, #1440 # inference size (pixels)//推理大小\r\n            max_det=50,  # maximum detections per image//每个图像的最大检测次数\r\n            # self.source = '0'\r\n            # self.device='0',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\r\n            view_img=False,  # show results\r\n            save_txt=False,  # save results to *.txt\r\n            save_conf=False,  # save confidences in --save-txt labels\r\n            save_crop=False,  # save cropped prediction boxes\r\n            nosave=False,  # do not save images/videos\r\n            classes=None,  # filter by class: --class 0, or --class 0 2 3\r\n            agnostic_nms=False,  # class-agnostic NMS\r\n            augment=False,  # augmented inference\r\n            visualize=False,  # visualize features\r\n            update=False,  # update all models\r\n            project='runs/detect',  # save results to project/name\r\n            name='exp',  # save results to project/name\r\n            exist_ok=False,  # existing project/name ok, do not increment\r\n            line_thickness=3,  # bounding box thickness (pixels)//边界框厚度\r\n            hide_labels=False,  # hide labels\r\n            hide_conf=False,  # hide confidences\r\n            half=False,  # use FP16 half-precision inference\r\n            ):\r\n\r\n        save_img = not nosave and not self.source.endswith('.txt')  # save inference images\r\n        webcam = self.source.isnumeric() or self.source.endswith('.txt') or self.source.lower().startswith(\r\n            ('rtsp://', 'rtmp://', 'http://', 'https://'))\r\n\r\n        # Directories\r\n        save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\r\n        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\r\n\r\n        # Initialize\r\n        # try:\r\n        set_logging()\r\n        device = select_device(self.device)  ### from utils.torch_utils import select_device\r\n        half &= device.type != '0'  #'cpu'# half precision only supported on CUDA\r\n\r\n        # Load model\r\n        model = attempt_load(self.weights, map_location=device)  # load FP32 model  from models.experimental import attempt_load\r\n        stride = int(model.stride.max())  # model stride\r\n        imgsz = check_img_size(imgsz, s=stride)  # check image size\r\n        names = model.module.names if hasattr(model, 'module') else model.names  # get class names\r\n        if half:\r\n            model.half()  # to FP16\r\n\r\n        # Second-stage classifier\r\n        classify = False\r\n        if classify:\r\n            modelc = load_classifier(name='resnet50', n=2)  # initialize\r\n            modelc.load_state_dict(torch.load('resnet50.pt', map_location=device)['model']).to(device).eval()\r\n\r\n        # Dataloader\r\n        if webcam: ###self.source.isnumeric() or self.source.endswith('.txt') or\r\n            view_img = check_imshow()\r\n            cudnn.benchmark = True  # set True to speed up constant image size inference\r\n            dataset = LoadStreams(self.source, img_size=imgsz, stride=stride)  #### loadstreams  return self.sources, img, img0, None\r\n            print('dataset type', type(dataset), dataset)\r\n            bs = len(dataset)  # batch_size\r\n            print('len(bs)=', bs)\r\n            # #### streams = LoadStreams\r\n\r\n        else:  ### load the images\r\n            dataset = LoadImages(self.source, img_size=imgsz, stride=stride)\r\n            bs = 1  # batch_size\r\n        vid_path, vid_writer = [None] * bs, [None] * bs\r\n\r\n\r\n        # Run inference 推理\r\n        if device.type != '0':#'cpu'\r\n            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\r\n        start_time = time.time()\r\n        t0 = time.time()\r\n        count = 0\r\n\r\n        # dataset = iter(dataset)  ##迭代器 iter 创建了一个迭代器对象，每次调用这个迭代器对象的__next__()方法时，都会调用 object\r\n\r\n        while True: ##### 采用循环来 检查是否 停止推理\r\n            print('marker while loop')\r\n            print(' while loop self.is_continue', self.is_continue)\r\n            print(' while loop self.jump_out', self.jump_out)\r\n            # print(' while loop camera.cap', type(self.vid_cap))\r\n\r\n            if self.jump_out:\r\n                self.vid_cap.release()  #### bug-2  无法释放摄像头  未解决\r\n                print('vid_cap.release -1', type(self.vid_cap))\r\n                self.send_percent.emit(0)\r\n                self.send_msg.emit('Stop')\r\n                if hasattr(self, 'out'):\r\n                    self.out.release()\r\n                print('jump_out push-1', self.jump_out)\r\n                break\r\n\r\n            # change model & device  20230810\r\n            if self.current_weight != self.weights:\r\n                # Load model\r\n                model = attempt_load(self.weights, map_location = device)  # load FP32 model\r\n                stride = int(model.stride.max())  # model stride\r\n                imgsz = check_img_size(imgsz, s=stride)  # check image size\r\n                names = model.module.names if hasattr(model, 'module') else model.names  # get class names\r\n                if half:\r\n                    model.half()  # to FP16\r\n                # Run inference\r\n                if device.type != '0':#'cpu'\r\n                    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\r\n                self.current_weight = self.weights\r\n\r\n            # load  streams\r\n\r\n            if self.is_continue:\r\n                # ### 使用 loadstreams  dataset = ： self.sources, img, img0, None\r\n                for path, img, im0s, self.vid_cap in dataset: ####  由于dataset在RUN中运行 会不断更新，所以此FOR循环 不会穷尽\r\n                    # print(type(path), type(img), type(im0s), type(self.vid_cap))\r\n                    ### show row image\r\n                    # cv2.imshow('ch0', im0s[0])\r\n                    # cv2.imshow('ch1', im0s[1])\r\n                    #### img recode\r\n                    img = torch.from_numpy(img).to(device)\r\n                    img = img.half() if half else img.float()  # uint8 to fp16/32\r\n                    img /= 255.0  # 0 - 255 to 0.0 - 1.0\r\n                    if img.ndimension() == 3:\r\n                        img = img.unsqueeze(0)\r\n                    statistic_dic = {name: 0 for name in names} ### made the diction\r\n                    # print('statisstic_dic-1',statistic_dic)\r\n                    count += 1  #### FSP counter\r\n                    if  count % 30 == 0 and count >= 30:\r\n                        fps = int(30/(time.time()-start_time))\r\n                        self.send_fps.emit('fps：'+str(fps))\r\n                        start_time = time.time()\r\n                    if self.vid_cap:\r\n                        percent = int(count/self.vid_cap.get(cv2.CAP_PROP_FRAME_COUNT)*self.percent_length)\r\n                        self.send_percent.emit(percent)\r\n                    else:\r\n                        percent = self.percent_length\r\n\r\n                    # Inference\r\n                    t1 = time_sync()\r\n                    # pred = model(img, augment=augment)[0] #### 预测  使用loadWebcam是 加载的model\r\n                    pred = model(img,\r\n                                 augment=augment,\r\n                                 visualize=increment_path(save_dir / Path(path).stem,\r\n                                                          mkdir=True) if visualize else False)[0]\r\n\r\n                    # Apply NMS\r\n                    pred = non_max_suppression(pred, self.conf_thres, self.iou_thres, classes, agnostic_nms, max_det=max_det)\r\n                    t2 = time_sync()\r\n\r\n                    # Apply Classifier\r\n                    if classify:\r\n                        pred = apply_classifier(pred, modelc, img, im0s)\r\n\r\n                    # Process detections\r\n                    for i, det in enumerate(pred):  # detections per image\r\n                        if webcam:  # batch_size >= 1     get the frame\r\n                            p, s, im0, frame = path[i], f'{i}: ', im0s[i].copy(), dataset.count\r\n                            label_chanel = str(i)\r\n                            # print(type(label_chanel),'img chanel=', label_chanel)\r\n                        else: ### image\r\n                            p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\r\n                        p = Path(p)  # to Path\r\n                        # save_path = str(save_dir / p.name)  # img.jpg\r\n                        # txt_path = str(save_dir / 'labels' / p.stem) + (dtxt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  #\r\n                        txt_path = 'result'\r\n                        s += '%gx%g ' % img.shape[2:]  # print string\r\n                        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\r\n                        imc = im0.copy() if save_crop else im0  # for save_crop\r\n                        if len(det):\r\n                            # Rescale boxes from img_size to im0 size\r\n                            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\r\n\r\n                            # Print results\r\n                            for c in det[:, -1].unique():\r\n                                n = (det[:, -1] == c).sum()  # detections per class\r\n                                s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\r\n                            # Write results\r\n                            for *xyxy, conf, cls in reversed(det):\r\n                                if save_txt:  # Write to file\r\n                                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(\r\n                                        -1).tolist()  # normalized xywh\r\n                                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\r\n                                    with open(txt_path + '.txt', 'a') as f:\r\n                                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\r\n\r\n                                if save_img or save_crop or view_img:  # Add bbox to image\r\n                                    c = int(cls)  # integer class\r\n                                    statistic_dic[names[c]] += 1\r\n                                    # print('statisstic_dic-2',statistic_dic)\r\n                                    label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\r\n                                    plot_one_box(xyxy, im0, label=label, color=colors(c, True),\r\n                                                 line_thickness=line_thickness)\r\n                                    if save_crop:\r\n                                        print('save_one_box')\r\n                            # print('detection is running')\r\n\r\n                        FSP = int(1 / (t2 - t1))\r\n                        # print(f'{s}Done. ({t2 - t1:.3f}s FSP={FSP})')\r\n\r\n                        # Stream results   emit frame\r\n                        if self.is_continue: ###### 发送图片必须在  for i, det in enumerate(pred): 循环内\r\n                        # if view_img:\r\n                            cv2.putText(im0, str(f'FSP = {FSP}  CAM = {i}'), (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)\r\n                            res = cv2.resize(im0, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\r\n                            ## chanel-0  ##### show images\r\n                            if label_chanel == '0':\r\n                                self.send_img_ch0.emit(im0)  ### 发送图像\r\n                                # print('seng img : ch0')\r\n                            ## chanel-1\r\n                            if label_chanel == '1':\r\n                                self.send_img_ch1.emit(im0)  ### 发送图像\r\n                                # print('seng img : ch1')\r\n                            # chanel-2\r\n                            if label_chanel == '2':\r\n                                self.send_img_ch2.emit(im0)  ### 发送图像\r\n                                # print('seng img : ch2')\r\n                            ## chanel-3\r\n                            if label_chanel == '3':\r\n                                self.send_img_ch3.emit(im0)  #### 发送图像\r\n                                # print('seng img : ch3')\r\n                            ## chanel-4\r\n                            if label_chanel == '4':\r\n                                 self.send_img_ch4.emit(im0)  #### 发送图像\r\n                                 # print('seng img : ch4')\r\n                            ## chanel-5\r\n                            if label_chanel == '5':\r\n                                 self.send_img_ch5.emit(im0)  #### 发送图像\r\n                                 # print('seng img : ch5')\r\n                            ### ## send the detected result\r\n                            self.send_statistic.emit(statistic_dic)\r\n                            # print('emit statistic_dic', statistic_dic)\r\n                    if self.rate_check:\r\n                        time.sleep(1/self.rate)\r\n                    # im0 = annotator.result()\r\n                    # Write results\r\n                    if self.save_fold: #### when autosave is  true\r\n                        os.makedirs(self.save_fold, exist_ok=True)\r\n                        if self.vid_cap is None: ####save as .jpg\r\n                            save_path = os.path.join(self.save_fold,\r\n                                                     time.strftime('%Y_%m_%d_%H_%M_%S',\r\n                                                                   time.localtime()) + '.jpg')\r\n                            cv2.imwrite(save_path, im0)\r\n                            print(str(f'save as .jpg  CAM = {i}'))#& str(save_path))\r\n                        else: ### self.vid_cap is cv2capture save as .mp4\r\n                            if count == 1:\r\n                                ori_fps = int(self.vid_cap.get(cv2.CAP_PROP_FPS))\r\n                                if ori_fps == 0:\r\n                                    ori_fps = 25\r\n                                # width = int(self.vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n                                # height = int(self.vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n                                width, height = im0.shape[1], im0.shape[0]\r\n                                save_path = os.path.join(self.save_fold, time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime()) + '.mp4')\r\n                                self.out = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), ori_fps,\r\n                                                           (width, height))\r\n                            self.out.write(im0)\r\n                            print( str(f'save as .mp4  CAM = {i}')) # & str(save_path))\r\n                    if self.jump_out:\r\n                        print('jump_out push-2', self.jump_out)\r\n                        self.is_continue = False\r\n                        self.vid_cap.release()  #### bug-2  无法释放摄像头  未解决\r\n                        print('self.vid_cap.release-2', type(self.vid_cap))\r\n                        self.send_percent.emit(0)\r\n                        self.send_msg.emit('Stop')\r\n                        if hasattr(self, 'out'):\r\n                            self.out.release()\r\n                            print('self.out.release')\r\n                        break\r\n\r\n                if percent == self.percent_length:\r\n                    print(count)\r\n                    self.send_percent.emit(0)\r\n                    self.send_msg.emit('finished')\r\n                    if hasattr(self, 'out'):\r\n                        self.out.release()\r\n                    break\r\n            else:\r\n                print('is_continue break', self.is_continue)\r\n        #### 生成结果文件夹\r\n        # if save_txt or save_img:\r\n        #     s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\r\n        #     print(f\"Results saved to {save_dir}{s}\")\r\n\r\n        if update:\r\n            strip_optimizer(self.weights)  # update model (to fix SourceChangeWarning)\r\n\r\n        # except Exception as e:\r\n        #     self.send_msg.emit('%s' % e)\r\n\r\n\r\n\r\nclass MainWindow(QMainWindow, Ui_mainWindow):\r\n    def __init__(self, parent=None):\r\n        super(MainWindow, self).__init__(parent)\r\n        self.setupUi(self)\r\n        self.m_flag = False\r\n\r\n        # style 1: window can be stretched\r\n        # self.setWindowFlags(Qt.CustomizeWindowHint | Qt.WindowStaysOnTopHint)\r\n\r\n        # style 2: window can not be stretched\r\n        self.setWindowFlags(Qt.Window | Qt.FramelessWindowHint\r\n                            | Qt.WindowSystemMenuHint | Qt.WindowMinimizeButtonHint | Qt.WindowMaximizeButtonHint)\r\n        # self.setWindowOpacity(0.85)  # Transparency of window\r\n\r\n        self.minButton.clicked.connect(self.showMinimized)\r\n        self.maxButton.clicked.connect(self.max_or_restore)\r\n        # show Maximized window\r\n        self.maxButton.animateClick(10)\r\n        self.closeButton.clicked.connect(self.close)\r\n\r\n        self.qtimer = QTimer(self)\r\n        self.qtimer.setSingleShot(True)\r\n        self.qtimer.timeout.connect(lambda: self.statistic_label.clear())\r\n\r\n        # search models automatically\r\n        self.comboBox.clear()\r\n        self.pt_list = os.listdir('./pt')\r\n        self.pt_list = [file for file in self.pt_list if file.endswith('.pt')]\r\n        self.pt_list.sort(key=lambda x: os.path.getsize('./pt/'+x))\r\n        self.comboBox.clear()\r\n        self.comboBox.addItems(self.pt_list)\r\n\r\n        self.qtimer_search = QTimer(self)\r\n        self.qtimer_search.timeout.connect(lambda: self.search_pt())\r\n        self.qtimer_search.start(2000)\r\n\r\n        # yolov5 thread\r\n        self.det_thread = DetThread()\r\n        self.model_type = self.comboBox.currentText()  ### get model from combobox\r\n        self.device_type = self.comboBox_device.currentText()  ###  get device type from combobox\r\n        self.source_type = self.comboBox_source.currentText()  ###  get device type from combobox\r\n        self.port_type = self.comboBox_port.currentText() ###  get port type from combobox\r\n        self.det_thread.weights = \"./pt/%s\" % self.model_type  # difined\r\n        self.det_thread.device = self.device_type # difined  device\r\n        self.det_thread.source = self.source_type\r\n        self.det_thread.percent_length = self.progressBar.maximum()\r\n        #### the connect funtion transform to  def run_or_continue(self):\r\n        self.det_thread.send_img_ch0.connect(lambda x: self.show_image(x, self.video_label_ch0))\r\n        self.det_thread.send_img_ch1.connect(lambda x: self.show_image(x, self.video_label_ch1))\r\n        self.det_thread.send_img_ch2.connect(lambda x: self.show_image(x, self.video_label_ch2))\r\n        self.det_thread.send_img_ch3.connect(lambda x: self.show_image(x, self.video_label_ch3))\r\n        self.det_thread.send_img_ch4.connect(lambda x: self.show_image(x, self.video_label_ch11))\r\n        self.det_thread.send_img_ch5.connect(lambda x: self.show_image(x, self.video_label_ch12))\r\n        #### tab-2\r\n        self.det_thread.send_img_ch0.connect(lambda x: self.show_image(x, self.video_label_ch4))\r\n        #### tab-3\r\n        self.det_thread.send_img_ch1.connect(lambda x: self.show_image(x, self.video_label_ch5))\r\n        #### tab-4\r\n        self.det_thread.send_img_ch2.connect(lambda x: self.show_image(x, self.video_label_ch6))\r\n        #### tab-5\r\n        self.det_thread.send_img_ch3.connect(lambda x: self.show_image(x, self.video_label_ch7))\r\n        #### tab-6\r\n        self.det_thread.send_img_ch4.connect(lambda x: self.show_image(x, self.video_label_ch8))\r\n        #### tab-7\r\n        self.det_thread.send_img_ch5.connect(lambda x: self.show_image(x, self.video_label_ch9))\r\n\r\n        self.det_thread.send_statistic.connect(self.show_statistic)\r\n        self.det_thread.send_msg.connect(lambda x: self.show_msg(x))\r\n        self.det_thread.send_percent.connect(lambda x: self.progressBar.setValue(x))\r\n        self.det_thread.send_fps.connect(lambda x: self.fps_label.setText(x))\r\n\r\n        self.fileButton.clicked.connect(self.open_file)\r\n        self.cameraButton.clicked.connect(self.chose_cam)\r\n        self.rtspButton.clicked.connect(self.chose_rtsp)\r\n\r\n        self.runButton.clicked.connect(self.run_or_continue)\r\n        self.runButton_modbus.clicked.connect(self.modbus_on_off)\r\n\r\n        self.stopButton.clicked.connect(self.stop)\r\n\r\n        self.comboBox.currentTextChanged.connect(self.change_model)\r\n        self.comboBox_device.currentTextChanged.connect(self.change_device)\r\n        self.comboBox_source.currentTextChanged.connect(self.change_source)\r\n        self.comboBox_port.currentTextChanged.connect(self.change_port)\r\n\r\n        self.confSpinBox.valueChanged.connect(lambda x: self.change_val(x, 'confSpinBox'))\r\n        self.confSlider.valueChanged.connect(lambda x: self.change_val(x, 'confSlider'))\r\n        self.iouSpinBox.valueChanged.connect(lambda x: self.change_val(x, 'iouSpinBox'))\r\n        self.iouSlider.valueChanged.connect(lambda x: self.change_val(x, 'iouSlider'))\r\n        self.rateSpinBox.valueChanged.connect(lambda x: self.change_val(x, 'rateSpinBox'))\r\n        self.rateSlider.valueChanged.connect(lambda x: self.change_val(x, 'rateSlider'))\r\n\r\n        self.checkBox.clicked.connect(self.checkrate)\r\n        self.saveCheckBox.clicked.connect(self.is_save)\r\n        self.load_setting()  #### loading config\r\n\r\n\r\n    def run_or_continue(self):\r\n        # self.det_thread.source = 'streams.txt'\r\n        # self.det_thread.send_img_ch0.connect(lambda x: self.show_image(x, self.video_label_ch0))\r\n        # self.det_thread.send_img_ch1.connect(lambda x: self.show_image(x, self.video_label_ch1))\r\n        # self.det_thread.send_img_ch2.connect(lambda x: self.show_image(x, self.video_label_ch2))\r\n        # self.det_thread.send_img_ch3.connect(lambda x: self.show_image(x, self.video_label_ch3))\r\n        self.det_thread.jump_out = False\r\n        print('runbutton is check', self.runButton.isChecked())\r\n        if self.runButton.isChecked():\r\n            self.runButton.setText('PAUSE')\r\n            self.saveCheckBox.setEnabled(False)\r\n            self.det_thread.is_continue = True\r\n            if not self.det_thread.isRunning():\r\n                self.det_thread.start()\r\n            device = os.path.basename(self.det_thread.device)  ### only for display\r\n            source = os.path.basename(self.det_thread.source)  ### only for display\r\n            source = str(source) if source.isnumeric() else source  ### only for display\r\n            self.statistic_msg('Detecting >> model：{}，device: {}, source：{}'.\r\n                               format(os.path.basename(self.det_thread.weights),device,\r\n                                      source))\r\n            print('self.det_thread.is_continue', self.det_thread.is_continue)\r\n        else:\r\n            self.det_thread.is_continue = False\r\n            self.runButton.setText('RUN')\r\n            self.statistic_msg('Pause')\r\n            print('self.det_thread.is_continue', self.det_thread.is_continue)\r\n\r\n    def thread_mudbus_run(self):\r\n        global modbus_flag\r\n        modbus_flag = True\r\n        #### hexcode  ######\r\n        IN0_READ = '01 02 00 00 00 01 B9 CA'\r\n        IN1_READ = '01 02 00 01 00 01 E8 0A'\r\n        IN2_READ = '01 02 00 02 00 01 18 0A'\r\n        IN3_READ = '01 02 00 03 00 01 49 CA'\r\n        DO0_ON = '01 05 00 00 FF 00 8C 3A'\r\n        DO0_OFF = '01 05 00 00 00 00 CD CA'\r\n        DO1_ON = '01 05 00 01 FF 00 DD FA'\r\n        DO1_OFF = '01 05 00 01 00 00 9C 0A'\r\n        DO2_ON = '01 05 00 02 FF 00 2D FA'\r\n        DO2_OFF = '01 05 00 02 00 00 6C 0A'\r\n        DO3_ON = '01 05 00 03 FF 00 7C 3A'\r\n        DO3_OFF = '01 05 00 03 00 00 3D CA'\r\n\r\n        DO_ALL_ON = '01 0F 00 00 00 04 01 FF 7E D6'\r\n        DO_ALL_OFF = '01 0F 00 00 00 04 01 00 3E 96' ##OUT1-4  OFF  全部继电器关闭  初始化\r\n\r\n        # self.ret = None\r\n        self.port_type = self.comboBox_port.currentText()\r\n        print(type(self.port_type), self.port_type)\r\n        # try:\r\n        #     self.ser, self.ret, _ = modbus_rtu.openport(port='COM5', baudrate=9600, timeout=5)  # 打开端口\r\n        #     print('self.ret',self.ret)\r\n        # except Exception as e:\r\n        #     self.ret = False\r\n        #     print('open port error', e)\r\n        #     self.statistic_msg(str(e))\r\n        #     self.runButton_modbus.setChecked(False)\r\n\r\n        if self.ret: ### openport sucessfully\r\n            feedback_data = modbus_rtu.writedata(self.ser, DO_ALL_OFF)  ###OUT1-4  OFF  全部继电器关闭  初始化\r\n            self.runButton_modbus.setChecked(True)\r\n            print('thread_mudbus_run modbus_flag = True')\r\n            feedback_list = []\r\n\r\n            while self.runButton_modbus.isChecked() and modbus_flag:\r\n\r\n                feedback_data_IN0 = modbus_rtu.writedata(self.ser, IN0_READ)  #### 检查IN1 触发 返回01 02 01 00 a188\r\n                if feedback_data_IN0:#### 有返回数据\r\n                    text_IN0 = feedback_data_IN0[0:8]  ## 读取8位字符\r\n                    if text_IN0 == '01020101':\r\n                        self.checkBox_10.setChecked(True)\r\n                    else:\r\n                        self.checkBox_10.setChecked(False)\r\n                    print('text_IN0', text_IN0)\r\n                    feedback_list.append(text_IN0)\r\n                    feedback_data = modbus_rtu.writedata(self.ser, DO0_ON)  ###1号继电器打开  运行准备 DO1 =1\r\n                else: #### 无返回数据\r\n                    no_feedback = modbus_rtu.writedata(self.ser, DO2_ON)  ###3号继电器打开   控制器无返回数据 D03 =1\r\n                    print('no_feedback data')\r\n\r\n                feedback_data_IN1 = modbus_rtu.writedata(self.ser, IN1_READ)  #### 检查IN2 触发 返回01 02 01 00 a188\r\n                if feedback_data_IN1:  #### 有返回数据\r\n                    text_IN1 = feedback_data_IN1[0:8]  ## 读取8位字符\r\n                    if text_IN1 == '01020101':\r\n                        self.checkBox_11.setChecked(True)\r\n                    else:\r\n                        self.checkBox_11.setChecked(False)\r\n                    print('text_IN1', text_IN1)\r\n                    feedback_list.append(text_IN1)\r\n                else:  #### 无返回数据\r\n                    no_feedback = modbus_rtu.writedata(self.ser,DO2_ON)  ###3号继电器打开   控制器无返回数据 D03 =1\r\n                    print('no_feedback data')\r\n\r\n                feedback_data_IN2 = modbus_rtu.writedata(self.ser,IN2_READ)  #### 检查IN2 触发 返回01 02 01 00 a188\r\n                if feedback_data_IN2:  #### 有返回数据\r\n                    text_IN2 = feedback_data_IN2[0:8]  ## 读取8位字符\r\n                    if text_IN2 == '01020101':\r\n                        self.checkBox_12.setChecked(True)\r\n                    else:\r\n                        self.checkBox_12.setChecked(False)\r\n                    print('text_IN2', text_IN2)\r\n                    feedback_list.append(text_IN2)\r\n                else:  #### 无返回数据\r\n                    no_feedback = modbus_rtu.writedata(self.ser,DO2_ON)  ###3号继电器打开   控制器无返回数据 D03 =1\r\n                    print('no_feedback data')\r\n\r\n                feedback_data_IN3 = modbus_rtu.writedata(self.ser,IN3_READ)  ####\r\n                if feedback_data_IN3:  #### 有返回数据\r\n                    text_IN3 = feedback_data_IN3[0:8]  ## 读取8位字符\r\n                    if text_IN3 == '01020101':\r\n                        self.checkBox_13.setChecked(True)\r\n                    else:\r\n                        self.checkBox_13.setChecked(False)\r\n                    print('text_IN3', text_IN3)\r\n                    feedback_list.append(text_IN3)\r\n                else:  #### 无返回数据\r\n                    no_feedback = modbus_rtu.writedata(self.ser,DO2_ON)  ###3号继电器打开   控制器无返回数据 D03 =1\r\n                    print('no_feedback data')\r\n\r\n                if len(feedback_list) == 20:\r\n                    feedback_list.clear()\r\n                else:\r\n                    self.statistic_msg(str(feedback_list))\r\n\r\n                #### 同步UI 信号\r\n                intput_box_list = [self.checkBox_10.isChecked(), self.checkBox_11.isChecked(), self.checkBox_12.isChecked(), self.checkBox_13.isChecked()]\r\n                output_box_list =[self.checkBox_2.isChecked()]#,self.checkBox_3.isChecked(),self.checkBox_4.isChecked(),self.checkBox_5.isChecked()]\r\n\r\n                for i , n in enumerate(output_box_list):\r\n                    if n == True:\r\n                        print('scratch detected')\r\n                        feedback_data = modbus_rtu.writedata(self.ser, DO3_ON)  ### OUT4 = 1\r\n                    else:\r\n                        print('scratch has not detected')\r\n                        feedback_data = modbus_rtu.writedata(self.ser, DO3_OFF)  ### OUT4 = 0\r\n\r\n            else:\r\n                modbus_flag = False\r\n                print('modbus shut off')\r\n                shut_coil = modbus_rtu.writedata(self.ser, DO_ALL_OFF)  ###OUT1-4  OFF  全部继电器关闭  初始化\r\n\r\n                self.ser.close()\r\n\r\n\r\n    def modbus_on_off(self):\r\n        global modbus_flag\r\n        # if not modbus_flag:\r\n        if self.runButton_modbus.isChecked():\r\n            print('runButton_modbus.isChecked')\r\n            modbus_flag = True\r\n            print('set  modbus_flag = True')\r\n            try:\r\n                self.ser, self.ret, error = modbus_rtu.openport(self.port_type, 9600, 5)  # 打开端口\r\n            except Exception as e:\r\n                print('openport erro -1', e)\r\n                self.statistic_msg(str(e))\r\n\r\n            if not self.ret:\r\n                self.runButton_modbus.setChecked(False)\r\n                MessageBox(\r\n                    self.closeButton, title='Error', text='Connection Error: '+ str(error), time=2000,\r\n                    auto=True).exec_()\r\n                print('port did not open')\r\n                try:\r\n                    self.ser, self.ret, error = modbus_rtu.openport(self.port_type, 9600, 5)  # 打开端口\r\n                    if self.ret:\r\n                        _thread.start_new_thread(myWin.thread_mudbus_run, ())  #### 启动检测 信号 循环\r\n                except Exception as e:\r\n                    print('openport erro-2', e)\r\n                    self.statistic_msg(str(e))\r\n            else: ### self.ret is  True\r\n                self.runButton_modbus.setChecked(True)\r\n                _thread.start_new_thread(myWin.thread_mudbus_run, ())  #### 启动检测 信号 循环\r\n\r\n        else: #### shut down modbus\r\n            print('runButton_modbus.is unChecked')\r\n            modbus_flag = False\r\n            self.runButton_modbus.setChecked(False)\r\n            print('shut down modbus_flag = False')\r\n\r\n\r\n    def stop(self):\r\n        if not self.det_thread.jump_out:\r\n            self.det_thread.jump_out = True\r\n        self.saveCheckBox.setEnabled(True)\r\n        # self.det_thread.stop()  #### bug-1 加入 停止线程会卡死  未解决\r\n\r\n    def search_pt(self):\r\n        pt_list = os.listdir('./pt')\r\n        pt_list = [file for file in pt_list if file.endswith('.pt')]\r\n        pt_list.sort(key=lambda x: os.path.getsize('./pt/' + x))\r\n\r\n        if pt_list != self.pt_list:\r\n            self.pt_list = pt_list\r\n            self.comboBox.clear()\r\n            self.comboBox.addItems(self.pt_list)\r\n\r\n    def is_save(self):\r\n        if self.saveCheckBox.isChecked():\r\n            self.det_thread.save_fold = './auto_save/mp4'  ### save result as .mp4\r\n        else:\r\n            self.det_thread.save_fold = None\r\n\r\n    def checkrate(self):  #####latency checkbox\r\n        if self.checkBox.isChecked():\r\n            self.det_thread.rate_check = True\r\n        else:\r\n            self.det_thread.rate_check = False\r\n\r\n    def chose_rtsp(self):\r\n        self.rtsp_window = Window()\r\n        config_file = 'config/ip.json'\r\n        if not os.path.exists(config_file):\r\n            ip = \"rtsp://admin:admin888@192.168.1.67:555\"\r\n            new_config = {\"ip\": ip}\r\n            new_json = json.dumps(new_config, ensure_ascii=False, indent=2)\r\n            with open(config_file, 'w', encoding='utf-8') as f:\r\n                f.write(new_json)\r\n        else:\r\n            config = json.load(open(config_file, 'r', encoding='utf-8'))\r\n            ip = config['ip']\r\n        self.rtsp_window.rtspEdit.setText(ip)\r\n        self.rtsp_window.show()\r\n        self.rtsp_window.rtspButton.clicked.connect(lambda: self.load_rtsp(self.rtsp_window.rtspEdit.text()))\r\n\r\n    def load_rtsp(self, ip):\r\n        try:\r\n            self.stop()\r\n            MessageBox(\r\n                self.closeButton, title='Tips', text='Loading rtsp stream', time=1000, auto=True).exec_()\r\n            self.det_thread.source = ip\r\n            new_config = {\"ip\": ip}\r\n            new_json = json.dumps(new_config, ensure_ascii=False, indent=2)\r\n            with open('config/ip.json', 'w', encoding='utf-8') as f:\r\n                f.write(new_json)\r\n            self.statistic_msg('Loading rtsp：{}'.format(ip))\r\n            self.rtsp_window.close()\r\n        except Exception as e:\r\n            self.statistic_msg('%s' % e)\r\n\r\n    def chose_cam(self):\r\n        try:\r\n            self.stop()  #### stop running thread\r\n            MessageBox(\r\n                self.closeButton, title='Enumerate Cameras', text='Loading camera', time=200, auto=True).exec_()# self.closeButton, title='Enumerate Cameras', text='Loading camera', time=2000, auto=True).exec_()\r\n            # get the number of local cameras\r\n            _, cams = Camera().get_cam_num()\r\n            print('enum_camera:', cams)\r\n            self.statistic_msg('enum camera：{}'.format(cams))\r\n            popMenu = QMenu()\r\n            popMenu.setFixedWidth(self.cameraButton.width())\r\n            popMenu.setStyleSheet('''\r\n                                            QMenu {\r\n                                            font-size: 16px;\r\n                                            font-family: \"Microsoft YaHei UI\";\r\n                                            font-weight: light;\r\n                                            color:white;\r\n                                            padding-left: 5px;\r\n                                            padding-right: 5px;\r\n                                            padding-top: 4px;\r\n                                            padding-bottom: 4px;\r\n                                            border-style: solid;\r\n                                            border-width: 0px;\r\n                                            border-color: rgba(255, 255, 255, 255);\r\n                                            border-radius: 3px;\r\n                                            background-color: rgba(200, 200, 200,50);}\r\n                                            ''')\r\n\r\n            for cam in cams:\r\n                exec(\"action_%s = QAction('%s')\" % (cam, cam))\r\n                exec(\"popMenu.addAction(action_%s)\" % cam)\r\n            x = self.groupBox_5.mapToGlobal(self.cameraButton.pos()).x()\r\n            y = self.groupBox_5.mapToGlobal(self.cameraButton.pos()).y()\r\n            y = y + self.cameraButton.frameGeometry().height()\r\n            pos = QPoint(x, y)\r\n            action = popMenu.exec_(pos)\r\n            if action:\r\n                self.det_thread.source = action.text()  ##### choose source\r\n                self.statistic_msg('Loading camera：{}'.format(action.text()))\r\n        except Exception as e:\r\n            self.statistic_msg('%s' % e)\r\n\r\n    def change_val(self, x, flag):\r\n        if flag == 'confSpinBox':\r\n            self.confSlider.setValue(int(x*100))\r\n        elif flag == 'confSlider':\r\n            self.confSpinBox.setValue(x/100)\r\n            self.det_thread.conf_thres = x/100\r\n        elif flag == 'iouSpinBox':\r\n            self.iouSlider.setValue(int(x*100))\r\n        elif flag == 'iouSlider':\r\n            self.iouSpinBox.setValue(x/100)\r\n            self.det_thread.iou_thres = x/100\r\n        elif flag == 'rateSpinBox':\r\n            self.rateSlider.setValue(x)\r\n        elif flag == 'rateSlider':\r\n            self.rateSpinBox.setValue(x)\r\n            self.det_thread.rate = x * 10\r\n        else:\r\n            pass\r\n\r\n    def statistic_msg(self, msg):\r\n        self.statistic_label.setText(msg)\r\n        print(msg)\r\n        # self.qtimer.start(3000)\r\n\r\n    def show_msg(self, msg):\r\n        self.runButton.setChecked(Qt.Unchecked)\r\n        self.statistic_msg(msg)\r\n        if msg == \"Finished\":\r\n            self.saveCheckBox.setEnabled(True)\r\n\r\n    def change_model(self, x):\r\n        self.model_type = self.comboBox.currentText()  #comboBox\r\n        self.det_thread.weights = \"./pt/%s\" % self.model_type\r\n        self.statistic_msg('Change model to %s' % x)\r\n\r\n    def change_device(self, x):\r\n        self.device_type = self.comboBox_device.currentText()\r\n        self.det_thread.device = self.device_type\r\n        self.statistic_msg('Change device to %s' % x)\r\n\r\n    def change_source(self, x):\r\n        self.source_type = self.comboBox_source.currentText()\r\n        self.det_thread.source = self.source_type\r\n        self.statistic_msg('Change source to %s' % x)\r\n\r\n    def change_port(self, x):\r\n        self.port_type = self.comboBox_port.currentText()\r\n        # self.det_thread.source = self.source_type\r\n        self.statistic_msg('Change port to %s' % x)\r\n\r\n\r\n    def open_file(self):\r\n        config_file = 'config/fold.json'\r\n        # config = json.load(open(config_file, 'r', encoding='utf-8'))\r\n        config = json.load(open(config_file, 'r', encoding='utf-8'))\r\n        open_fold = config['open_fold']\r\n        if not os.path.exists(open_fold):\r\n            open_fold = os.getcwd()\r\n        name, _ = QFileDialog.getOpenFileName(self, 'Video/image', open_fold, \"Pic File(*.mp4 *.mkv *.avi *.flv \"\r\n                                                                          \"*.jpg *.png)\")\r\n        if name:\r\n            self.det_thread.source = name\r\n            self.statistic_msg('Loaded file：{}'.format(os.path.basename(name)))\r\n            config['open_fold'] = os.path.dirname(name)\r\n            config_json = json.dumps(config, ensure_ascii=False, indent=2)\r\n            with open(config_file, 'w', encoding='utf-8') as f:\r\n                f.write(config_json)\r\n            self.stop()\r\n\r\n    def max_or_restore(self):  ### window size control\r\n        if self.maxButton.isChecked():\r\n            self.showMaximized()\r\n        else:\r\n            self.showNormal()\r\n\r\n\r\n    def mousePressEvent(self, event):\r\n        self.m_Position = event.pos()\r\n        if event.button() == Qt.LeftButton:\r\n            if 0 < self.m_Position.x() < self.groupBox.pos().x() + self.groupBox.width() and \\\r\n                    0 < self.m_Position.y() < self.groupBox.pos().y() + self.groupBox.height():\r\n                self.m_flag = True\r\n\r\n    def mouseMoveEvent(self, QMouseEvent):\r\n        if Qt.LeftButton and self.m_flag:\r\n            self.move(QMouseEvent.globalPos() - self.m_Position)\r\n\r\n    def mouseReleaseEvent(self, QMouseEvent):\r\n        self.m_flag = False\r\n\r\n    @staticmethod\r\n    def show_image(img_src, label):  ### input img_src  output to pyqt label\r\n        try:\r\n            ih, iw, _ = img_src.shape\r\n            w = label.geometry().width()\r\n            h = label.geometry().height()\r\n            # keep original aspect ratio\r\n            if iw/w > ih/h:\r\n                scal = w / iw\r\n                nw = w\r\n                nh = int(scal * ih)\r\n                img_src_ = cv2.resize(img_src, (nw, nh))\r\n\r\n            else:\r\n                scal = h / ih\r\n                nw = int(scal * iw)\r\n                nh = h\r\n                img_src_ = cv2.resize(img_src, (nw, nh))\r\n\r\n            frame = cv2.cvtColor(img_src_, cv2.COLOR_BGR2RGB)\r\n            img = QImage(frame.data, frame.shape[1], frame.shape[0], frame.shape[2] * frame.shape[1],\r\n                         QImage.Format_RGB888)\r\n            label.setPixmap(QPixmap.fromImage(img))\r\n\r\n        except Exception as e:\r\n            print(repr(e))\r\n\r\n    def show_statistic(self, statistic_dic):  ### predicttion  output\r\n        import re\r\n        try:\r\n            self.resultWidget.clear()\r\n            statistic_dic = sorted(statistic_dic.items(), key=lambda x: x[1], reverse=True)\r\n            statistic_dic = [i for i in statistic_dic if i[1] > 0] ## append to List  while the value greater than 0\r\n            results = [' '+str(i[0]) + '：' + str(i[1]) for i in statistic_dic]  ### reform the list\r\n            # print('output result:', type(results), results)\r\n            self.resultWidget.addItems(results)\r\n            if len(results) :\r\n                print((len(results)))\r\n                for i , n in enumerate(results):\r\n                    # str = re.sub(\"[\\u4e00-\\u9fa5\\0-9\\,\\。]\", \"\", i)\r\n                    # print('class name = ', n)\r\n                    if i == 0:\r\n                        self.checkBox_2.setChecked(True)\r\n                    # else:\r\n                    #     self.checkBox_2.setChecked(False)\r\n                    if i == 1:\r\n                        self.checkBox_3.setChecked(True)\r\n                    # else:\r\n                    #     self.checkBox_3.setChecked(False)\r\n                    if i == 2:\r\n                        self.checkBox_4.setChecked(True)\r\n                    # else:\r\n                    #     self.checkBox_4.setChecked(False)\r\n                    if i == 3:\r\n                        self.checkBox_5.setChecked(True)\r\n                    # else:\r\n                    #     self.checkBox_5.setChecked(False)\r\n                    # if i == 5:\r\n                    #     self.checkBox_6.setChecked(True)\r\n\r\n                    # self.checkBox_2.setText(str(i))\r\n            else:\r\n                self.checkBox_2.setChecked(False)\r\n                self.checkBox_3.setChecked(False)\r\n                self.checkBox_4.setChecked(False)\r\n                self.checkBox_5.setChecked(False)\r\n                self.checkBox_6.setChecked(False)\r\n                # self.checkBox_2.setText(\"\")\r\n                print(\"result = []\")\r\n\r\n        except Exception as e:\r\n            print(repr(e))\r\n\r\n    def load_setting(self):\r\n        config_file = 'config/setting.json'\r\n\r\n        if not os.path.exists(config_file):\r\n            iou = 0.26\r\n            conf = 0.33\r\n            rate = 10\r\n            check = 0\r\n            savecheck = 0\r\n            device = 0\r\n            port = \"COM3\"\r\n            new_config = {\"iou\": iou,\r\n                          \"conf\": conf,\r\n                          \"rate\": rate,\r\n                          \"check\": check,\r\n                          \"savecheck\": savecheck,\r\n                          \"device\": device,\r\n                          \"port\": port\r\n                          }\r\n            new_json = json.dumps(new_config, ensure_ascii=False, indent=2)\r\n            with open(config_file, 'w', encoding='utf-8') as f:\r\n                f.write(new_json)\r\n        else:\r\n            config = json.load(open(config_file, 'r', encoding='utf-8'))\r\n            print('load config:',type(config), config)\r\n            if len(config) != 8 : ### 参数不足时  补充参数\r\n                iou = 0.26\r\n                conf = 0.33\r\n                rate = 10\r\n                check = 0\r\n                savecheck = 0\r\n                device = 0\r\n                port = \"COM3\"\r\n                source = 0\r\n            else:\r\n                iou = config['iou']\r\n                conf = config['conf']\r\n                rate = config['rate']\r\n                check = config['check']\r\n                savecheck = config['savecheck']\r\n                device = config['device'] ## index number\r\n                port = config['port'] ## index number\r\n                source = config['source']\r\n        ### 依据存储的json文件 更新 ui参数\r\n        self.confSpinBox.setValue(conf)\r\n        self.iouSpinBox.setValue(iou)\r\n        self.rateSpinBox.setValue(rate)\r\n        self.checkBox.setCheckState(check)\r\n        self.det_thread.rate_check = check\r\n        self.saveCheckBox.setCheckState(savecheck)\r\n        self.is_save() ###auto save  checkbox\r\n\r\n        self.comboBox_device.setCurrentIndex(device) # 设置当前索引号 \"device\": 0\r\n        self.comboBox_port.setCurrentIndex(port)  # 设置当前索引号 \"port\": \"COM0\"\r\n        self.comboBox_source.setCurrentIndex(source)  # 设置当前索引号 \"port\": \"COM0\"\r\n    def closeEvent(self, event):\r\n        self.det_thread.jump_out = True\r\n        config_path = 'config/setting.json'\r\n        config = dict()\r\n        config['iou'] = self.confSpinBox.value()\r\n        config['conf'] = self.iouSpinBox.value()\r\n        config['rate'] = self.rateSpinBox.value()\r\n        config['check'] = self.checkBox.checkState()  ### Latency funtion\r\n        config['savecheck'] = self.saveCheckBox.checkState() ### Auto Save\r\n        config['device'] = self.comboBox_device.currentIndex() ### 获取当前索引号\r\n        config['port'] = self.comboBox_port.currentIndex()  ### 获取当前索引号\r\n        config['source'] = self.comboBox_source.currentIndex()  ### 获取当前索引号\r\n        ####新增参数 请在此处添加， 运行UI后 点击关闭按钮 后保存为 json文件 地址= ./config/setting.json\r\n        config_json = json.dumps(config, ensure_ascii=False, indent=2)\r\n        with open(config_path, 'w', encoding='utf-8') as f:\r\n            f.write(config_json)\r\n            print('confi_json write')\r\n        MessageBox(\r\n            self.closeButton, title='Tips', text='Program is exiting.', time=2000, auto=True).exec_()\r\n        sys.exit(0)\r\n\r\n    def load_config(self):   ####  初始化 modbus connection\r\n      global winsound_freq, winsound_time, winsound_freq_2, winsound_time_2\r\n      try:\r\n          ### 提取备份数据 当出现断电关机数据丢失时， 将Cahce中 备份文件拷贝出来\r\n          cache_path = os.path.dirname(os.path.realpath(__file__)) + r'\\config'\r\n          to_path = os.path.dirname(os.path.realpath(__file__))  ### root path\r\n\r\n          for root, dirs, files in os.walk(\r\n                  cache_path):  # root 表示当前正在访问的文件夹路径# dirs 表示该文件夹下的子目录名list # files 表示该文件夹下的文件list\r\n              # print('files',files) ####['edgevalue.db.bak', 'edgevalue.db.dat', 'edgevalue.db.dir']\r\n              for i in files:\r\n                  from_path = os.path.join(root, i)  # 合并成一个完整路径\r\n                  # copy(from_path, to_path)  ### 第一个参数 是复制对象， 第二个是 复制到文件夹\r\n                  # print('from_path', from_path)\r\n                  # print('to_path', to_path)\r\n              print('files in config has been coppied sucessfully')\r\n\r\n          # self.ser, self.ret , error = modbus_rtu.openport(self.port_type, 9600, 5)  # 打开端口\r\n\r\n      except Exception as e:\r\n          print('openport erro', e)\r\n          self.statistic_msg(str(e))\r\n\r\n\r\n\r\n\r\n####  for  testing  ↓ ##################################################\r\ndef cvshow_image(img):  ### input img_src  output to pyqt label\r\n    try:\r\n        cv2.imshow('Image', img)\r\n    except Exception as e:\r\n        print(repr(e))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    app = QApplication(sys.argv)\r\n    myWin = MainWindow() #### 实例化\r\n    myWin.show()\r\n    print('prameters load completed')\r\n    myWin.runButton_modbus.setChecked(True)\r\n    myWin.modbus_on_off()### start modbus\r\n    # time.sleep(1)\r\n    # print('thread_mudbus_run start')\r\n    # _thread.start_new_thread(myWin.thread_mudbus_run, ())  #### 启动检测 信号 循环\r\n\r\n\r\n    #### 调试用代码\r\n    # det_thread = DetThread() #### 实例化\r\n    # det_thread.weights = \"pt/yolov5s.pt\"\r\n    # det_thread.device = '0'\r\n    # det_thread.source = 'streams.txt'\r\n    # det_thread.is_continue = True\r\n    # det_thread.start()   ###\r\n    # # ##### connect UI  调试输出到 UI  ↓\r\n    # det_thread.send_img_ch0.connect(lambda x: myWin.show_image(x, myWin.video_label_ch0))\r\n    # det_thread.send_img_ch1.connect(lambda x: myWin.show_image(x, myWin.video_label_ch1))\r\n    # det_thread.send_img_ch2.connect(lambda x: myWin.show_image(x, myWin.video_label_ch2))\r\n    # det_thread.send_img_ch3.connect(lambda x: myWin.show_image(x, myWin.video_label_ch3))\r\n\r\n    # 单独输出 调试模式 ↓\r\n    # det_thread.send_img_ch0.connect(lambda x: cvshow_image(x))\r\n\r\n    # myWin.showMaximized()\r\n    sys.exit(app.exec_())\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	(revision 38763aecd12af542eecef5d90e69df6fc12d9b25)
+++ b/main.py	(date 1708316089975)
@@ -63,7 +63,7 @@
 
     @torch.no_grad()
     def run(self,
-            imgsz=320, #1440 # inference size (pixels)//推理大小
+            imgsz=640, #1440 # inference size (pixels)//推理大小
             max_det=50,  # maximum detections per image//每个图像的最大检测次数
             # self.source = '0'
             # self.device='0',  # cuda device, i.e. 0 or 0,1,2,3 or cpu
@@ -256,13 +256,13 @@
                                         print('save_one_box')
                             # print('detection is running')
 
-                        FSP = int(1 / (t2 - t1))
+                        fsp = int(1 / (t2 - t1))
                         # print(f'{s}Done. ({t2 - t1:.3f}s FSP={FSP})')
 
                         # Stream results   emit frame
                         if self.is_continue: ###### 发送图片必须在  for i, det in enumerate(pred): 循环内
                         # if view_img:
-                            cv2.putText(im0, str(f'FSP = {FSP}  CAM = {i}'), (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)
+                            cv2.putText(im0, str(f'FSP = {fsp}  CAM = {i}'), (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)
                             res = cv2.resize(im0, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)
                             ## chanel-0  ##### show images
                             if label_chanel == '0':
@@ -514,55 +514,55 @@
 
             while self.runButton_modbus.isChecked() and modbus_flag:
 
-                feedback_data_IN0 = modbus_rtu.writedata(self.ser, IN0_READ)  #### 检查IN1 触发 返回01 02 01 00 a188
-                if feedback_data_IN0:#### 有返回数据
-                    text_IN0 = feedback_data_IN0[0:8]  ## 读取8位字符
-                    if text_IN0 == '01020101':
+                feedback_data_in0 = modbus_rtu.writedata(self.ser, IN0_READ)  #### 检查IN1 触发 返回01 02 01 00 a188
+                if feedback_data_in0:#### 有返回数据
+                    text_in0 = feedback_data_in0[0:8]  ## 读取8位字符
+                    if text_in0 == '01020101':
                         self.checkBox_10.setChecked(True)
                     else:
                         self.checkBox_10.setChecked(False)
-                    print('text_IN0', text_IN0)
-                    feedback_list.append(text_IN0)
+                    print('text_IN0', text_in0)
+                    feedback_list.append(text_in0)
                     feedback_data = modbus_rtu.writedata(self.ser, DO0_ON)  ###1号继电器打开  运行准备 DO1 =1
                 else: #### 无返回数据
                     no_feedback = modbus_rtu.writedata(self.ser, DO2_ON)  ###3号继电器打开   控制器无返回数据 D03 =1
                     print('no_feedback data')
 
-                feedback_data_IN1 = modbus_rtu.writedata(self.ser, IN1_READ)  #### 检查IN2 触发 返回01 02 01 00 a188
-                if feedback_data_IN1:  #### 有返回数据
-                    text_IN1 = feedback_data_IN1[0:8]  ## 读取8位字符
-                    if text_IN1 == '01020101':
+                feedback_data_in1 = modbus_rtu.writedata(self.ser, IN1_READ)  #### 检查IN2 触发 返回01 02 01 00 a188
+                if feedback_data_in1:  #### 有返回数据
+                    text_in1 = feedback_data_in1[0:8]  ## 读取8位字符
+                    if text_in1 == '01020101':
                         self.checkBox_11.setChecked(True)
                     else:
                         self.checkBox_11.setChecked(False)
-                    print('text_IN1', text_IN1)
-                    feedback_list.append(text_IN1)
+                    print('text_IN1', text_in1)
+                    feedback_list.append(text_in1)
                 else:  #### 无返回数据
                     no_feedback = modbus_rtu.writedata(self.ser,DO2_ON)  ###3号继电器打开   控制器无返回数据 D03 =1
                     print('no_feedback data')
 
-                feedback_data_IN2 = modbus_rtu.writedata(self.ser,IN2_READ)  #### 检查IN2 触发 返回01 02 01 00 a188
-                if feedback_data_IN2:  #### 有返回数据
-                    text_IN2 = feedback_data_IN2[0:8]  ## 读取8位字符
-                    if text_IN2 == '01020101':
+                feedback_data_in2 = modbus_rtu.writedata(self.ser,IN2_READ)  #### 检查IN2 触发 返回01 02 01 00 a188
+                if feedback_data_in2:  #### 有返回数据
+                    text_in2 = feedback_data_in2[0:8]  ## 读取8位字符
+                    if text_in2 == '01020101':
                         self.checkBox_12.setChecked(True)
                     else:
                         self.checkBox_12.setChecked(False)
-                    print('text_IN2', text_IN2)
-                    feedback_list.append(text_IN2)
+                    print('text_IN2', text_in2)
+                    feedback_list.append(text_in2)
                 else:  #### 无返回数据
                     no_feedback = modbus_rtu.writedata(self.ser,DO2_ON)  ###3号继电器打开   控制器无返回数据 D03 =1
                     print('no_feedback data')
 
-                feedback_data_IN3 = modbus_rtu.writedata(self.ser,IN3_READ)  ####
-                if feedback_data_IN3:  #### 有返回数据
-                    text_IN3 = feedback_data_IN3[0:8]  ## 读取8位字符
-                    if text_IN3 == '01020101':
+                feedback_data_in3 = modbus_rtu.writedata(self.ser,IN3_READ)  ####
+                if feedback_data_in3:  #### 有返回数据
+                    text_in3 = feedback_data_in3[0:8]  ## 读取8位字符
+                    if text_in3 == '01020101':
                         self.checkBox_13.setChecked(True)
                     else:
                         self.checkBox_13.setChecked(False)
-                    print('text_IN3', text_IN3)
-                    feedback_list.append(text_IN3)
+                    print('text_IN3', text_in3)
+                    feedback_list.append(text_in3)
                 else:  #### 无返回数据
                     no_feedback = modbus_rtu.writedata(self.ser,DO2_ON)  ###3号继电器打开   控制器无返回数据 D03 =1
                     print('no_feedback data')
@@ -577,7 +577,7 @@
                 output_box_list =[self.checkBox_2.isChecked()]#,self.checkBox_3.isChecked(),self.checkBox_4.isChecked(),self.checkBox_5.isChecked()]
 
                 for i , n in enumerate(output_box_list):
-                    if n == True:
+                    if n:
                         print('scratch detected')
                         feedback_data = modbus_rtu.writedata(self.ser, DO3_ON)  ### OUT4 = 1
                     else:
@@ -1034,5 +1034,3 @@
 
     # myWin.showMaximized()
     sys.exit(app.exec_())
-
-
Index: utils/datasets.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># YOLOv5 \uD83D\uDE80 by Ultralytics, GPL-3.0 license\r\n\"\"\"\r\nDataloaders and dataset utils\r\n\"\"\"\r\n\r\nimport glob\r\nimport hashlib\r\nimport json\r\nimport math\r\nimport os\r\nimport random\r\nimport shutil\r\nimport time\r\nfrom itertools import repeat\r\nfrom multiprocessing.pool import Pool, ThreadPool\r\nfrom pathlib import Path\r\nfrom threading import Thread\r\nfrom zipfile import ZipFile\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport yaml\r\nfrom PIL import ExifTags, Image, ImageOps\r\nfrom torch.utils.data import DataLoader, Dataset, dataloader, distributed\r\nfrom tqdm import tqdm\r\n\r\nfrom utils.augmentations import Albumentations, augment_hsv, copy_paste, letterbox, mixup, random_perspective\r\nfrom utils.general import (DATASETS_DIR, LOGGER, NUM_THREADS, check_dataset, check_requirements, check_yaml, clean_str,\r\n                           segments2boxes, xyn2xy, xywh2xyxy, xywhn2xyxy, xyxy2xywhn)\r\nfrom utils.torch_utils import torch_distributed_zero_first\r\n\r\n# Parameters\r\nHELP_URL = 'https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data'\r\nIMG_FORMATS = ['bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp']  # include image suffixes\r\nVID_FORMATS = ['asf', 'avi', 'gif', 'm4v', 'mkv', 'mov', 'mp4', 'mpeg', 'mpg', 'wmv']  # include video suffixes\r\n\r\n# Get orientation exif tag\r\nfor orientation in ExifTags.TAGS.keys():\r\n    if ExifTags.TAGS[orientation] == 'Orientation':\r\n        break\r\n\r\n\r\ndef get_hash(paths):\r\n    # Returns a single hash value of a list of paths (files or dirs)\r\n    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes\r\n    h = hashlib.md5(str(size).encode())  # hash sizes\r\n    h.update(''.join(paths).encode())  # hash paths\r\n    return h.hexdigest()  # return hash\r\n\r\n\r\ndef exif_size(img):\r\n    # Returns exif-corrected PIL size\r\n    s = img.size  # (width, height)\r\n    try:\r\n        rotation = dict(img._getexif().items())[orientation]\r\n        if rotation == 6:  # rotation 270\r\n            s = (s[1], s[0])\r\n        elif rotation == 8:  # rotation 90\r\n            s = (s[1], s[0])\r\n    except Exception:\r\n        pass\r\n\r\n    return s\r\n\r\n\r\ndef exif_transpose(image):\r\n    \"\"\"\r\n    Transpose a PIL image accordingly if it has an EXIF Orientation tag.\r\n    Inplace version of https://github.com/python-pillow/Pillow/blob/master/src/PIL/ImageOps.py exif_transpose()\r\n\r\n    :param image: The image to transpose.\r\n    :return: An image.\r\n    \"\"\"\r\n    exif = image.getexif()\r\n    orientation = exif.get(0x0112, 1)  # default 1\r\n    if orientation > 1:\r\n        method = {2: Image.FLIP_LEFT_RIGHT,\r\n                  3: Image.ROTATE_180,\r\n                  4: Image.FLIP_TOP_BOTTOM,\r\n                  5: Image.TRANSPOSE,\r\n                  6: Image.ROTATE_270,\r\n                  7: Image.TRANSVERSE,\r\n                  8: Image.ROTATE_90,\r\n                  }.get(orientation)\r\n        if method is not None:\r\n            image = image.transpose(method)\r\n            del exif[0x0112]\r\n            image.info[\"exif\"] = exif.tobytes()\r\n    return image\r\n\r\n\r\ndef create_dataloader(path, imgsz, batch_size, stride, single_cls=False, hyp=None, augment=False, cache=False, pad=0.0,\r\n                      rect=False, rank=-1, workers=8, image_weights=False, quad=False, prefix='', shuffle=False):\r\n    if rect and shuffle:\r\n        LOGGER.warning('WARNING: --rect is incompatible with DataLoader shuffle, setting shuffle=False')\r\n        shuffle = False\r\n    with torch_distributed_zero_first(rank):  # init dataset *.cache only once if DDP\r\n        dataset = LoadImagesAndLabels(path, imgsz, batch_size,\r\n                                      augment=augment,  # augmentation\r\n                                      hyp=hyp,  # hyperparameters\r\n                                      rect=rect,  # rectangular batches\r\n                                      cache_images=cache,\r\n                                      single_cls=single_cls,\r\n                                      stride=int(stride),\r\n                                      pad=pad,\r\n                                      image_weights=image_weights,\r\n                                      prefix=prefix)\r\n\r\n    batch_size = min(batch_size, len(dataset))\r\n    nd = torch.cuda.device_count()  # number of CUDA devices\r\n    nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])  # number of workers\r\n    sampler = None if rank == -1 else distributed.DistributedSampler(dataset, shuffle=shuffle)\r\n    loader = DataLoader if image_weights else InfiniteDataLoader  # only DataLoader allows for attribute updates\r\n    return loader(dataset,\r\n                  batch_size=batch_size,\r\n                  shuffle=shuffle and sampler is None,\r\n                  num_workers=nw,\r\n                  sampler=sampler,\r\n                  pin_memory=True,\r\n                  collate_fn=LoadImagesAndLabels.collate_fn4 if quad else LoadImagesAndLabels.collate_fn), dataset\r\n\r\n\r\nclass InfiniteDataLoader(dataloader.DataLoader):\r\n    \"\"\" Dataloader that reuses workers\r\n\r\n    Uses same syntax as vanilla DataLoader\r\n    \"\"\"\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))\r\n        self.iterator = super().__iter__()\r\n\r\n    def __len__(self):\r\n        return len(self.batch_sampler.sampler)\r\n\r\n    def __iter__(self):\r\n        for i in range(len(self)):\r\n            yield next(self.iterator)\r\n\r\n\r\nclass _RepeatSampler:\r\n    \"\"\" Sampler that repeats forever\r\n\r\n    Args:\r\n        sampler (Sampler)\r\n    \"\"\"\r\n\r\n    def __init__(self, sampler):\r\n        self.sampler = sampler\r\n\r\n    def __iter__(self):\r\n        while True:\r\n            yield from iter(self.sampler)\r\n\r\n\r\nclass LoadImages:\r\n    # YOLOv5 image/video dataloader, i.e. `python detect.py --source image.jpg/vid.mp4`\r\n    def __init__(self, path, img_size=640, stride=32, auto=True):\r\n        p = str(Path(path).resolve())  # os-agnostic absolute path\r\n        if '*' in p:\r\n            files = sorted(glob.glob(p, recursive=True))  # glob\r\n        elif os.path.isdir(p):\r\n            files = sorted(glob.glob(os.path.join(p, '*.*')))  # dir\r\n        elif os.path.isfile(p):\r\n            files = [p]  # files\r\n        else:\r\n            raise Exception(f'ERROR: {p} does not exist')\r\n\r\n        images = [x for x in files if x.split('.')[-1].lower() in IMG_FORMATS]\r\n        videos = [x for x in files if x.split('.')[-1].lower() in VID_FORMATS]\r\n        ni, nv = len(images), len(videos)\r\n\r\n        self.img_size = img_size\r\n        self.stride = stride\r\n        self.files = images + videos\r\n        self.nf = ni + nv  # number of files\r\n        self.video_flag = [False] * ni + [True] * nv\r\n        self.mode = 'image'\r\n        self.auto = auto\r\n        if any(videos):\r\n            self.new_video(videos[0])  # new video\r\n        else:\r\n            self.cap = None\r\n        assert self.nf > 0, f'No images or videos found in {p}. ' \\\r\n                            f'Supported formats are:\\nimages: {IMG_FORMATS}\\nvideos: {VID_FORMATS}'\r\n\r\n    def __iter__(self):\r\n        self.count = 0\r\n        return self\r\n\r\n    def __next__(self):\r\n        if self.count == self.nf:\r\n            raise StopIteration\r\n        path = self.files[self.count]\r\n\r\n        if self.video_flag[self.count]:\r\n            # Read video\r\n            self.mode = 'video'\r\n            ret_val, img0 = self.cap.read()\r\n            while not ret_val:\r\n                self.count += 1\r\n                self.cap.release()\r\n                if self.count == self.nf:  # last video\r\n                    raise StopIteration\r\n                else:\r\n                    path = self.files[self.count]\r\n                    self.new_video(path)\r\n                    ret_val, img0 = self.cap.read()\r\n\r\n            self.frame += 1\r\n            s = f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}: '\r\n\r\n        else:\r\n            # Read image\r\n            self.count += 1\r\n            img0 = cv2.imread(path)  # BGR\r\n            assert img0 is not None, f'Image Not Found {path}'\r\n            s = f'image {self.count}/{self.nf} {path}: '\r\n\r\n        # Padded resize\r\n        img = letterbox(img0, self.img_size, stride=self.stride, auto=self.auto)[0]\r\n\r\n        # Convert\r\n        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\r\n        img = np.ascontiguousarray(img)\r\n\r\n        return path, img, img0, self.cap\r\n\r\n    def new_video(self, path):\r\n        self.frame = 0\r\n        self.cap = cv2.VideoCapture(path)\r\n        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\r\n\r\n    def __len__(self):\r\n        return self.nf  # number of files\r\n\r\n\r\nclass LoadWebcam:  # for inference\r\n    # YOLOv5 local webcam dataloader, i.e. `python detect.py --source 0`\r\n    def __init__(self, pipe='0', img_size=640, stride=32):\r\n        self.img_size = img_size\r\n        self.stride = stride\r\n        self.pipe = eval(pipe) if pipe.isnumeric() else pipe\r\n        self.cap = cv2.VideoCapture(self.pipe)  # video capture object\r\n        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)  # set buffer size\r\n\r\n    def __iter__(self):\r\n        self.count = -1\r\n        return self\r\n\r\n    def __next__(self):\r\n        self.count += 1\r\n        if cv2.waitKey(1) == ord('q'):  # q to quit\r\n            self.cap.release()\r\n            cv2.destroyAllWindows()\r\n            raise StopIteration\r\n\r\n        # Read frame\r\n        ret_val, img0 = self.cap.read()\r\n        img0 = cv2.flip(img0, 1)  # flip left-right\r\n\r\n        # Print\r\n        assert ret_val, f'Camera Error {self.pipe}'\r\n        img_path = 'webcam.jpg'\r\n        s = f'webcam {self.count}: '\r\n\r\n        # Padded resize\r\n        img = letterbox(img0, self.img_size, stride=self.stride)[0]\r\n\r\n        # Convert\r\n        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\r\n        img = np.ascontiguousarray(img)\r\n\r\n        return img_path, img, img0, self.cap\r\n\r\n    def __len__(self):\r\n        return 0\r\n\r\n\r\nclass LoadStreams:\r\n    # YOLOv5 streamloader, i.e. `python detect.py --source 'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP streams`\r\n    def __init__(self, sources='streams.txt', img_size=640, stride=32, auto=True):\r\n        self.mode = 'stream'\r\n        self.img_size = img_size  #(2160,2600) # img_size\r\n        self.stride = stride\r\n        self.cap = None\r\n        if os.path.isfile(sources):\r\n            with open(sources) as f:\r\n                sources = [x.strip() for x in f.read().strip().splitlines() if len(x.strip())]\r\n        else:\r\n            sources = [sources]\r\n\r\n        n = len(sources)\r\n        self.imgs, self.fps, self.frames, self.threads = [None] * n, [0] * n, [0] * n, [None] * n\r\n        self.sources = [clean_str(x) for x in sources]  # clean source names for later\r\n        self.auto = auto\r\n        for i, s in enumerate(sources):  # index, source\r\n            # Start thread to read frames from video stream\r\n            st = f'{i + 1}/{n}: {s}... '\r\n            if 'youtube.com/' in s or 'youtu.be/' in s:  # if source is YouTube video\r\n                check_requirements(('pafy', 'youtube_dl==2020.12.2'))\r\n                import pafy\r\n                s = pafy.new(s).getbest(preftype=\"mp4\").url  # YouTube URL\r\n            s = eval(s) if s.isnumeric() else s  # i.e. s = '0' local webcam\r\n            self.cap = cv2.VideoCapture(s) ### get  the streams\r\n            assert self.cap.isOpened(), f'{st}Failed to open {s}'\r\n            w = int(self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 2240))  # w = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n            h = int(self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 960))  # h = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n            fps = self.cap.get(cv2.CAP_PROP_FPS)  # warning: may return 0 or nan\r\n            self.frames[i] = max(int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT)), 0) or float('inf')  # infinite stream fallback\r\n            self.fps[i] = max((fps if math.isfinite(fps) else 0) % 100, 0) or 30  # 30 FPS fallback\r\n\r\n            _, self.imgs[i] = self.cap.read()  # guarantee first frame\r\n            self.threads[i] = Thread(target=self.update, args=([i, self.cap, s]), daemon=True)\r\n            LOGGER.info(f\"{st} Success ({self.frames[i]} frames {w}x{h} at {self.fps[i]:.2f} FPS)\")\r\n            self.threads[i].start()\r\n        LOGGER.info('')  # newline\r\n\r\n        # check for common shapes\r\n        s = np.stack([letterbox(x, self.img_size, stride=self.stride, auto=self.auto)[0].shape for x in self.imgs])\r\n        self.rect = np.unique(s, axis=0).shape[0] == 1  # rect inference if all shapes equal\r\n        if not self.rect:\r\n            LOGGER.warning('WARNING: Stream shapes differ. For optimal performance supply similarly-shaped streams.')\r\n\r\n    def update(self, i, cap, stream):\r\n        # Read stream `i` frames in daemon thread\r\n        n, f, read = 0, self.frames[i], 1  # frame number, frame array, inference every 'read' frame\r\n        while cap.isOpened() and n < f:\r\n            n += 1\r\n            # _, self.imgs[index] = cap.read()\r\n            cap.grab()\r\n            if n % read == 0:\r\n                success, im = cap.retrieve()\r\n                if success:\r\n                    self.imgs[i] = im\r\n                else:\r\n                    LOGGER.warning('WARNING: Video stream unresponsive, please check your IP camera connection.')\r\n                    self.imgs[i] = np.zeros_like(self.imgs[i])\r\n                    cap.open(stream)  # re-open stream if signal was lost\r\n            time.sleep(1 / self.fps[i])  # wait time\r\n\r\n    def __iter__(self):\r\n        self.count = -1\r\n        return self\r\n\r\n    def __next__(self):\r\n        self.count += 1\r\n        if not all(x.is_alive() for x in self.threads) or cv2.waitKey(1) == ord('q'):  # q to quit\r\n            cv2.destroyAllWindows()\r\n            raise StopIteration\r\n\r\n        # Letterbox\r\n        img0 = self.imgs.copy()\r\n        img = [letterbox(x, self.img_size, stride=self.stride, auto=self.rect and self.auto)[0] for x in img0]\r\n\r\n        # Stack\r\n        img = np.stack(img, 0)\r\n\r\n        # Convert\r\n        img = img[..., ::-1].transpose((0, 3, 1, 2))  # BGR to RGB, BHWC to BCHW\r\n        img = np.ascontiguousarray(img)\r\n\r\n        return self.sources, img, img0, self.cap\r\n\r\n    def __len__(self):\r\n        return len(self.sources)  # 1E12 frames = 32 streams at 30 FPS for 30 years\r\n\r\n\r\ndef img2label_paths(img_paths):\r\n    # Define label paths as a function of image paths\r\n    sa, sb = os.sep + 'images' + os.sep, os.sep + 'labels' + os.sep  # /images/, /labels/ substrings\r\n    return [sb.join(x.rsplit(sa, 1)).rsplit('.', 1)[0] + '.txt' for x in img_paths]\r\n\r\n\r\nclass LoadImagesAndLabels(Dataset):\r\n    # YOLOv5 train_loader/val_loader, loads images and labels for training and validation\r\n    cache_version = 0.6  # dataset labels *.cache version\r\n\r\n    def __init__(self, path, img_size=640, batch_size=16, augment=False, hyp=None, rect=False, image_weights=False,\r\n                 cache_images=False, single_cls=False, stride=32, pad=0.0, prefix=''):\r\n        self.img_size = img_size\r\n        self.augment = augment\r\n        self.hyp = hyp\r\n        self.image_weights = image_weights\r\n        self.rect = False if image_weights else rect\r\n        self.mosaic = self.augment and not self.rect  # load 4 images at a time into a mosaic (only during training)\r\n        self.mosaic_border = [-img_size // 2, -img_size // 2]\r\n        self.stride = stride\r\n        self.path = path\r\n        self.albumentations = Albumentations() if augment else None\r\n\r\n        try:\r\n            f = []  # image files\r\n            for p in path if isinstance(path, list) else [path]:\r\n                p = Path(p)  # os-agnostic\r\n                if p.is_dir():  # dir\r\n                    f += glob.glob(str(p / '**' / '*.*'), recursive=True)\r\n                    # f = list(p.rglob('*.*'))  # pathlib\r\n                elif p.is_file():  # file\r\n                    with open(p) as t:\r\n                        t = t.read().strip().splitlines()\r\n                        parent = str(p.parent) + os.sep\r\n                        f += [x.replace('./', parent) if x.startswith('./') else x for x in t]  # local to global path\r\n                        # f += [p.parent / x.lstrip(os.sep) for x in t]  # local to global path (pathlib)\r\n                else:\r\n                    raise Exception(f'{prefix}{p} does not exist')\r\n            self.img_files = sorted(x.replace('/', os.sep) for x in f if x.split('.')[-1].lower() in IMG_FORMATS)\r\n            # self.img_files = sorted([x for x in f if x.suffix[1:].lower() in IMG_FORMATS])  # pathlib\r\n            assert self.img_files, f'{prefix}No images found'\r\n        except Exception as e:\r\n            raise Exception(f'{prefix}Error loading data from {path}: {e}\\nSee {HELP_URL}')\r\n\r\n        # Check cache\r\n        self.label_files = img2label_paths(self.img_files)  # labels\r\n        cache_path = (p if p.is_file() else Path(self.label_files[0]).parent).with_suffix('.cache')\r\n        try:\r\n            cache, exists = np.load(cache_path, allow_pickle=True).item(), True  # load dict\r\n            assert cache['version'] == self.cache_version  # same version\r\n            assert cache['hash'] == get_hash(self.label_files + self.img_files)  # same hash\r\n        except Exception:\r\n            cache, exists = self.cache_labels(cache_path, prefix), False  # cache\r\n\r\n        # Display cache\r\n        nf, nm, ne, nc, n = cache.pop('results')  # found, missing, empty, corrupt, total\r\n        if exists:\r\n            d = f\"Scanning '{cache_path}' images and labels... {nf} found, {nm} missing, {ne} empty, {nc} corrupt\"\r\n            tqdm(None, desc=prefix + d, total=n, initial=n)  # display cache results\r\n            if cache['msgs']:\r\n                LOGGER.info('\\n'.join(cache['msgs']))  # display warnings\r\n        assert nf > 0 or not augment, f'{prefix}No labels in {cache_path}. Can not train without labels. See {HELP_URL}'\r\n\r\n        # Read cache\r\n        [cache.pop(k) for k in ('hash', 'version', 'msgs')]  # remove items\r\n        labels, shapes, self.segments = zip(*cache.values())\r\n        self.labels = list(labels)\r\n        self.shapes = np.array(shapes, dtype=np.float64)\r\n        self.img_files = list(cache.keys())  # update\r\n        self.label_files = img2label_paths(cache.keys())  # update\r\n        n = len(shapes)  # number of images\r\n        bi = np.floor(np.arange(n) / batch_size).astype(int)  # batch index\r\n        nb = bi[-1] + 1  # number of batches\r\n        self.batch = bi  # batch index of image\r\n        self.n = n\r\n        self.indices = range(n)\r\n\r\n        # Update labels\r\n        include_class = []  # filter labels to include only these classes (optional)\r\n        include_class_array = np.array(include_class).reshape(1, -1)\r\n        for i, (label, segment) in enumerate(zip(self.labels, self.segments)):\r\n            if include_class:\r\n                j = (label[:, 0:1] == include_class_array).any(1)\r\n                self.labels[i] = label[j]\r\n                if segment:\r\n                    self.segments[i] = segment[j]\r\n            if single_cls:  # single-class training, merge all classes into 0\r\n                self.labels[i][:, 0] = 0\r\n                if segment:\r\n                    self.segments[i][:, 0] = 0\r\n\r\n        # Rectangular Training\r\n        if self.rect:\r\n            # Sort by aspect ratio\r\n            s = self.shapes  # wh\r\n            ar = s[:, 1] / s[:, 0]  # aspect ratio\r\n            irect = ar.argsort()\r\n            self.img_files = [self.img_files[i] for i in irect]\r\n            self.label_files = [self.label_files[i] for i in irect]\r\n            self.labels = [self.labels[i] for i in irect]\r\n            self.shapes = s[irect]  # wh\r\n            ar = ar[irect]\r\n\r\n            # Set training image shapes\r\n            shapes = [[1, 1]] * nb\r\n            for i in range(nb):\r\n                ari = ar[bi == i]\r\n                mini, maxi = ari.min(), ari.max()\r\n                if maxi < 1:\r\n                    shapes[i] = [maxi, 1]\r\n                elif mini > 1:\r\n                    shapes[i] = [1, 1 / mini]\r\n\r\n            self.batch_shapes = np.ceil(np.array(shapes) * img_size / stride + pad).astype(int) * stride\r\n\r\n        # Cache images into RAM/disk for faster training (WARNING: large datasets may exceed system resources)\r\n        self.imgs, self.img_npy = [None] * n, [None] * n\r\n        if cache_images:\r\n            if cache_images == 'disk':\r\n                self.im_cache_dir = Path(Path(self.img_files[0]).parent.as_posix() + '_npy')\r\n                self.img_npy = [self.im_cache_dir / Path(f).with_suffix('.npy').name for f in self.img_files]\r\n                self.im_cache_dir.mkdir(parents=True, exist_ok=True)\r\n            gb = 0  # Gigabytes of cached images\r\n            self.img_hw0, self.img_hw = [None] * n, [None] * n\r\n            results = ThreadPool(NUM_THREADS).imap(self.load_image, range(n))\r\n            pbar = tqdm(enumerate(results), total=n)\r\n            for i, x in pbar:\r\n                if cache_images == 'disk':\r\n                    if not self.img_npy[i].exists():\r\n                        np.save(self.img_npy[i].as_posix(), x[0])\r\n                    gb += self.img_npy[i].stat().st_size\r\n                else:  # 'ram'\r\n                    self.imgs[i], self.img_hw0[i], self.img_hw[i] = x  # im, hw_orig, hw_resized = load_image(self, i)\r\n                    gb += self.imgs[i].nbytes\r\n                pbar.desc = f'{prefix}Caching images ({gb / 1E9:.1f}GB {cache_images})'\r\n            pbar.close()\r\n\r\n    def cache_labels(self, path=Path('./labels.cache'), prefix=''):\r\n        # Cache dataset labels, check images and read shapes\r\n        x = {}  # dict\r\n        nm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # number missing, found, empty, corrupt, messages\r\n        desc = f\"{prefix}Scanning '{path.parent / path.stem}' images and labels...\"\r\n        with Pool(NUM_THREADS) as pool:\r\n            pbar = tqdm(pool.imap(verify_image_label, zip(self.img_files, self.label_files, repeat(prefix))),\r\n                        desc=desc, total=len(self.img_files))\r\n            for im_file, lb, shape, segments, nm_f, nf_f, ne_f, nc_f, msg in pbar:\r\n                nm += nm_f\r\n                nf += nf_f\r\n                ne += ne_f\r\n                nc += nc_f\r\n                if im_file:\r\n                    x[im_file] = [lb, shape, segments]\r\n                if msg:\r\n                    msgs.append(msg)\r\n                pbar.desc = f\"{desc}{nf} found, {nm} missing, {ne} empty, {nc} corrupt\"\r\n\r\n        pbar.close()\r\n        if msgs:\r\n            LOGGER.info('\\n'.join(msgs))\r\n        if nf == 0:\r\n            LOGGER.warning(f'{prefix}WARNING: No labels found in {path}. See {HELP_URL}')\r\n        x['hash'] = get_hash(self.label_files + self.img_files)\r\n        x['results'] = nf, nm, ne, nc, len(self.img_files)\r\n        x['msgs'] = msgs  # warnings\r\n        x['version'] = self.cache_version  # cache version\r\n        try:\r\n            np.save(path, x)  # save cache for next time\r\n            path.with_suffix('.cache.npy').rename(path)  # remove .npy suffix\r\n            LOGGER.info(f'{prefix}New cache created: {path}')\r\n        except Exception as e:\r\n            LOGGER.warning(f'{prefix}WARNING: Cache directory {path.parent} is not writeable: {e}')  # not writeable\r\n        return x\r\n\r\n    def __len__(self):\r\n        return len(self.img_files)\r\n\r\n    # def __iter__(self):\r\n    #     self.count = -1\r\n    #     print('ran dataset iter')\r\n    #     #self.shuffled_vector = np.random.permutation(self.nF) if self.augment else np.arange(self.nF)\r\n    #     return self\r\n\r\n    def __getitem__(self, index):\r\n        index = self.indices[index]  # linear, shuffled, or image_weights\r\n\r\n        hyp = self.hyp\r\n        mosaic = self.mosaic and random.random() < hyp['mosaic']\r\n        if mosaic:\r\n            # Load mosaic\r\n            img, labels = self.load_mosaic(index)\r\n            shapes = None\r\n\r\n            # MixUp augmentation\r\n            if random.random() < hyp['mixup']:\r\n                img, labels = mixup(img, labels, *self.load_mosaic(random.randint(0, self.n - 1)))\r\n\r\n        else:\r\n            # Load image\r\n            img, (h0, w0), (h, w) = self.load_image(index)\r\n\r\n            # Letterbox\r\n            shape = self.batch_shapes[self.batch[index]] if self.rect else self.img_size  # final letterboxed shape\r\n            img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)\r\n            shapes = (h0, w0), ((h / h0, w / w0), pad)  # for COCO mAP rescaling\r\n\r\n            labels = self.labels[index].copy()\r\n            if labels.size:  # normalized xywh to pixel xyxy format\r\n                labels[:, 1:] = xywhn2xyxy(labels[:, 1:], ratio[0] * w, ratio[1] * h, padw=pad[0], padh=pad[1])\r\n\r\n            if self.augment:\r\n                img, labels = random_perspective(img, labels,\r\n                                                 degrees=hyp['degrees'],\r\n                                                 translate=hyp['translate'],\r\n                                                 scale=hyp['scale'],\r\n                                                 shear=hyp['shear'],\r\n                                                 perspective=hyp['perspective'])\r\n\r\n        nl = len(labels)  # number of labels\r\n        if nl:\r\n            labels[:, 1:5] = xyxy2xywhn(labels[:, 1:5], w=img.shape[1], h=img.shape[0], clip=True, eps=1E-3)\r\n\r\n        if self.augment:\r\n            # Albumentations\r\n            img, labels = self.albumentations(img, labels)\r\n            nl = len(labels)  # update after albumentations\r\n\r\n            # HSV color-space\r\n            augment_hsv(img, hgain=hyp['hsv_h'], sgain=hyp['hsv_s'], vgain=hyp['hsv_v'])\r\n\r\n            # Flip up-down\r\n            if random.random() < hyp['flipud']:\r\n                img = np.flipud(img)\r\n                if nl:\r\n                    labels[:, 2] = 1 - labels[:, 2]\r\n\r\n            # Flip left-right\r\n            if random.random() < hyp['fliplr']:\r\n                img = np.fliplr(img)\r\n                if nl:\r\n                    labels[:, 1] = 1 - labels[:, 1]\r\n\r\n            # Cutouts\r\n            # labels = cutout(img, labels, p=0.5)\r\n            # nl = len(labels)  # update after cutout\r\n\r\n        labels_out = torch.zeros((nl, 6))\r\n        if nl:\r\n            labels_out[:, 1:] = torch.from_numpy(labels)\r\n\r\n        # Convert\r\n        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\r\n        img = np.ascontiguousarray(img)\r\n\r\n        return torch.from_numpy(img), labels_out, self.img_files[index], shapes\r\n\r\n    def load_image(self, i):\r\n        # loads 1 image from dataset index 'i', returns (im, original hw, resized hw)\r\n        im = self.imgs[i]\r\n        if im is None:  # not cached in RAM\r\n            npy = self.img_npy[i]\r\n            if npy and npy.exists():  # load npy\r\n                im = np.load(npy)\r\n            else:  # read image\r\n                f = self.img_files[i]\r\n                im = cv2.imread(f)  # BGR\r\n                assert im is not None, f'Image Not Found {f}'\r\n            h0, w0 = im.shape[:2]  # orig hw\r\n            r = self.img_size / max(h0, w0)  # ratio\r\n            if r != 1:  # if sizes are not equal\r\n                im = cv2.resize(im,\r\n                                (int(w0 * r), int(h0 * r)),\r\n                                interpolation=cv2.INTER_LINEAR if (self.augment or r > 1) else cv2.INTER_AREA)\r\n            return im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized\r\n        else:\r\n            return self.imgs[i], self.img_hw0[i], self.img_hw[i]  # im, hw_original, hw_resized\r\n\r\n    def load_mosaic(self, index):\r\n        # YOLOv5 4-mosaic loader. Loads 1 image + 3 random images into a 4-image mosaic\r\n        labels4, segments4 = [], []\r\n        s = self.img_size\r\n        yc, xc = (int(random.uniform(-x, 2 * s + x)) for x in self.mosaic_border)  # mosaic center x, y\r\n        indices = [index] + random.choices(self.indices, k=3)  # 3 additional image indices\r\n        random.shuffle(indices)\r\n        for i, index in enumerate(indices):\r\n            # Load image\r\n            img, _, (h, w) = self.load_image(index)\r\n\r\n            # place img in img4\r\n            if i == 0:  # top left\r\n                img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\r\n                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\r\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\r\n            elif i == 1:  # top right\r\n                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\r\n                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\r\n            elif i == 2:  # bottom left\r\n                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\r\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)\r\n            elif i == 3:  # bottom right\r\n                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\r\n                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\r\n\r\n            img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\r\n            padw = x1a - x1b\r\n            padh = y1a - y1b\r\n\r\n            # Labels\r\n            labels, segments = self.labels[index].copy(), self.segments[index].copy()\r\n            if labels.size:\r\n                labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padw, padh)  # normalized xywh to pixel xyxy format\r\n                segments = [xyn2xy(x, w, h, padw, padh) for x in segments]\r\n            labels4.append(labels)\r\n            segments4.extend(segments)\r\n\r\n        # Concat/clip labels\r\n        labels4 = np.concatenate(labels4, 0)\r\n        for x in (labels4[:, 1:], *segments4):\r\n            np.clip(x, 0, 2 * s, out=x)  # clip when using random_perspective()\r\n        # img4, labels4 = replicate(img4, labels4)  # replicate\r\n\r\n        # Augment\r\n        img4, labels4, segments4 = copy_paste(img4, labels4, segments4, p=self.hyp['copy_paste'])\r\n        img4, labels4 = random_perspective(img4, labels4, segments4,\r\n                                           degrees=self.hyp['degrees'],\r\n                                           translate=self.hyp['translate'],\r\n                                           scale=self.hyp['scale'],\r\n                                           shear=self.hyp['shear'],\r\n                                           perspective=self.hyp['perspective'],\r\n                                           border=self.mosaic_border)  # border to remove\r\n\r\n        return img4, labels4\r\n\r\n    def load_mosaic9(self, index):\r\n        # YOLOv5 9-mosaic loader. Loads 1 image + 8 random images into a 9-image mosaic\r\n        labels9, segments9 = [], []\r\n        s = self.img_size\r\n        indices = [index] + random.choices(self.indices, k=8)  # 8 additional image indices\r\n        random.shuffle(indices)\r\n        hp, wp = -1, -1  # height, width previous\r\n        for i, index in enumerate(indices):\r\n            # Load image\r\n            img, _, (h, w) = self.load_image(index)\r\n\r\n            # place img in img9\r\n            if i == 0:  # center\r\n                img9 = np.full((s * 3, s * 3, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\r\n                h0, w0 = h, w\r\n                c = s, s, s + w, s + h  # xmin, ymin, xmax, ymax (base) coordinates\r\n            elif i == 1:  # top\r\n                c = s, s - h, s + w, s\r\n            elif i == 2:  # top right\r\n                c = s + wp, s - h, s + wp + w, s\r\n            elif i == 3:  # right\r\n                c = s + w0, s, s + w0 + w, s + h\r\n            elif i == 4:  # bottom right\r\n                c = s + w0, s + hp, s + w0 + w, s + hp + h\r\n            elif i == 5:  # bottom\r\n                c = s + w0 - w, s + h0, s + w0, s + h0 + h\r\n            elif i == 6:  # bottom left\r\n                c = s + w0 - wp - w, s + h0, s + w0 - wp, s + h0 + h\r\n            elif i == 7:  # left\r\n                c = s - w, s + h0 - h, s, s + h0\r\n            elif i == 8:  # top left\r\n                c = s - w, s + h0 - hp - h, s, s + h0 - hp\r\n\r\n            padx, pady = c[:2]\r\n            x1, y1, x2, y2 = (max(x, 0) for x in c)  # allocate coords\r\n\r\n            # Labels\r\n            labels, segments = self.labels[index].copy(), self.segments[index].copy()\r\n            if labels.size:\r\n                labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padx, pady)  # normalized xywh to pixel xyxy format\r\n                segments = [xyn2xy(x, w, h, padx, pady) for x in segments]\r\n            labels9.append(labels)\r\n            segments9.extend(segments)\r\n\r\n            # Image\r\n            img9[y1:y2, x1:x2] = img[y1 - pady:, x1 - padx:]  # img9[ymin:ymax, xmin:xmax]\r\n            hp, wp = h, w  # height, width previous\r\n\r\n        # Offset\r\n        yc, xc = (int(random.uniform(0, s)) for _ in self.mosaic_border)  # mosaic center x, y\r\n        img9 = img9[yc:yc + 2 * s, xc:xc + 2 * s]\r\n\r\n        # Concat/clip labels\r\n        labels9 = np.concatenate(labels9, 0)\r\n        labels9[:, [1, 3]] -= xc\r\n        labels9[:, [2, 4]] -= yc\r\n        c = np.array([xc, yc])  # centers\r\n        segments9 = [x - c for x in segments9]\r\n\r\n        for x in (labels9[:, 1:], *segments9):\r\n            np.clip(x, 0, 2 * s, out=x)  # clip when using random_perspective()\r\n        # img9, labels9 = replicate(img9, labels9)  # replicate\r\n\r\n        # Augment\r\n        img9, labels9 = random_perspective(img9, labels9, segments9,\r\n                                           degrees=self.hyp['degrees'],\r\n                                           translate=self.hyp['translate'],\r\n                                           scale=self.hyp['scale'],\r\n                                           shear=self.hyp['shear'],\r\n                                           perspective=self.hyp['perspective'],\r\n                                           border=self.mosaic_border)  # border to remove\r\n\r\n        return img9, labels9\r\n\r\n    @staticmethod\r\n    def collate_fn(batch):\r\n        img, label, path, shapes = zip(*batch)  # transposed\r\n        for i, lb in enumerate(label):\r\n            lb[:, 0] = i  # add target image index for build_targets()\r\n        return torch.stack(img, 0), torch.cat(label, 0), path, shapes\r\n\r\n    @staticmethod\r\n    def collate_fn4(batch):\r\n        img, label, path, shapes = zip(*batch)  # transposed\r\n        n = len(shapes) // 4\r\n        img4, label4, path4, shapes4 = [], [], path[:n], shapes[:n]\r\n\r\n        ho = torch.tensor([[0.0, 0, 0, 1, 0, 0]])\r\n        wo = torch.tensor([[0.0, 0, 1, 0, 0, 0]])\r\n        s = torch.tensor([[1, 1, 0.5, 0.5, 0.5, 0.5]])  # scale\r\n        for i in range(n):  # zidane torch.zeros(16,3,720,1280)  # BCHW\r\n            i *= 4\r\n            if random.random() < 0.5:\r\n                im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2.0, mode='bilinear', align_corners=False)[\r\n                    0].type(img[i].type())\r\n                lb = label[i]\r\n            else:\r\n                im = torch.cat((torch.cat((img[i], img[i + 1]), 1), torch.cat((img[i + 2], img[i + 3]), 1)), 2)\r\n                lb = torch.cat((label[i], label[i + 1] + ho, label[i + 2] + wo, label[i + 3] + ho + wo), 0) * s\r\n            img4.append(im)\r\n            label4.append(lb)\r\n\r\n        for i, lb in enumerate(label4):\r\n            lb[:, 0] = i  # add target image index for build_targets()\r\n\r\n        return torch.stack(img4, 0), torch.cat(label4, 0), path4, shapes4\r\n\r\n\r\n# Ancillary functions --------------------------------------------------------------------------------------------------\r\ndef create_folder(path='./new'):\r\n    # Create folder\r\n    if os.path.exists(path):\r\n        shutil.rmtree(path)  # delete output folder\r\n    os.makedirs(path)  # make new output folder\r\n\r\n\r\ndef flatten_recursive(path=DATASETS_DIR / 'coco128'):\r\n    # Flatten a recursive directory by bringing all files to top level\r\n    new_path = Path(str(path) + '_flat')\r\n    create_folder(new_path)\r\n    for file in tqdm(glob.glob(str(Path(path)) + '/**/*.*', recursive=True)):\r\n        shutil.copyfile(file, new_path / Path(file).name)\r\n\r\n\r\ndef extract_boxes(path=DATASETS_DIR / 'coco128'):  # from utils.datasets import *; extract_boxes()\r\n    # Convert detection dataset into classification dataset, with one directory per class\r\n    path = Path(path)  # images dir\r\n    shutil.rmtree(path / 'classifier') if (path / 'classifier').is_dir() else None  # remove existing\r\n    files = list(path.rglob('*.*'))\r\n    n = len(files)  # number of files\r\n    for im_file in tqdm(files, total=n):\r\n        if im_file.suffix[1:] in IMG_FORMATS:\r\n            # image\r\n            im = cv2.imread(str(im_file))[..., ::-1]  # BGR to RGB\r\n            h, w = im.shape[:2]\r\n\r\n            # labels\r\n            lb_file = Path(img2label_paths([str(im_file)])[0])\r\n            if Path(lb_file).exists():\r\n                with open(lb_file) as f:\r\n                    lb = np.array([x.split() for x in f.read().strip().splitlines()], dtype=np.float32)  # labels\r\n\r\n                for j, x in enumerate(lb):\r\n                    c = int(x[0])  # class\r\n                    f = (path / 'classifier') / f'{c}' / f'{path.stem}_{im_file.stem}_{j}.jpg'  # new filename\r\n                    if not f.parent.is_dir():\r\n                        f.parent.mkdir(parents=True)\r\n\r\n                    b = x[1:] * [w, h, w, h]  # box\r\n                    # b[2:] = b[2:].max()  # rectangle to square\r\n                    b[2:] = b[2:] * 1.2 + 3  # pad\r\n                    b = xywh2xyxy(b.reshape(-1, 4)).ravel().astype(np.int)\r\n\r\n                    b[[0, 2]] = np.clip(b[[0, 2]], 0, w)  # clip boxes outside of image\r\n                    b[[1, 3]] = np.clip(b[[1, 3]], 0, h)\r\n                    assert cv2.imwrite(str(f), im[b[1]:b[3], b[0]:b[2]]), f'box failure in {f}'\r\n\r\n\r\ndef autosplit(path=DATASETS_DIR / 'coco128/images', weights=(0.9, 0.1, 0.0), annotated_only=False):\r\n    \"\"\" Autosplit a dataset into train/val/test splits and save path/autosplit_*.txt files\r\n    Usage: from utils.datasets import *; autosplit()\r\n    Arguments\r\n        path:            Path to images directory\r\n        weights:         Train, val, test weights (list, tuple)\r\n        annotated_only:  Only use images with an annotated txt file\r\n    \"\"\"\r\n    path = Path(path)  # images dir\r\n    files = sorted(x for x in path.rglob('*.*') if x.suffix[1:].lower() in IMG_FORMATS)  # image files only\r\n    n = len(files)  # number of files\r\n    random.seed(0)  # for reproducibility\r\n    indices = random.choices([0, 1, 2], weights=weights, k=n)  # assign each image to a split\r\n\r\n    txt = ['autosplit_train.txt', 'autosplit_val.txt', 'autosplit_test.txt']  # 3 txt files\r\n    [(path.parent / x).unlink(missing_ok=True) for x in txt]  # remove existing\r\n\r\n    print(f'Autosplitting images from {path}' + ', using *.txt labeled images only' * annotated_only)\r\n    for i, img in tqdm(zip(indices, files), total=n):\r\n        if not annotated_only or Path(img2label_paths([str(img)])[0]).exists():  # check label\r\n            with open(path.parent / txt[i], 'a') as f:\r\n                f.write('./' + img.relative_to(path.parent).as_posix() + '\\n')  # add image to txt file\r\n\r\n\r\ndef verify_image_label(args):\r\n    # Verify one image-label pair\r\n    im_file, lb_file, prefix = args\r\n    nm, nf, ne, nc, msg, segments = 0, 0, 0, 0, '', []  # number (missing, found, empty, corrupt), message, segments\r\n    try:\r\n        # verify images\r\n        im = Image.open(im_file)\r\n        im.verify()  # PIL verify\r\n        shape = exif_size(im)  # image size\r\n        assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'\r\n        assert im.format.lower() in IMG_FORMATS, f'invalid image format {im.format}'\r\n        if im.format.lower() in ('jpg', 'jpeg'):\r\n            with open(im_file, 'rb') as f:\r\n                f.seek(-2, 2)\r\n                if f.read() != b'\\xff\\xd9':  # corrupt JPEG\r\n                    ImageOps.exif_transpose(Image.open(im_file)).save(im_file, 'JPEG', subsampling=0, quality=100)\r\n                    msg = f'{prefix}WARNING: {im_file}: corrupt JPEG restored and saved'\r\n\r\n        # verify labels\r\n        if os.path.isfile(lb_file):\r\n            nf = 1  # label found\r\n            with open(lb_file) as f:\r\n                lb = [x.split() for x in f.read().strip().splitlines() if len(x)]\r\n                if any([len(x) > 8 for x in lb]):  # is segment\r\n                    classes = np.array([x[0] for x in lb], dtype=np.float32)\r\n                    segments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in lb]  # (cls, xy1...)\r\n                    lb = np.concatenate((classes.reshape(-1, 1), segments2boxes(segments)), 1)  # (cls, xywh)\r\n                lb = np.array(lb, dtype=np.float32)\r\n            nl = len(lb)\r\n            if nl:\r\n                assert lb.shape[1] == 5, f'labels require 5 columns, {lb.shape[1]} columns detected'\r\n                assert (lb >= 0).all(), f'negative label values {lb[lb < 0]}'\r\n                assert (lb[:, 1:] <= 1).all(), f'non-normalized or out of bounds coordinates {lb[:, 1:][lb[:, 1:] > 1]}'\r\n                _, i = np.unique(lb, axis=0, return_index=True)\r\n                if len(i) < nl:  # duplicate row check\r\n                    lb = lb[i]  # remove duplicates\r\n                    if segments:\r\n                        segments = segments[i]\r\n                    msg = f'{prefix}WARNING: {im_file}: {nl - len(i)} duplicate labels removed'\r\n            else:\r\n                ne = 1  # label empty\r\n                lb = np.zeros((0, 5), dtype=np.float32)\r\n        else:\r\n            nm = 1  # label missing\r\n            lb = np.zeros((0, 5), dtype=np.float32)\r\n        return im_file, lb, shape, segments, nm, nf, ne, nc, msg\r\n    except Exception as e:\r\n        nc = 1\r\n        msg = f'{prefix}WARNING: {im_file}: ignoring corrupt image/label: {e}'\r\n        return [None, None, None, None, nm, nf, ne, nc, msg]\r\n\r\n\r\ndef dataset_stats(path='coco128.yaml', autodownload=False, verbose=False, profile=False, hub=False):\r\n    \"\"\" Return dataset statistics dictionary with images and instances counts per split per class\r\n    To run in parent directory: export PYTHONPATH=\"$PWD/yolov5\"\r\n    Usage1: from utils.datasets import *; dataset_stats('coco128.yaml', autodownload=True)\r\n    Usage2: from utils.datasets import *; dataset_stats('path/to/coco128_with_yaml.zip')\r\n    Arguments\r\n        path:           Path to data.yaml or data.zip (with data.yaml inside data.zip)\r\n        autodownload:   Attempt to download dataset if not found locally\r\n        verbose:        Print stats dictionary\r\n    \"\"\"\r\n\r\n    def round_labels(labels):\r\n        # Update labels to integer class and 6 decimal place floats\r\n        return [[int(c), *(round(x, 4) for x in points)] for c, *points in labels]\r\n\r\n    def unzip(path):\r\n        # Unzip data.zip TODO: CONSTRAINT: path/to/abc.zip MUST unzip to 'path/to/abc/'\r\n        if str(path).endswith('.zip'):  # path is data.zip\r\n            assert Path(path).is_file(), f'Error unzipping {path}, file not found'\r\n            ZipFile(path).extractall(path=path.parent)  # unzip\r\n            dir = path.with_suffix('')  # dataset directory == zip name\r\n            return True, str(dir), next(dir.rglob('*.yaml'))  # zipped, data_dir, yaml_path\r\n        else:  # path is data.yaml\r\n            return False, None, path\r\n\r\n    def hub_ops(f, max_dim=1920):\r\n        # HUB ops for 1 image 'f': resize and save at reduced quality in /dataset-hub for web/app viewing\r\n        f_new = im_dir / Path(f).name  # dataset-hub image filename\r\n        try:  # use PIL\r\n            im = Image.open(f)\r\n            r = max_dim / max(im.height, im.width)  # ratio\r\n            if r < 1.0:  # image too large\r\n                im = im.resize((int(im.width * r), int(im.height * r)))\r\n            im.save(f_new, 'JPEG', quality=75, optimize=True)  # save\r\n        except Exception as e:  # use OpenCV\r\n            print(f'WARNING: HUB ops PIL failure {f}: {e}')\r\n            im = cv2.imread(f)\r\n            im_height, im_width = im.shape[:2]\r\n            r = max_dim / max(im_height, im_width)  # ratio\r\n            if r < 1.0:  # image too large\r\n                im = cv2.resize(im, (int(im_width * r), int(im_height * r)), interpolation=cv2.INTER_AREA)\r\n            cv2.imwrite(str(f_new), im)\r\n\r\n    zipped, data_dir, yaml_path = unzip(Path(path))\r\n    with open(check_yaml(yaml_path), errors='ignore') as f:\r\n        data = yaml.safe_load(f)  # data dict\r\n        if zipped:\r\n            data['path'] = data_dir  # TODO: should this be dir.resolve()?\r\n    check_dataset(data, autodownload)  # download dataset if missing\r\n    hub_dir = Path(data['path'] + ('-hub' if hub else ''))\r\n    stats = {'nc': data['nc'], 'names': data['names']}  # statistics dictionary\r\n    for split in 'train', 'val', 'test':\r\n        if data.get(split) is None:\r\n            stats[split] = None  # i.e. no test set\r\n            continue\r\n        x = []\r\n        dataset = LoadImagesAndLabels(data[split])  # load dataset\r\n        for label in tqdm(dataset.labels, total=dataset.n, desc='Statistics'):\r\n            x.append(np.bincount(label[:, 0].astype(int), minlength=data['nc']))\r\n        x = np.array(x)  # shape(128x80)\r\n        stats[split] = {'instance_stats': {'total': int(x.sum()), 'per_class': x.sum(0).tolist()},\r\n                        'image_stats': {'total': dataset.n, 'unlabelled': int(np.all(x == 0, 1).sum()),\r\n                                        'per_class': (x > 0).sum(0).tolist()},\r\n                        'labels': [{str(Path(k).name): round_labels(v.tolist())} for k, v in\r\n                                   zip(dataset.img_files, dataset.labels)]}\r\n\r\n        if hub:\r\n            im_dir = hub_dir / 'images'\r\n            im_dir.mkdir(parents=True, exist_ok=True)\r\n            for _ in tqdm(ThreadPool(NUM_THREADS).imap(hub_ops, dataset.img_files), total=dataset.n, desc='HUB Ops'):\r\n                pass\r\n\r\n    # Profile\r\n    stats_path = hub_dir / 'stats.json'\r\n    if profile:\r\n        for _ in range(1):\r\n            file = stats_path.with_suffix('.npy')\r\n            t1 = time.time()\r\n            np.save(file, stats)\r\n            t2 = time.time()\r\n            x = np.load(file, allow_pickle=True)\r\n            print(f'stats.npy times: {time.time() - t2:.3f}s read, {t2 - t1:.3f}s write')\r\n\r\n            file = stats_path.with_suffix('.json')\r\n            t1 = time.time()\r\n            with open(file, 'w') as f:\r\n                json.dump(stats, f)  # save stats *.json\r\n            t2 = time.time()\r\n            with open(file) as f:\r\n                x = json.load(f)  # load hyps dict\r\n            print(f'stats.json times: {time.time() - t2:.3f}s read, {t2 - t1:.3f}s write')\r\n\r\n    # Save, print and return\r\n    if hub:\r\n        print(f'Saving {stats_path.resolve()}...')\r\n        with open(stats_path, 'w') as f:\r\n            json.dump(stats, f)  # save stats.json\r\n    if verbose:\r\n        print(json.dumps(stats, indent=2, sort_keys=False))\r\n    return stats\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/utils/datasets.py b/utils/datasets.py
--- a/utils/datasets.py	(revision 38763aecd12af542eecef5d90e69df6fc12d9b25)
+++ b/utils/datasets.py	(date 1708316090131)
@@ -307,8 +307,8 @@
             s = eval(s) if s.isnumeric() else s  # i.e. s = '0' local webcam
             self.cap = cv2.VideoCapture(s) ### get  the streams
             assert self.cap.isOpened(), f'{st}Failed to open {s}'
-            w = int(self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 2240))  # w = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
-            h = int(self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 960))  # h = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
+            w = int(self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 2600))  # w = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
+            h = int(self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1000))  # h = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
             fps = self.cap.get(cv2.CAP_PROP_FPS)  # warning: may return 0 or nan
             self.frames[i] = max(int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT)), 0) or float('inf')  # infinite stream fallback
             self.fps[i] = max((fps if math.isfinite(fps) else 0) % 100, 0) or 30  # 30 FPS fallback
Index: note.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from utils.general import apply_classifier\r\nfrom utils.datasets import LoadStreams, LoadImages\r\n\r\nfunction datasets():\r\n    class LoadStreams(source, img_size=imgsz, stride=stride)\r\n        def __init__(self, sources='streams.txt', img_size=640, stride=32, auto=True):\r\n    return self.sources, img, img0, None\r\n\r\ndataset = LoadStreams(source, img_size=imgsz, stride=stride)\r\n    return self.sources, img, img0, None\r\n\r\nfunction detect():\r\nfor path, img, im0s, vid_cap in dataset:\r\n\r\ndef apply_classifier(x, model, img, im0):\r\n    return x\r\npred = apply_classifier(pred, modelc, img, im0s)\r\n\r\n# Process detections\r\nfor i, det in enumerate(pred):  # detections per image\r\n    if webcam:  # batch_size >= 1\r\n        p, s, im0, frame = path[i], f'{i}: ', im0s[i].copy(), dataset.count\r\n    else:\r\n        p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\r\n\r\n\r\nres = cv2.resize(im0, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\r\ncv2.imshow(str(p), res)  ##### show images\r\n\r\n\r\npred = apply_classifier(pred, modelc, img, im0s)\r\n\r\nfor path, img, im0s, vid_cap in dataset:\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/note.py b/note.py
--- a/note.py	(revision 38763aecd12af542eecef5d90e69df6fc12d9b25)
+++ b/note.py	(date 1708316089975)
@@ -1,6 +1,8 @@
 from utils.general import apply_classifier
 from utils.datasets import LoadStreams, LoadImages
 
+################# 此文件为测试用草稿纸 ############################
+
 function datasets():
     class LoadStreams(source, img_size=imgsz, stride=stride)
         def __init__(self, sources='streams.txt', img_size=640, stride=32, auto=True):
@@ -31,4 +33,3 @@
 pred = apply_classifier(pred, modelc, img, im0s)
 
 for path, img, im0s, vid_cap in dataset:
-
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"Black\">\r\n    <option name=\"sdkName\" value=\"pyqt5-yolov5\" />\r\n  </component>\r\n  <component name=\"ProjectRootManager\" version=\"2\" project-jdk-name=\"PYQT5-YOLOV5\" project-jdk-type=\"Python SDK\" />\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/misc.xml b/.idea/misc.xml
--- a/.idea/misc.xml	(revision 38763aecd12af542eecef5d90e69df6fc12d9b25)
+++ b/.idea/misc.xml	(date 1708316572079)
@@ -1,7 +1,10 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
   <component name="Black">
-    <option name="sdkName" value="pyqt5-yolov5" />
+    <option name="sdkName" value="Python 3.8 (yolo_pyqt)" />
   </component>
   <component name="ProjectRootManager" version="2" project-jdk-name="PYQT5-YOLOV5" project-jdk-type="Python SDK" />
+  <component name="PyCharmProfessionalAdvertiser">
+    <option name="shown" value="true" />
+  </component>
 </project>
\ No newline at end of file
Index: detect.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"Run inference with a YOLOv5 model on images, videos, directories, streams\r\n\r\nUsage:\r\n    $ python path/to/detect.py --source path/to/img.jpg --weights yolov5s.pt --img 640\r\n\"\"\"\r\n\r\nimport argparse\r\nimport sys\r\nimport time\r\nfrom pathlib import Path\r\n\r\nimport cv2\r\nimport torch\r\nimport torch.backends.cudnn as cudnn\r\n\r\nFILE = Path(__file__).absolute()\r\nsys.path.append(FILE.parents[0].as_posix())  # add yolov5/ to path\r\n\r\nfrom models.experimental import attempt_load\r\nfrom utils.datasets import LoadStreams, LoadImages\r\nfrom utils.general import check_img_size, check_requirements, check_imshow, colorstr, non_max_suppression,apply_classifier, \\\r\n    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path #, save_one_box\r\nfrom utils.plots import colors, plot_one_box\r\nfrom utils.torch_utils import select_device, time_sync, load_classifier\r\n\r\n\r\n# @torch.no_grad()\r\ndef run(weights='pt/yolov5s.pt',  # model.pt path(s)\r\n        source='0',  # file/dir/URL/glob, 0 for webcam\r\n        imgsz=640,  # inference size (pixels)\r\n        conf_thres=0.25,  # confidence threshold\r\n        iou_thres=0.45,  # NMS IOU threshold\r\n        max_det=1000,  # maximum detections per image\r\n        device='0',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\r\n        view_img=False,  # show results\r\n        save_txt=False,  # save results to *.txt\r\n        save_conf=False,  # save confidences in --save-txt labels\r\n        save_crop=False,  # save cropped prediction boxes\r\n        nosave=False,  # do not save images/videos\r\n        classes=None,  # filter by class: --class 0, or --class 0 2 3\r\n        agnostic_nms=False,  # class-agnostic NMS\r\n        augment=False,  # augmented inference\r\n        visualize=False,  # visualize features\r\n        update=False,  # update all models\r\n        project='runs/detect',  # save results to project/name\r\n        name='exp',  # save results to project/name\r\n        exist_ok=False,  # existing project/name ok, do not increment\r\n        line_thickness=3,  # bounding box thickness (pixels)\r\n        hide_labels=False,  # hide labels\r\n        hide_conf=False,  # hide confidences\r\n        half=False,  # use FP16 half-precision inference\r\n        ):\r\n    save_img = not nosave and not source.endswith('.txt')  # save inference images\r\n    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\r\n        ('rtsp://', 'rtmp://', 'http://', 'https://'))\r\n\r\n    # Directories\r\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\r\n    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\r\n\r\n    # Initialize\r\n    set_logging()\r\n    device = select_device(device)  #from utils.torch_utils import select_device, time_sync, load_classifier\r\n    half &= device.type != 'cpu'  # half precision only supported on CUDA\r\n\r\n    # Load model\r\n    model = attempt_load(weights, map_location=device)  # load FP32 model\r\n    stride = int(model.stride.max())  # model stride\r\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\r\n    names = model.module.names if hasattr(model, 'module') else model.names  # get class names\r\n    if half:\r\n        model.half()  # to FP16\r\n\r\n    # Second-stage classifier\r\n    classify = False\r\n    if classify:\r\n        modelc = load_classifier(name='resnet50', n=2)  # initialize\r\n        modelc.load_state_dict(torch.load('resnet50.pt', map_location=device)['model']).to(device).eval()\r\n\r\n    # Dataloader\r\n    if webcam:\r\n        view_img = check_imshow()\r\n        cudnn.benchmark = True  # set True to speed up constant image size inference\r\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride)  #### return self.sources, img, img0, None\r\n        print('dataset type', type(dataset))\r\n        bs = len(dataset)  # batch_size\r\n        print('camera batch size', bs)\r\n    else:\r\n        dataset = LoadImages(source, img_size=imgsz, stride=stride)\r\n        bs = 1  # batch_size\r\n    vid_path, vid_writer = [None] * bs, [None] * bs\r\n\r\n    # Run inference  推理\r\n    if device.type != 'cpu':\r\n        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\r\n    t0 = time.time()\r\n    # grab frame\r\n    for path, img, im0s, vid_cap in dataset:\r\n\r\n        cv2.imshow('im0s-0', im0s[0])  ##### show raw images\r\n        cv2.imshow('im0s-1', im0s[1])  ##### show raw images\r\n        print(type(path), type(img), type(im0s), type(vid_cap))\r\n        #### img recode\r\n        img = torch.from_numpy(img).to(device)\r\n        img = img.half() if half else img.float()  # uint8 to fp16/32\r\n        img /= 255.0  # 0 - 255 to 0.0 - 1.0\r\n        if img.ndimension() == 3:\r\n            img = img.unsqueeze(0)\r\n\r\n\r\n\r\n        # Apply NMS\r\n        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\r\n        t2 = time_sync()\r\n\r\n        # Apply Classifier\r\n        if classify:\r\n            pred = apply_classifier(pred, modelc, img, im0s)\r\n\r\n        # Process detections\r\n        for i, det in enumerate(pred):  # detections per image\r\n            if webcam:  # batch_size >= 1\r\n                p, s, im0, frame = path[i], f'{i}: ', im0s[i].copy(), dataset.count\r\n                # cv2.imshow(str(p), im0)  ##### show raw images\r\n            else:\r\n                p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\r\n\r\n            p = Path(p)  # to Path\r\n            save_path = str(save_dir / p.name)  # img.jpg\r\n            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\r\n            s += '%gx%g ' % img.shape[2:]  # print string\r\n            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\r\n            imc = im0.copy() if save_crop else im0  # for save_crop\r\n            if len(det):\r\n                # Rescale boxes from img_size to im0 size\r\n                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\r\n\r\n                # Print results\r\n                for c in det[:, -1].unique():\r\n                    n = (det[:, -1] == c).sum()  # detections per class\r\n                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\r\n\r\n                # Write results\r\n                for *xyxy, conf, cls in reversed(det):\r\n                    if save_txt:  # Write to file\r\n                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\r\n                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\r\n                        with open(txt_path + '.txt', 'a') as f:\r\n                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\r\n\r\n                    if save_img or save_crop or view_img:  # Add bbox to image\r\n                        c = int(cls)  # integer class\r\n                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\r\n                        plot_one_box(xyxy, im0, label=label, color=colors(c, True), line_thickness=line_thickness)\r\n                        if save_crop:\r\n                            print('save_one_box')\r\n                            # save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\r\n\r\n            # Print time (inference + NMS)\r\n            FSP = int(1 / (t2 - t1))\r\n            print(f'{s}Done. ({t2 - t1:.3f}s FSP={FSP})')\r\n            # print(f'FSP={FSP}')\r\n\r\n            # Stream results\r\n            # if view_img:\r\n                # cv2.putText(im0, str(f'FSP={FSP}'), (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0),1)\r\n                # res = cv2.resize(im0,None,fx=1,fy=1,interpolation=cv2.INTER_CUBIC)\r\n                # cv2.imshow(str(p), res)  ##### show images\r\n                # cv2.waitKey(1)  # 1 millisecond\r\n\r\n            # Save results (image with detections)\r\n            if save_img:\r\n                if dataset.mode == 'image':\r\n                    cv2.imwrite(save_path, im0)\r\n                else:  # 'video' or 'stream'\r\n                    if vid_path[i] != save_path:  # new video\r\n                        vid_path[i] = save_path\r\n                        if isinstance(vid_writer[i], cv2.VideoWriter):\r\n                            vid_writer[i].release()  # release previous video writer\r\n                        if vid_cap:  # video\r\n                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\r\n                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n                        else:  # stream\r\n                            fps, w, h = 30, im0.shape[1], im0.shape[0]\r\n                            save_path += '.mp4'\r\n                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\r\n                    vid_writer[i].write(im0)\r\n\r\n    if save_txt or save_img:\r\n        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\r\n        print(f\"Results saved to {save_dir}{s}\")\r\n\r\n    if update:\r\n        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)\r\n\r\n    print(f'Done. ({time.time() - t0:.3f}s)')\r\n\r\n\r\ndef parse_opt():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--weights', nargs='+', type=str, default='pt/connector-best-v4.pt', help='model.pt path(s)')\r\n    parser.add_argument('--source', type=str, default='0', help='file/dir/URL/glob, 0 for webcam')\r\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='inference size (pixels)')\r\n    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\r\n    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\r\n    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\r\n    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\r\n    parser.add_argument('--view-img', action='store_true', help='show results')\r\n    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\r\n    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\r\n    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')\r\n    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\r\n    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\r\n    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\r\n    parser.add_argument('--augment', action='store_true', help='augmented inference')\r\n    parser.add_argument('--visualize', action='store_true', help='visualize features')\r\n    parser.add_argument('--update', action='store_true', help='update all models')\r\n    parser.add_argument('--project', default='runs/detect', help='save results to project/name')\r\n    parser.add_argument('--name', default='exp', help='save results to project/name')\r\n    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\r\n    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\r\n    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\r\n    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\r\n    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\r\n    opt = parser.parse_args()\r\n    return opt\r\n\r\n\r\ndef main(opt):\r\n    print(colorstr('detect: ') + ', '.join(f'{k}={v}' for k, v in vars(opt).items()))\r\n    check_requirements(exclude=('tensorboard', 'thop'))\r\n    run(**vars(opt))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    opt = parse_opt()\r\n    main(opt)\r\n##### rtsp://admin:admin@169.254.13.110
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/detect.py b/detect.py
--- a/detect.py	(revision 38763aecd12af542eecef5d90e69df6fc12d9b25)
+++ b/detect.py	(date 1708316089975)
@@ -97,8 +97,8 @@
     # grab frame
     for path, img, im0s, vid_cap in dataset:
 
-        cv2.imshow('im0s-0', im0s[0])  ##### show raw images
-        cv2.imshow('im0s-1', im0s[1])  ##### show raw images
+        # cv2.imshow('im0s-0', im0s[0])  ##### show raw images
+        # cv2.imshow('im0s-1', im0s[1])  ##### show raw images
         print(type(path), type(img), type(im0s), type(vid_cap))
         #### img recode
         img = torch.from_numpy(img).to(device)
Index: config/setting.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n  \"iou\": 0.3,\r\n  \"conf\": 0.24,\r\n  \"rate\": 5,\r\n  \"check\": 0,\r\n  \"savecheck\": 0,\r\n  \"device\": 0,\r\n  \"port\": 4,\r\n  \"source\": 6\r\n}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/config/setting.json b/config/setting.json
--- a/config/setting.json	(revision 38763aecd12af542eecef5d90e69df6fc12d9b25)
+++ b/config/setting.json	(date 1708316089975)
@@ -1,10 +1,10 @@
 {
-  "iou": 0.3,
-  "conf": 0.24,
-  "rate": 5,
+  "iou": 0.44,
+  "conf": 0.53,
+  "rate": 1,
   "check": 0,
   "savecheck": 0,
   "device": 0,
   "port": 4,
-  "source": 6
+  "source": 0
 }
\ No newline at end of file
