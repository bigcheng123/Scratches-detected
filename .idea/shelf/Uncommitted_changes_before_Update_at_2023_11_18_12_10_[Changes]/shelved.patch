Index: streams.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>0\r\n1
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/streams.txt b/streams.txt
--- a/streams.txt	(revision b62d1b7be59dfe5fcbb2560404b82e004124288f)
+++ b/streams.txt	(date 1700278744557)
@@ -1,2 +1,2 @@
 0
-1
\ No newline at end of file
+1
Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from PyQt5.QtWidgets import QApplication, QMainWindow, QFileDialog, QMenu, QAction\r\nfrom ui_files.main_win import Ui_mainWindow\r\nfrom ui_files.dialog.rtsp_win import Window\r\n\r\nfrom PyQt5.QtCore import Qt, QPoint, QTimer, QThread, pyqtSignal\r\nfrom PyQt5.QtGui import QImage, QPixmap, QPainter, QIcon\r\nfrom pathlib import Path\r\nimport sys\r\nimport json\r\nimport numpy as np\r\nimport torch\r\nimport torch.backends.cudnn as cudnn\r\nimport os\r\nimport time\r\nimport cv2\r\nimport modbus_rtu\r\nimport _thread\r\n\r\nfrom shutil import copy\r\n\r\nfrom models.experimental import attempt_load\r\nfrom utils.datasets import LoadImages, LoadWebcam, LoadStreams\r\nfrom utils.CustomMessageBox import MessageBox\r\nfrom utils.general import check_img_size, check_requirements, check_imshow, colorstr, non_max_suppression, \\\r\n    apply_classifier, scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\r\n# from utils.plots import colors, plot_one_box, plot_one_box_PIL\r\nfrom utils.plots import Annotator, colors, save_one_box,plot_one_box\r\n\r\nfrom utils.torch_utils import select_device,time_sync,load_classifier\r\nfrom utils.capnums import Camera\r\n\r\n## 设置全局变量\r\nmodbus_flag = False\r\n\r\n\r\nclass DetThread(QThread): ###继承 QThread\r\n    send_img_ch0 = pyqtSignal(np.ndarray)  ### CH0 output image\r\n    send_img_ch1 = pyqtSignal(np.ndarray)  ### CH1 output image\r\n    send_img_ch2 = pyqtSignal(np.ndarray)  ### CH1 output image\r\n    send_img_ch3 = pyqtSignal(np.ndarray)  ### CH1 output image\r\n    send_statistic = pyqtSignal(dict)  ###\r\n    # emit：detecting/pause/stop/finished/error msg\r\n    send_msg = pyqtSignal(str)\r\n    send_percent = pyqtSignal(int)\r\n    send_fps = pyqtSignal(str)\r\n\r\n    def __init__(self):\r\n        super(DetThread, self).__init__()\r\n        self.weights = './yolov5s.pt'\r\n        self.current_weight = './yolov5s.pt'\r\n        self.source = '0'\r\n        self.device = '0'\r\n        self.conf_thres = 0.25\r\n        self.iou_thres = 0.45\r\n        self.jump_out = False                   # jump out of the loop\r\n        self.is_continue = False                # continue/pause\r\n        self.percent_length = 1000              # progress bar\r\n        self.rate_check = True                  # Whether to enable delay\r\n        self.rate = 100\r\n        self.save_fold = None  ####'./result'\r\n\r\n    @torch.no_grad()\r\n    def run(self,\r\n            imgsz=1440,  # inference size (pixels)\r\n            max_det=1000,  # maximum detections per image\r\n            # self.source = '0'\r\n            # self.device='0',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\r\n            view_img = False,  # show results\r\n            save_txt=False,  # save results to *.txt\r\n            save_conf=False,  # save confidences in --save-txt labels\r\n            save_crop=False,  # save cropped prediction boxes\r\n            nosave=False,  # do not save images/videos\r\n            classes=None,  # filter by class: --class 0, or --class 0 2 3\r\n            agnostic_nms=False,  # class-agnostic NMS\r\n            augment=False,  # augmented inference\r\n            visualize=False,  # visualize features\r\n            update=False,  # update all models\r\n            project='runs/detect',  # save results to project/name\r\n            name='exp',  # save results to project/name\r\n            exist_ok=False,  # existing project/name ok, do not increment\r\n            line_thickness=3,  # bounding box thickness (pixels)\r\n            hide_labels=False,  # hide labels\r\n            hide_conf=False,  # hide confidences\r\n            half=False,  # use FP16 half-precision inference\r\n            ):\r\n\r\n        save_img = not nosave and not self.source.endswith('.txt')  # save inference images\r\n        webcam = self.source.isnumeric() or self.source.endswith('.txt') or self.source.lower().startswith(\r\n            ('rtsp://', 'rtmp://', 'http://', 'https://'))\r\n\r\n        # Directories\r\n        save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\r\n        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\r\n\r\n        # Initialize\r\n        # try:\r\n        set_logging()\r\n        device = select_device(self.device)  ### from utils.torch_utils import select_device\r\n        half &= device.type != 'cpu'  # half precision only supported on CUDA\r\n\r\n        # Load model\r\n        model = attempt_load(self.weights, map_location=device)  # load FP32 model  from models.experimental import attempt_load\r\n        stride = int(model.stride.max())  # model stride\r\n        imgsz = check_img_size(imgsz, s=stride)  # check image size\r\n        names = model.module.names if hasattr(model, 'module') else model.names  # get class names\r\n        if half:\r\n            model.half()  # to FP16\r\n\r\n        # Second-stage classifier\r\n        classify = False\r\n        if classify:\r\n            modelc = load_classifier(name='resnet50', n=2)  # initialize\r\n            modelc.load_state_dict(torch.load('resnet50.pt', map_location=device)['model']).to(device).eval()\r\n\r\n        # Dataloader\r\n        if webcam: ###self.source.isnumeric() or self.source.endswith('.txt') or\r\n            view_img = check_imshow()\r\n            cudnn.benchmark = True  # set True to speed up constant image size inference\r\n            dataset = LoadStreams(self.source, img_size=imgsz, stride=stride)  #### loadstreams  return self.sources, img, img0, None\r\n            print('dataset type', type(dataset), dataset)\r\n            bs = len(dataset)  # batch_size\r\n            print('len(bs)=', bs)\r\n            # #### streams = LoadStreams\r\n\r\n        else:  ### load the images\r\n            dataset = LoadImages(self.source, img_size=imgsz, stride=stride)\r\n            bs = 1  # batch_size\r\n        vid_path, vid_writer = [None] * bs, [None] * bs\r\n\r\n\r\n        # Run inference 推理\r\n        if device.type != 'cpu':\r\n            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\r\n        start_time = time.time()\r\n        t0 = time.time()\r\n        count = 0\r\n\r\n        # dataset = iter(dataset)  ##迭代器 iter 创建了一个迭代器对象，每次调用这个迭代器对象的__next__()方法时，都会调用 object\r\n\r\n        while True: ##### 采用循环来 检查是否 停止推理\r\n            print('marker while loop')\r\n            print(' while loop self.is_continue', self.is_continue)\r\n            print(' while loop self.jump_out', self.jump_out)\r\n            # print(' while loop camera.cap', type(self.vid_cap))\r\n\r\n            if self.jump_out:\r\n                self.vid_cap.release()  #### bug-2  无法释放摄像头  未解决\r\n                print('vid_cap.release -1', type(self.vid_cap))\r\n                self.send_percent.emit(0)\r\n                self.send_msg.emit('Stop')\r\n                if hasattr(self, 'out'):\r\n                    self.out.release()\r\n                print('jump_out push-1', self.jump_out)\r\n                break\r\n\r\n            # change model & device  20230810\r\n            if self.current_weight != self.weights:\r\n                # Load model\r\n                model = attempt_load(self.weights, map_location = device)  # load FP32 model\r\n                stride = int(model.stride.max())  # model stride\r\n                imgsz = check_img_size(imgsz, s=stride)  # check image size\r\n                names = model.module.names if hasattr(model, 'module') else model.names  # get class names\r\n                if half:\r\n                    model.half()  # to FP16\r\n                # Run inference\r\n                if device.type != 'cpu':\r\n                    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\r\n                self.current_weight = self.weights\r\n\r\n            # load  streams\r\n\r\n            if self.is_continue:\r\n                # ### 使用 loadstreams  dataset = ： self.sources, img, img0, None\r\n                for path, img, im0s, self.vid_cap in dataset: ####  由于dataset在RUN中运行 会不断更新，所以此FOR循环 不会穷尽\r\n                    # print(type(path), type(img), type(im0s), type(self.vid_cap))\r\n                    ### show row image\r\n                    # cv2.imshow('ch0', im0s[0])\r\n                    # cv2.imshow('ch1', im0s[1])\r\n                    #### img recode\r\n                    img = torch.from_numpy(img).to(device)\r\n                    img = img.half() if half else img.float()  # uint8 to fp16/32\r\n                    img /= 255.0  # 0 - 255 to 0.0 - 1.0\r\n                    if img.ndimension() == 3:\r\n                        img = img.unsqueeze(0)\r\n                    statistic_dic = {name: 0 for name in names} ### made the diction\r\n                    # print('statisstic_dic-1',statistic_dic)\r\n                    count += 1  #### FSP counter\r\n                    if  count % 30 == 0 and count >= 30:\r\n                        fps = int(30/(time.time()-start_time))\r\n                        self.send_fps.emit('fps：'+str(fps))\r\n                        start_time = time.time()\r\n                    if self.vid_cap:\r\n                        percent = int(count/self.vid_cap.get(cv2.CAP_PROP_FRAME_COUNT)*self.percent_length)\r\n                        self.send_percent.emit(percent)\r\n                    else:\r\n                        percent = self.percent_length\r\n\r\n                    # Inference\r\n                    t1 = time_sync()\r\n                    # pred = model(img, augment=augment)[0] #### 预测  使用loadWebcam是 加载的model\r\n                    pred = model(img,\r\n                                 augment=augment,\r\n                                 visualize=increment_path(save_dir / Path(path).stem,\r\n                                                          mkdir=True) if visualize else False)[0]\r\n\r\n                    # Apply NMS\r\n                    pred = non_max_suppression(pred, self.conf_thres, self.iou_thres, classes, agnostic_nms, max_det=max_det)\r\n                    t2 = time_sync()\r\n\r\n                    # Apply Classifier\r\n                    if classify:\r\n                        pred = apply_classifier(pred, modelc, img, im0s)\r\n\r\n                    # Process detections\r\n                    for i, det in enumerate(pred):  # detections per image\r\n                        if webcam:  # batch_size >= 1     get the frame\r\n                            p, s, im0, frame = path[i], f'{i}: ', im0s[i].copy(), dataset.count\r\n                            label_chanel = str(i)\r\n                            # print(type(label_chanel),'img chanel=', label_chanel)\r\n                        else: ### image\r\n                            p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\r\n                        p = Path(p)  # to Path\r\n                        # save_path = str(save_dir / p.name)  # img.jpg\r\n                        # txt_path = str(save_dir / 'labels' / p.stem) + (dtxt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  #\r\n                        txt_path = 'result'\r\n                        s += '%gx%g ' % img.shape[2:]  # print string\r\n                        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\r\n                        imc = im0.copy() if save_crop else im0  # for save_crop\r\n                        if len(det):\r\n                            # Rescale boxes from img_size to im0 size\r\n                            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\r\n\r\n                            # Print results\r\n                            for c in det[:, -1].unique():\r\n                                n = (det[:, -1] == c).sum()  # detections per class\r\n                                s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\r\n                            # Write results\r\n                            for *xyxy, conf, cls in reversed(det):\r\n                                if save_txt:  # Write to file\r\n                                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(\r\n                                        -1).tolist()  # normalized xywh\r\n                                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\r\n                                    with open(txt_path + '.txt', 'a') as f:\r\n                                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\r\n\r\n                                if save_img or save_crop or view_img:  # Add bbox to image\r\n                                    c = int(cls)  # integer class\r\n                                    statistic_dic[names[c]] += 1\r\n                                    # print('statisstic_dic-2',statistic_dic)\r\n                                    label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\r\n                                    plot_one_box(xyxy, im0, label=label, color=colors(c, True),\r\n                                                 line_thickness=line_thickness)\r\n                                    if save_crop:\r\n                                        print('save_one_box')\r\n                            # print('detection is running')\r\n\r\n                        FSP = int(1 / (t2 - t1))\r\n                        # print(f'{s}Done. ({t2 - t1:.3f}s FSP={FSP})')\r\n\r\n                        # Stream results   emit frame\r\n                        if self.is_continue: ###### 发送图片必须在  for i, det in enumerate(pred): 循环内\r\n                        # if view_img:\r\n                            cv2.putText(im0, str(f'FSP={FSP}'), (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)\r\n                            res = cv2.resize(im0, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\r\n                            ## chanel-0  ##### show images\r\n                            if label_chanel == '0':\r\n                                self.send_img_ch0.emit(im0)  ### 发送图像\r\n                                # print('seng img : ch0')\r\n                            ## chanel-1\r\n                            if label_chanel == '1':\r\n                                self.send_img_ch1.emit(im0)  ### 发送图像\r\n                                print('seng img : ch1')\r\n                            # chanel-2\r\n                            if label_chanel == '2':\r\n                                self.send_img_ch2.emit(im0)  ### 发送图像\r\n                                print('seng img : ch2')\r\n                            ## chanel-3\r\n                            if label_chanel == '3':\r\n                                self.send_img_ch3.emit(im0)  #### 发送图像\r\n                                print('seng img : ch3')\r\n                            ### ## send the detected result\r\n                            self.send_statistic.emit(statistic_dic)\r\n                            # print('emit statistic_dic', statistic_dic)\r\n                    if self.rate_check:\r\n                        time.sleep(1/self.rate)\r\n                    # im0 = annotator.result()\r\n                    # Write results\r\n                    if self.save_fold:\r\n                        os.makedirs(self.save_fold, exist_ok=True)\r\n                        if self.vid_cap is None:\r\n                            save_path = os.path.join(self.save_fold,\r\n                                                     time.strftime('%Y_%m_%d_%H_%M_%S',\r\n                                                                   time.localtime()) + '.jpg')\r\n                            cv2.imwrite(save_path, im0)\r\n                        else: ### self.vid_cap is cv2capture\r\n                            if count == 1:\r\n                                ori_fps = int(self.vid_cap.get(cv2.CAP_PROP_FPS))\r\n                                if ori_fps == 0:\r\n                                    ori_fps = 25\r\n                                # width = int(self.vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n                                # height = int(self.vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n                                width, height = im0.shape[1], im0.shape[0]\r\n                                save_path = os.path.join(self.save_fold, time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime()) + '.mp4')\r\n                                self.out = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), ori_fps,\r\n                                                           (width, height))\r\n                            self.out.write(im0)\r\n\r\n                    if self.jump_out:\r\n                        print('jump_out push-2', self.jump_out)\r\n                        self.is_continue = False\r\n                        self.vid_cap.release()  #### bug-2  无法释放摄像头  未解决\r\n                        print('self.vid_cap.release-2', type(self.vid_cap))\r\n                        self.send_percent.emit(0)\r\n                        self.send_msg.emit('Stop')\r\n                        if hasattr(self, 'out'):\r\n                            self.out.release()\r\n                            print('self.out.release')\r\n                        break\r\n\r\n                if percent == self.percent_length:\r\n                    print(count)\r\n                    self.send_percent.emit(0)\r\n                    self.send_msg.emit('finished')\r\n                    if hasattr(self, 'out'):\r\n                        self.out.release()\r\n                    break\r\n            else:\r\n                print('is_continue break', self.is_continue)\r\n        #### 生成结果文件夹\r\n        # if save_txt or save_img:\r\n        #     s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\r\n        #     print(f\"Results saved to {save_dir}{s}\")\r\n\r\n        if update:\r\n            strip_optimizer(self.weights)  # update model (to fix SourceChangeWarning)\r\n\r\n        # except Exception as e:\r\n        #     self.send_msg.emit('%s' % e)\r\n\r\n\r\n\r\nclass MainWindow(QMainWindow, Ui_mainWindow):\r\n    def __init__(self, parent=None):\r\n        super(MainWindow, self).__init__(parent)\r\n        self.setupUi(self)\r\n        self.m_flag = False\r\n\r\n        # style 1: window can be stretched\r\n        # self.setWindowFlags(Qt.CustomizeWindowHint | Qt.WindowStaysOnTopHint)\r\n\r\n        # style 2: window can not be stretched\r\n        self.setWindowFlags(Qt.Window | Qt.FramelessWindowHint\r\n                            | Qt.WindowSystemMenuHint | Qt.WindowMinimizeButtonHint | Qt.WindowMaximizeButtonHint)\r\n        # self.setWindowOpacity(0.85)  # Transparency of window\r\n\r\n        self.minButton.clicked.connect(self.showMinimized)\r\n        self.maxButton.clicked.connect(self.max_or_restore)\r\n        # show Maximized window\r\n        self.maxButton.animateClick(10)\r\n        self.closeButton.clicked.connect(self.close)\r\n\r\n        self.qtimer = QTimer(self)\r\n        self.qtimer.setSingleShot(True)\r\n        self.qtimer.timeout.connect(lambda: self.statistic_label.clear())\r\n\r\n        # search models automatically\r\n        self.comboBox.clear()\r\n        self.pt_list = os.listdir('./pt')\r\n        self.pt_list = [file for file in self.pt_list if file.endswith('.pt')]\r\n        self.pt_list.sort(key=lambda x: os.path.getsize('./pt/'+x))\r\n        self.comboBox.clear()\r\n        self.comboBox.addItems(self.pt_list)\r\n\r\n        self.qtimer_search = QTimer(self)\r\n        self.qtimer_search.timeout.connect(lambda: self.search_pt())\r\n        self.qtimer_search.start(2000)\r\n\r\n        # yolov5 thread\r\n        self.det_thread = DetThread()\r\n        self.model_type = self.comboBox.currentText()  ### get model from combobox\r\n        self.device_type = self.comboBox_device.currentText()  ###  get device type from combobox\r\n        self.source_type = self.comboBox_source.currentText()  ###  get device type from combobox\r\n        self.port_type = self.comboBox_port.currentText() ###  get port type from combobox\r\n        self.det_thread.weights = \"./pt/%s\" % self.model_type  # difined\r\n        self.det_thread.device = self.device_type # difined  device\r\n        self.det_thread.source = self.source_type\r\n        self.det_thread.percent_length = self.progressBar.maximum()\r\n        #### the connect funtion transform to  def run_or_continue(self):\r\n        self.det_thread.send_img_ch0.connect(lambda x: self.show_image(x, self.video_label_ch0))\r\n        self.det_thread.send_img_ch1.connect(lambda x: self.show_image(x, self.video_label_ch1))\r\n        self.det_thread.send_img_ch2.connect(lambda x: self.show_image(x, self.video_label_ch2))\r\n        self.det_thread.send_img_ch3.connect(lambda x: self.show_image(x, self.video_label_ch3))\r\n        #### tab-2\r\n        self.det_thread.send_img_ch0.connect(lambda x: self.show_image(x, self.video_label_ch4))\r\n\r\n        self.det_thread.send_statistic.connect(self.show_statistic)\r\n        self.det_thread.send_msg.connect(lambda x: self.show_msg(x))\r\n        self.det_thread.send_percent.connect(lambda x: self.progressBar.setValue(x))\r\n        self.det_thread.send_fps.connect(lambda x: self.fps_label.setText(x))\r\n\r\n        self.fileButton.clicked.connect(self.open_file)\r\n        self.cameraButton.clicked.connect(self.chose_cam)\r\n        self.rtspButton.clicked.connect(self.chose_rtsp)\r\n\r\n        self.runButton.clicked.connect(self.run_or_continue)\r\n        self.runButton_modbus.clicked.connect(self.modbus_on_off)\r\n\r\n        self.stopButton.clicked.connect(self.stop)\r\n\r\n        self.comboBox.currentTextChanged.connect(self.change_model)\r\n        self.comboBox_device.currentTextChanged.connect(self.change_device)\r\n        self.comboBox_source.currentTextChanged.connect(self.change_source)\r\n        self.comboBox_port.currentTextChanged.connect(self.change_port)\r\n\r\n        self.confSpinBox.valueChanged.connect(lambda x: self.change_val(x, 'confSpinBox'))\r\n        self.confSlider.valueChanged.connect(lambda x: self.change_val(x, 'confSlider'))\r\n        self.iouSpinBox.valueChanged.connect(lambda x: self.change_val(x, 'iouSpinBox'))\r\n        self.iouSlider.valueChanged.connect(lambda x: self.change_val(x, 'iouSlider'))\r\n        self.rateSpinBox.valueChanged.connect(lambda x: self.change_val(x, 'rateSpinBox'))\r\n        self.rateSlider.valueChanged.connect(lambda x: self.change_val(x, 'rateSlider'))\r\n\r\n        self.checkBox.clicked.connect(self.checkrate)\r\n        self.saveCheckBox.clicked.connect(self.is_save)\r\n        self.load_setting()  #### loading config\r\n\r\n\r\n    def run_or_continue(self):\r\n        # self.det_thread.source = 'streams.txt'\r\n        # self.det_thread.send_img_ch0.connect(lambda x: self.show_image(x, self.video_label_ch0))\r\n        # self.det_thread.send_img_ch1.connect(lambda x: self.show_image(x, self.video_label_ch1))\r\n        # self.det_thread.send_img_ch2.connect(lambda x: self.show_image(x, self.video_label_ch2))\r\n        # self.det_thread.send_img_ch3.connect(lambda x: self.show_image(x, self.video_label_ch3))\r\n        self.det_thread.jump_out = False\r\n        print('runbutton is check', self.runButton.isChecked())\r\n        if self.runButton.isChecked():\r\n            self.runButton.setText('PAUSE')\r\n            self.saveCheckBox.setEnabled(False)\r\n            self.det_thread.is_continue = True\r\n            if not self.det_thread.isRunning():\r\n                self.det_thread.start()\r\n            device = os.path.basename(self.det_thread.device)  ### only for display\r\n            source = os.path.basename(self.det_thread.source)  ### only for display\r\n            source = str(source) if source.isnumeric() else source  ### only for display\r\n            self.statistic_msg('Detecting >> model：{}，device: {}, source：{}'.\r\n                               format(os.path.basename(self.det_thread.weights),device,\r\n                                      source))\r\n            print('self.det_thread.is_continue', self.det_thread.is_continue)\r\n        else:\r\n            self.det_thread.is_continue = False\r\n            self.runButton.setText('RUN')\r\n            self.statistic_msg('Pause')\r\n            print('self.det_thread.is_continue', self.det_thread.is_continue)\r\n\r\n    def thread_mudbus_run(self):\r\n        global modbus_flag\r\n        modbus_flag = True\r\n        #### hexcode  ######\r\n        IN0_READ = '01 02 00 00 00 01 B9 CA'\r\n        IN1_READ = '01 02 00 01 00 01 E8 0A'\r\n        IN2_READ = '01 02 00 02 00 01 18 0A'\r\n        IN3_READ = '01 02 00 03 00 01 49 CA'\r\n        DO0_ON = '01 05 00 00 FF 00 8C 3A'\r\n        DO0_OFF = '01 05 00 00 00 00 CD CA'\r\n        DO1_ON = '01 05 00 01 FF 00 DD FA'\r\n        DO1_OFF = '01 05 00 01 00 00 9C 0A'\r\n        DO2_ON = '01 05 00 02 FF 00 2D FA'\r\n        DO2_OFF = '01 05 00 02 00 00 6C 0A'\r\n        DO3_ON = '01 05 00 03 FF 00 7C 3A'\r\n        DO3_OFF = '01 05 00 03 00 00 3D CA'\r\n\r\n        DO_ALL_ON = '01 0F 00 00 00 04 01 FF 7E D6'\r\n        DO_ALL_OFF = '01 0F 00 00 00 04 01 00 3E 96' ##OUT1-4  OFF  全部继电器关闭  初始化\r\n\r\n        # self.ret = None\r\n        self.port_type = self.comboBox_port.currentText()\r\n        print(type(self.port_type), self.port_type)\r\n        # try:\r\n        #     self.ser, self.ret, _ = modbus_rtu.openport(port='COM5', baudrate=9600, timeout=5)  # 打开端口\r\n        #     print('self.ret',self.ret)\r\n        # except Exception as e:\r\n        #     self.ret = False\r\n        #     print('open port error', e)\r\n        #     self.statistic_msg(str(e))\r\n        #     self.runButton_modbus.setChecked(False)\r\n\r\n        if self.ret: ### openport sucessfully\r\n            feedback_data = modbus_rtu.writedata(self.ser, DO_ALL_OFF)  ###OUT1-4  OFF  全部继电器关闭  初始化\r\n            self.runButton_modbus.setChecked(True)\r\n            print('thread_mudbus_run modbus_flag = True')\r\n            feedback_list = []\r\n\r\n            while self.runButton_modbus.isChecked() and modbus_flag:\r\n\r\n                feedback_data_IN0 = modbus_rtu.writedata(self.ser, IN0_READ)  #### 检查IN1 触发 返回01 02 01 00 a188\r\n                if feedback_data_IN0:#### 有返回数据\r\n                    text_IN0 = feedback_data_IN0[0:8]  ## 读取8位字符\r\n                    if text_IN0 == '01020101':\r\n                        self.checkBox_10.setChecked(True)\r\n                    else:\r\n                        self.checkBox_10.setChecked(False)\r\n                    print('text_IN0', text_IN0)\r\n                    feedback_list.append(text_IN0)\r\n                    feedback_data = modbus_rtu.writedata(self.ser, DO0_ON)  ###1号继电器打开  运行准备 DO1 =1\r\n                else: #### 无返回数据\r\n                    no_feedback = modbus_rtu.writedata(self.ser, DO2_ON)  ###3号继电器打开   控制器无返回数据 D03 =1\r\n                    print('no_feedback data')\r\n\r\n                feedback_data_IN1 = modbus_rtu.writedata(self.ser, IN1_READ)  #### 检查IN2 触发 返回01 02 01 00 a188\r\n                if feedback_data_IN1:  #### 有返回数据\r\n                    text_IN1 = feedback_data_IN1[0:8]  ## 读取8位字符\r\n                    if text_IN1 == '01020101':\r\n                        self.checkBox_11.setChecked(True)\r\n                    else:\r\n                        self.checkBox_11.setChecked(False)\r\n                    print('text_IN1', text_IN1)\r\n                    feedback_list.append(text_IN1)\r\n                else:  #### 无返回数据\r\n                    no_feedback = modbus_rtu.writedata(self.ser,DO2_ON)  ###3号继电器打开   控制器无返回数据 D03 =1\r\n                    print('no_feedback data')\r\n\r\n                feedback_data_IN2 = modbus_rtu.writedata(self.ser,IN2_READ)  #### 检查IN2 触发 返回01 02 01 00 a188\r\n                if feedback_data_IN2:  #### 有返回数据\r\n                    text_IN2 = feedback_data_IN2[0:8]  ## 读取8位字符\r\n                    if text_IN2 == '01020101':\r\n                        self.checkBox_12.setChecked(True)\r\n                    else:\r\n                        self.checkBox_12.setChecked(False)\r\n                    print('text_IN2', text_IN2)\r\n                    feedback_list.append(text_IN2)\r\n                else:  #### 无返回数据\r\n                    no_feedback = modbus_rtu.writedata(self.ser,DO2_ON)  ###3号继电器打开   控制器无返回数据 D03 =1\r\n                    print('no_feedback data')\r\n\r\n                feedback_data_IN3 = modbus_rtu.writedata(self.ser,IN3_READ)  ####\r\n                if feedback_data_IN3:  #### 有返回数据\r\n                    text_IN3 = feedback_data_IN3[0:8]  ## 读取8位字符\r\n                    if text_IN3 == '01020101':\r\n                        self.checkBox_13.setChecked(True)\r\n                    else:\r\n                        self.checkBox_13.setChecked(False)\r\n                    print('text_IN3', text_IN3)\r\n                    feedback_list.append(text_IN3)\r\n                else:  #### 无返回数据\r\n                    no_feedback = modbus_rtu.writedata(self.ser,DO2_ON)  ###3号继电器打开   控制器无返回数据 D03 =1\r\n                    print('no_feedback data')\r\n\r\n                if len(feedback_list) == 20:\r\n                    feedback_list.clear()\r\n                else:\r\n                    self.statistic_msg(str(feedback_list))\r\n\r\n                #### 同步UI 信号\r\n                intput_box_list = [self.checkBox_10.isChecked(), self.checkBox_11.isChecked(), self.checkBox_12.isChecked(), self.checkBox_13.isChecked()]\r\n                output_box_list =[self.checkBox_2.isChecked()]#,self.checkBox_3.isChecked(),self.checkBox_4.isChecked(),self.checkBox_5.isChecked()]\r\n\r\n                for i , n in enumerate(output_box_list):\r\n                    if n == True:\r\n                        print('scratch detected')\r\n                        feedback_data = modbus_rtu.writedata(self.ser, DO3_ON)  ### OUT4 = 1\r\n                    else:\r\n                        print('scratch has not detected')\r\n                        feedback_data = modbus_rtu.writedata(self.ser, DO3_OFF)  ### OUT4 = 0\r\n\r\n            else:\r\n                modbus_flag = False\r\n                print('modbus shut off')\r\n                shut_coil = modbus_rtu.writedata(self.ser, DO_ALL_OFF)  ###OUT1-4  OFF  全部继电器关闭  初始化\r\n\r\n                self.ser.close()\r\n\r\n\r\n    def modbus_on_off(self):\r\n        global modbus_flag\r\n        # if not modbus_flag:\r\n        if self.runButton_modbus.isChecked():\r\n            print('runButton_modbus.isChecked')\r\n            modbus_flag = True\r\n            print('set  modbus_flag = True')\r\n            try:\r\n                self.ser, self.ret, error = modbus_rtu.openport(self.port_type, 9600, 5)  # 打开端口\r\n            except Exception as e:\r\n                print('openport erro -1', e)\r\n                self.statistic_msg(str(e))\r\n\r\n            if not self.ret:\r\n                self.runButton_modbus.setChecked(False)\r\n                MessageBox(\r\n                    self.closeButton, title='Error', text='Connection Error: '+ str(error), time=2000,\r\n                    auto=True).exec_()\r\n                print('port did not open')\r\n                try:\r\n                    self.ser, self.ret, error = modbus_rtu.openport(self.port_type, 9600, 5)  # 打开端口\r\n                    if self.ret:\r\n                        _thread.start_new_thread(myWin.thread_mudbus_run, ())  #### 启动检测 信号 循环\r\n                except Exception as e:\r\n                    print('openport erro-2', e)\r\n                    self.statistic_msg(str(e))\r\n            else: ### self.ret is  True\r\n                self.runButton_modbus.setChecked(True)\r\n                _thread.start_new_thread(myWin.thread_mudbus_run, ())  #### 启动检测 信号 循环\r\n\r\n        else: #### shut down modbus\r\n            print('runButton_modbus.is unChecked')\r\n            modbus_flag = False\r\n            self.runButton_modbus.setChecked(False)\r\n            print('shut down modbus_flag = False')\r\n\r\n\r\n    def stop(self):\r\n        if not self.det_thread.jump_out:\r\n            self.det_thread.jump_out = True\r\n        self.saveCheckBox.setEnabled(True)\r\n        # self.det_thread.stop()  #### bug-1 加入 停止线程会卡死  未解决\r\n\r\n    def search_pt(self):\r\n        pt_list = os.listdir('./pt')\r\n        pt_list = [file for file in pt_list if file.endswith('.pt')]\r\n        pt_list.sort(key=lambda x: os.path.getsize('./pt/' + x))\r\n\r\n        if pt_list != self.pt_list:\r\n            self.pt_list = pt_list\r\n            self.comboBox.clear()\r\n            self.comboBox.addItems(self.pt_list)\r\n\r\n    def is_save(self):\r\n        if self.saveCheckBox.isChecked():\r\n            self.det_thread.save_fold = './runs'  ### save result as .mp4\r\n        else:\r\n            self.det_thread.save_fold = None\r\n\r\n    def checkrate(self):\r\n        if self.checkBox.isChecked():\r\n            self.det_thread.rate_check = True\r\n        else:\r\n            self.det_thread.rate_check = False\r\n\r\n    def chose_rtsp(self):\r\n        self.rtsp_window = Window()\r\n        config_file = 'config/ip.json'\r\n        if not os.path.exists(config_file):\r\n            ip = \"rtsp://admin:admin888@192.168.1.67:555\"\r\n            new_config = {\"ip\": ip}\r\n            new_json = json.dumps(new_config, ensure_ascii=False, indent=2)\r\n            with open(config_file, 'w', encoding='utf-8') as f:\r\n                f.write(new_json)\r\n        else:\r\n            config = json.load(open(config_file, 'r', encoding='utf-8'))\r\n            ip = config['ip']\r\n        self.rtsp_window.rtspEdit.setText(ip)\r\n        self.rtsp_window.show()\r\n        self.rtsp_window.rtspButton.clicked.connect(lambda: self.load_rtsp(self.rtsp_window.rtspEdit.text()))\r\n\r\n    def load_rtsp(self, ip):\r\n        try:\r\n            self.stop()\r\n            MessageBox(\r\n                self.closeButton, title='Tips', text='Loading rtsp stream', time=1000, auto=True).exec_()\r\n            self.det_thread.source = ip\r\n            new_config = {\"ip\": ip}\r\n            new_json = json.dumps(new_config, ensure_ascii=False, indent=2)\r\n            with open('config/ip.json', 'w', encoding='utf-8') as f:\r\n                f.write(new_json)\r\n            self.statistic_msg('Loading rtsp：{}'.format(ip))\r\n            self.rtsp_window.close()\r\n        except Exception as e:\r\n            self.statistic_msg('%s' % e)\r\n\r\n    def chose_cam(self):\r\n        try:\r\n            self.stop()  #### stop running thread\r\n            MessageBox(\r\n                self.closeButton, title='Enumerate Cameras', text='Loading camera', time=2000, auto=True).exec_()\r\n            # get the number of local cameras\r\n            _, cams = Camera().get_cam_num()\r\n            print('enum_camera:', cams)\r\n            self.statistic_msg('enum camera：{}'.format(cams))\r\n            popMenu = QMenu()\r\n            popMenu.setFixedWidth(self.cameraButton.width())\r\n            popMenu.setStyleSheet('''\r\n                                            QMenu {\r\n                                            font-size: 16px;\r\n                                            font-family: \"Microsoft YaHei UI\";\r\n                                            font-weight: light;\r\n                                            color:white;\r\n                                            padding-left: 5px;\r\n                                            padding-right: 5px;\r\n                                            padding-top: 4px;\r\n                                            padding-bottom: 4px;\r\n                                            border-style: solid;\r\n                                            border-width: 0px;\r\n                                            border-color: rgba(255, 255, 255, 255);\r\n                                            border-radius: 3px;\r\n                                            background-color: rgba(200, 200, 200,50);}\r\n                                            ''')\r\n\r\n            for cam in cams:\r\n                exec(\"action_%s = QAction('%s')\" % (cam, cam))\r\n                exec(\"popMenu.addAction(action_%s)\" % cam)\r\n            x = self.groupBox_5.mapToGlobal(self.cameraButton.pos()).x()\r\n            y = self.groupBox_5.mapToGlobal(self.cameraButton.pos()).y()\r\n            y = y + self.cameraButton.frameGeometry().height()\r\n            pos = QPoint(x, y)\r\n            action = popMenu.exec_(pos)\r\n            if action:\r\n                self.det_thread.source = action.text()  ##### choose source\r\n                self.statistic_msg('Loading camera：{}'.format(action.text()))\r\n        except Exception as e:\r\n            self.statistic_msg('%s' % e)\r\n\r\n    def change_val(self, x, flag):\r\n        if flag == 'confSpinBox':\r\n            self.confSlider.setValue(int(x*100))\r\n        elif flag == 'confSlider':\r\n            self.confSpinBox.setValue(x/100)\r\n            self.det_thread.conf_thres = x/100\r\n        elif flag == 'iouSpinBox':\r\n            self.iouSlider.setValue(int(x*100))\r\n        elif flag == 'iouSlider':\r\n            self.iouSpinBox.setValue(x/100)\r\n            self.det_thread.iou_thres = x/100\r\n        elif flag == 'rateSpinBox':\r\n            self.rateSlider.setValue(x)\r\n        elif flag == 'rateSlider':\r\n            self.rateSpinBox.setValue(x)\r\n            self.det_thread.rate = x * 10\r\n        else:\r\n            pass\r\n\r\n    def statistic_msg(self, msg):\r\n        self.statistic_label.setText(msg)\r\n        print(msg)\r\n        # self.qtimer.start(3000)\r\n\r\n    def show_msg(self, msg):\r\n        self.runButton.setChecked(Qt.Unchecked)\r\n        self.statistic_msg(msg)\r\n        if msg == \"Finished\":\r\n            self.saveCheckBox.setEnabled(True)\r\n\r\n    def change_model(self, x):\r\n        self.model_type = self.comboBox.currentText()  #comboBox\r\n        self.det_thread.weights = \"./pt/%s\" % self.model_type\r\n        self.statistic_msg('Change model to %s' % x)\r\n\r\n    def change_device(self, x):\r\n        self.device_type = self.comboBox_device.currentText()\r\n        self.det_thread.device = self.device_type\r\n        self.statistic_msg('Change device to %s' % x)\r\n\r\n    def change_source(self, x):\r\n        self.source_type = self.comboBox_source.currentText()\r\n        self.det_thread.source = self.source_type\r\n        self.statistic_msg('Change source to %s' % x)\r\n\r\n    def change_port(self, x):\r\n        self.port_type = self.comboBox_port.currentText()\r\n        # self.det_thread.source = self.source_type\r\n        self.statistic_msg('Change port to %s' % x)\r\n\r\n\r\n    def open_file(self):\r\n        config_file = 'config/fold.json'\r\n        # config = json.load(open(config_file, 'r', encoding='utf-8'))\r\n        config = json.load(open(config_file, 'r', encoding='utf-8'))\r\n        open_fold = config['open_fold']\r\n        if not os.path.exists(open_fold):\r\n            open_fold = os.getcwd()\r\n        name, _ = QFileDialog.getOpenFileName(self, 'Video/image', open_fold, \"Pic File(*.mp4 *.mkv *.avi *.flv \"\r\n                                                                          \"*.jpg *.png)\")\r\n        if name:\r\n            self.det_thread.source = name\r\n            self.statistic_msg('Loaded file：{}'.format(os.path.basename(name)))\r\n            config['open_fold'] = os.path.dirname(name)\r\n            config_json = json.dumps(config, ensure_ascii=False, indent=2)\r\n            with open(config_file, 'w', encoding='utf-8') as f:\r\n                f.write(config_json)\r\n            self.stop()\r\n\r\n    def max_or_restore(self):  ### window size control\r\n        if self.maxButton.isChecked():\r\n            self.showMaximized()\r\n        else:\r\n            self.showNormal()\r\n\r\n\r\n    def mousePressEvent(self, event):\r\n        self.m_Position = event.pos()\r\n        if event.button() == Qt.LeftButton:\r\n            if 0 < self.m_Position.x() < self.groupBox.pos().x() + self.groupBox.width() and \\\r\n                    0 < self.m_Position.y() < self.groupBox.pos().y() + self.groupBox.height():\r\n                self.m_flag = True\r\n\r\n    def mouseMoveEvent(self, QMouseEvent):\r\n        if Qt.LeftButton and self.m_flag:\r\n            self.move(QMouseEvent.globalPos() - self.m_Position)\r\n\r\n    def mouseReleaseEvent(self, QMouseEvent):\r\n        self.m_flag = False\r\n\r\n    @staticmethod\r\n    def show_image(img_src, label):  ### input img_src  output to pyqt label\r\n        try:\r\n            ih, iw, _ = img_src.shape\r\n            w = label.geometry().width()\r\n            h = label.geometry().height()\r\n            # keep original aspect ratio\r\n            if iw/w > ih/h:\r\n                scal = w / iw\r\n                nw = w\r\n                nh = int(scal * ih)\r\n                img_src_ = cv2.resize(img_src, (nw, nh))\r\n\r\n            else:\r\n                scal = h / ih\r\n                nw = int(scal * iw)\r\n                nh = h\r\n                img_src_ = cv2.resize(img_src, (nw, nh))\r\n\r\n            frame = cv2.cvtColor(img_src_, cv2.COLOR_BGR2RGB)\r\n            img = QImage(frame.data, frame.shape[1], frame.shape[0], frame.shape[2] * frame.shape[1],\r\n                         QImage.Format_RGB888)\r\n            label.setPixmap(QPixmap.fromImage(img))\r\n\r\n        except Exception as e:\r\n            print(repr(e))\r\n\r\n    def show_statistic(self, statistic_dic):  ### predicttion  output\r\n        import re\r\n        try:\r\n            self.resultWidget.clear()\r\n            statistic_dic = sorted(statistic_dic.items(), key=lambda x: x[1], reverse=True)\r\n            statistic_dic = [i for i in statistic_dic if i[1] > 0] ## append to List  while the value greater than 0\r\n            results = [' '+str(i[0]) + '：' + str(i[1]) for i in statistic_dic]  ### reform the list\r\n            # print('output result:', type(results), results)\r\n            self.resultWidget.addItems(results)\r\n            if len(results) :\r\n                print((len(results)))\r\n                for i , n in enumerate(results):\r\n                    # str = re.sub(\"[\\u4e00-\\u9fa5\\0-9\\,\\。]\", \"\", i)\r\n                    # print('class name = ', n)\r\n                    if i == 0:\r\n                        self.checkBox_2.setChecked(True)\r\n                    # else:\r\n                    #     self.checkBox_2.setChecked(False)\r\n                    if i == 1:\r\n                        self.checkBox_3.setChecked(True)\r\n                    # else:\r\n                    #     self.checkBox_3.setChecked(False)\r\n                    if i == 2:\r\n                        self.checkBox_4.setChecked(True)\r\n                    # else:\r\n                    #     self.checkBox_4.setChecked(False)\r\n                    if i == 3:\r\n                        self.checkBox_5.setChecked(True)\r\n                    # else:\r\n                    #     self.checkBox_5.setChecked(False)\r\n                    # if i == 5:\r\n                    #     self.checkBox_6.setChecked(True)\r\n\r\n                    # self.checkBox_2.setText(str(i))\r\n            else:\r\n                self.checkBox_2.setChecked(False)\r\n                self.checkBox_3.setChecked(False)\r\n                self.checkBox_4.setChecked(False)\r\n                self.checkBox_5.setChecked(False)\r\n                self.checkBox_6.setChecked(False)\r\n                # self.checkBox_2.setText(\"\")\r\n                print(\"result = []\")\r\n\r\n        except Exception as e:\r\n            print(repr(e))\r\n\r\n    def load_setting(self):\r\n        config_file = 'config/setting.json'\r\n\r\n        if not os.path.exists(config_file):\r\n            iou = 0.26\r\n            conf = 0.33\r\n            rate = 10\r\n            check = 0\r\n            savecheck = 0\r\n            device = 0\r\n            port = \"COM3\"\r\n            new_config = {\"iou\": iou,\r\n                          \"conf\": conf,\r\n                          \"rate\": rate,\r\n                          \"check\": check,\r\n                          \"savecheck\": savecheck,\r\n                          \"device\": device,\r\n                          \"port\": port\r\n                          }\r\n            new_json = json.dumps(new_config, ensure_ascii=False, indent=2)\r\n            with open(config_file, 'w', encoding='utf-8') as f:\r\n                f.write(new_json)\r\n        else:\r\n            config = json.load(open(config_file, 'r', encoding='utf-8'))\r\n            print('load config:',type(config), config)\r\n            if len(config) != 8 : ### 参数不足时  补充参数\r\n                iou = 0.26\r\n                conf = 0.33\r\n                rate = 10\r\n                check = 0\r\n                savecheck = 0\r\n                device = 0\r\n                port = \"COM3\"\r\n                source = 0\r\n            else:\r\n                iou = config['iou']\r\n                conf = config['conf']\r\n                rate = config['rate']\r\n                check = config['check']\r\n                savecheck = config['savecheck']\r\n                device = config['device'] ## index number\r\n                port = config['port'] ## index number\r\n                source = config['source']\r\n        ### 依据存储的json文件 更新 ui参数\r\n        self.confSpinBox.setValue(conf)\r\n        self.iouSpinBox.setValue(iou)\r\n        self.rateSpinBox.setValue(rate)\r\n        self.checkBox.setCheckState(check)\r\n        self.det_thread.rate_check = check\r\n        self.saveCheckBox.setCheckState(savecheck)\r\n        self.is_save()\r\n\r\n        self.comboBox_device.setCurrentIndex(device) # 设置当前索引号 \"device\": 0\r\n        self.comboBox_port.setCurrentIndex(port)  # 设置当前索引号 \"port\": \"COM0\"\r\n        self.comboBox_source.setCurrentIndex(source)  # 设置当前索引号 \"port\": \"COM0\"\r\n    def closeEvent(self, event):\r\n        self.det_thread.jump_out = True\r\n        config_path = 'config/setting.json'\r\n        config = dict()\r\n        config['iou'] = self.confSpinBox.value()\r\n        config['conf'] = self.iouSpinBox.value()\r\n        config['rate'] = self.rateSpinBox.value()\r\n        config['check'] = self.checkBox.checkState()  ### Latency funtion\r\n        config['savecheck'] = self.saveCheckBox.checkState() ### Auto Save\r\n        config['device'] = self.comboBox_device.currentIndex() ### 获取当前索引号\r\n        config['port'] = self.comboBox_port.currentIndex()  ### 获取当前索引号\r\n        config['source'] = self.comboBox_source.currentIndex()  ### 获取当前索引号\r\n        ####新增参数 请在此处添加， 运行UI后 点击关闭按钮 后保存为 json文件 地址= ./config/setting.json\r\n        config_json = json.dumps(config, ensure_ascii=False, indent=2)\r\n        with open(config_path, 'w', encoding='utf-8') as f:\r\n            f.write(config_json)\r\n            print('confi_json write')\r\n        MessageBox(\r\n            self.closeButton, title='Tips', text='Program is exiting.', time=2000, auto=True).exec_()\r\n        sys.exit(0)\r\n\r\n    def load_config(self):   ####  初始化 modbus connection\r\n      global winsound_freq, winsound_time, winsound_freq_2, winsound_time_2\r\n      try:\r\n          ### 提取备份数据 当出现断电关机数据丢失时， 将Cahce中 备份文件拷贝出来\r\n          cache_path = os.path.dirname(os.path.realpath(__file__)) + r'\\config'\r\n          to_path = os.path.dirname(os.path.realpath(__file__))  ### root path\r\n\r\n          for root, dirs, files in os.walk(\r\n                  cache_path):  # root 表示当前正在访问的文件夹路径# dirs 表示该文件夹下的子目录名list # files 表示该文件夹下的文件list\r\n              # print('files',files) ####['edgevalue.db.bak', 'edgevalue.db.dat', 'edgevalue.db.dir']\r\n              for i in files:\r\n                  from_path = os.path.join(root, i)  # 合并成一个完整路径\r\n                  # copy(from_path, to_path)  ### 第一个参数 是复制对象， 第二个是 复制到文件夹\r\n                  # print('from_path', from_path)\r\n                  # print('to_path', to_path)\r\n              print('files in config has been coppied sucessfully')\r\n\r\n          # self.ser, self.ret , error = modbus_rtu.openport(self.port_type, 9600, 5)  # 打开端口\r\n\r\n      except Exception as e:\r\n          print('openport erro', e)\r\n          self.statistic_msg(str(e))\r\n\r\n\r\n\r\n\r\n####  for  testing  ↓ ##################################################\r\ndef cvshow_image(img):  ### input img_src  output to pyqt label\r\n    try:\r\n        cv2.imshow('Image', img)\r\n    except Exception as e:\r\n        print(repr(e))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    app = QApplication(sys.argv)\r\n    myWin = MainWindow() #### 实例化\r\n    myWin.show()\r\n    print('prameters load completed')\r\n    myWin.runButton_modbus.setChecked(True)\r\n    myWin.modbus_on_off()### start modbus\r\n    # time.sleep(1)\r\n    # print('thread_mudbus_run start')\r\n    # _thread.start_new_thread(myWin.thread_mudbus_run, ())  #### 启动检测 信号 循环\r\n\r\n\r\n    #### 调试用代码\r\n    # det_thread = DetThread() #### 实例化\r\n    # det_thread.weights = \"pt/yolov5s.pt\"\r\n    # det_thread.device = '0'\r\n    # det_thread.source = 'streams.txt'\r\n    # det_thread.is_continue = True\r\n    # det_thread.start()   ###\r\n    # # ##### connect UI  调试输出到 UI  ↓\r\n    # det_thread.send_img_ch0.connect(lambda x: myWin.show_image(x, myWin.video_label_ch0))\r\n    # det_thread.send_img_ch1.connect(lambda x: myWin.show_image(x, myWin.video_label_ch1))\r\n    # det_thread.send_img_ch2.connect(lambda x: myWin.show_image(x, myWin.video_label_ch2))\r\n    # det_thread.send_img_ch3.connect(lambda x: myWin.show_image(x, myWin.video_label_ch3))\r\n\r\n    # 单独输出 调试模式 ↓\r\n    # det_thread.send_img_ch0.connect(lambda x: cvshow_image(x))\r\n\r\n    # myWin.showMaximized()\r\n    sys.exit(app.exec_())\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	(revision b62d1b7be59dfe5fcbb2560404b82e004124288f)
+++ b/main.py	(date 1700279265987)
@@ -11,6 +11,7 @@
 import torch
 import torch.backends.cudnn as cudnn
 import os
+
 import time
 import cv2
 import modbus_rtu
@@ -61,8 +62,8 @@
 
     @torch.no_grad()
     def run(self,
-            imgsz=1440,  # inference size (pixels)
-            max_det=1000,  # maximum detections per image
+            imgsz=640,  # inference size (pixels)
+            max_det=50,  # maximum detections per image
             # self.source = '0'
             # self.device='0',  # cuda device, i.e. 0 or 0,1,2,3 or cpu
             view_img = False,  # show results
Index: train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># YOLOv5 \uD83D\uDE80 by Ultralytics, GPL-3.0 license\r\n\"\"\"\r\nTrain a YOLOv5 model on a custom dataset.\r\n\r\nModels and datasets download automatically from the latest YOLOv5 release.\r\nModels: https://github.com/ultralytics/yolov5/tree/master/models\r\nDatasets: https://github.com/ultralytics/yolov5/tree/master/data\r\nTutorial: https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data\r\n\r\nUsage:\r\n    $ python path/to/train.py --data coco128.yaml --weights yolov5s.pt --img 640  # from pretrained (RECOMMENDED)\r\n    $ python path/to/train.py --data coco128.yaml --weights '' --cfg yolov5s.yaml --img 640  # from scratch\r\n\"\"\"\r\n\r\nimport argparse\r\nimport math\r\nimport os\r\nimport random\r\nimport sys\r\nimport time\r\nfrom copy import deepcopy\r\nfrom datetime import datetime\r\nfrom pathlib import Path\r\n\r\nimport numpy as np\r\nimport torch\r\nimport torch.distributed as dist\r\nimport torch.nn as nn\r\nimport yaml\r\nfrom torch.cuda import amp\r\nfrom torch.nn.parallel import DistributedDataParallel as DDP\r\nfrom torch.optim import SGD, Adam, AdamW, lr_scheduler\r\nfrom tqdm import tqdm\r\n\r\nFILE = Path(__file__).resolve()\r\nROOT = FILE.parents[0]  # YOLOv5 root directory\r\nif str(ROOT) not in sys.path:\r\n    sys.path.append(str(ROOT))  # add ROOT to PATH\r\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\r\n\r\nimport val  # for end-of-epoch mAP\r\nfrom models.experimental import attempt_load\r\nfrom models.yolo import Model\r\nfrom utils.autoanchor import check_anchors\r\nfrom utils.autobatch import check_train_batch_size\r\nfrom utils.callbacks import Callbacks\r\nfrom utils.datasets import create_dataloader\r\nfrom utils.downloads import attempt_download\r\nfrom utils.general import (LOGGER, check_dataset, check_file, check_git_status, check_img_size, check_requirements,\r\n                           check_suffix, check_yaml, colorstr, get_latest_run, increment_path, init_seeds,\r\n                           intersect_dicts, labels_to_class_weights, labels_to_image_weights, methods, one_cycle,\r\n                           print_args, print_mutation, strip_optimizer)\r\nfrom utils.loggers import Loggers\r\nfrom utils.loggers.wandb.wandb_utils import check_wandb_resume\r\nfrom utils.loss import ComputeLoss\r\nfrom utils.metrics import fitness\r\nfrom utils.plots import plot_evolve, plot_labels\r\nfrom utils.torch_utils import EarlyStopping, ModelEMA, de_parallel, select_device, torch_distributed_zero_first\r\n\r\nLOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\r\nRANK = int(os.getenv('RANK', -1))\r\nWORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\r\n\r\n\r\ndef train(hyp,  # path/to/hyp.yaml or hyp dictionary\r\n          opt,\r\n          device,\r\n          callbacks\r\n          ):\r\n    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze = \\\r\n        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\r\n        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze\r\n\r\n    # Directories\r\n    w = save_dir / 'weights'  # weights dir\r\n    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir\r\n    last, best = w / 'last.pt', w / 'best.pt'\r\n\r\n    # Hyperparameters\r\n    if isinstance(hyp, str):\r\n        with open(hyp, errors='ignore') as f:\r\n            hyp = yaml.safe_load(f)  # load hyps dict\r\n    LOGGER.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))\r\n\r\n    # Save run settings\r\n    if not evolve:\r\n        with open(save_dir / 'hyp.yaml', 'w') as f:\r\n            yaml.safe_dump(hyp, f, sort_keys=False)\r\n        with open(save_dir / 'opt.yaml', 'w') as f:\r\n            yaml.safe_dump(vars(opt), f, sort_keys=False)\r\n\r\n    # Loggers\r\n    data_dict = None\r\n    if RANK in [-1, 0]:\r\n        loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\r\n        if loggers.wandb:\r\n            data_dict = loggers.wandb.data_dict\r\n            if resume:\r\n                weights, epochs, hyp, batch_size = opt.weights, opt.epochs, opt.hyp, opt.batch_size\r\n\r\n        # Register actions\r\n        for k in methods(loggers):\r\n            callbacks.register_action(k, callback=getattr(loggers, k))\r\n\r\n    # Config\r\n    plots = not evolve  # create plots\r\n    cuda = device.type != 'cpu'\r\n    init_seeds(1 + RANK)\r\n    with torch_distributed_zero_first(LOCAL_RANK):\r\n        data_dict = data_dict or check_dataset(data)  # check if None\r\n    train_path, val_path = data_dict['train'], data_dict['val']\r\n    nc = 1 if single_cls else int(data_dict['nc'])  # number of classes\r\n    names = ['item'] if single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names\r\n    assert len(names) == nc, f'{len(names)} names found for nc={nc} dataset in {data}'  # check\r\n    is_coco = isinstance(val_path, str) and val_path.endswith('coco/val2017.txt')  # COCO dataset\r\n\r\n    # Model\r\n    check_suffix(weights, '.pt')  # check weights\r\n    pretrained = weights.endswith('.pt')\r\n    if pretrained:\r\n        with torch_distributed_zero_first(LOCAL_RANK):\r\n            weights = attempt_download(weights)  # download if not found locally\r\n        ckpt = torch.load(weights, map_location='cpu')  # load checkpoint to CPU to avoid CUDA memory leak\r\n        model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\r\n        exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # exclude keys\r\n        csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\r\n        csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect\r\n        model.load_state_dict(csd, strict=False)  # load\r\n        LOGGER.info(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights}')  # report\r\n    else:\r\n        model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\r\n\r\n    # Freeze\r\n    freeze = [f'model.{x}.' for x in (freeze if len(freeze) > 1 else range(freeze[0]))]  # layers to freeze\r\n    for k, v in model.named_parameters():\r\n        v.requires_grad = True  # train all layers\r\n        if any(x in k for x in freeze):\r\n            LOGGER.info(f'freezing {k}')\r\n            v.requires_grad = False\r\n\r\n    # Image size\r\n    gs = max(int(model.stride.max()), 32)  # grid size (max stride)\r\n    imgsz = check_img_size(opt.imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple\r\n\r\n    # Batch size\r\n    if RANK == -1 and batch_size == -1:  # single-GPU only, estimate best batch size\r\n        batch_size = check_train_batch_size(model, imgsz)\r\n        loggers.on_params_update({\"batch_size\": batch_size})\r\n\r\n    # Optimizer\r\n    nbs = 64  # nominal batch size\r\n    accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\r\n    hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\r\n    LOGGER.info(f\"Scaled weight_decay = {hyp['weight_decay']}\")\r\n\r\n    g0, g1, g2 = [], [], []  # optimizer parameter groups\r\n    for v in model.modules():\r\n        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias\r\n            g2.append(v.bias)\r\n        if isinstance(v, nn.BatchNorm2d):  # weight (no decay)\r\n            g0.append(v.weight)\r\n        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)\r\n            g1.append(v.weight)\r\n\r\n    if opt.optimizer == 'Adam':\r\n        optimizer = Adam(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\r\n    elif opt.optimizer == 'AdamW':\r\n        optimizer = AdamW(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\r\n    else:\r\n        optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\r\n\r\n    optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})  # add g1 with weight_decay\r\n    optimizer.add_param_group({'params': g2})  # add g2 (biases)\r\n    LOGGER.info(f\"{colorstr('optimizer:')} {type(optimizer).__name__} with parameter groups \"\r\n                f\"{len(g0)} weight (no decay), {len(g1)} weight, {len(g2)} bias\")\r\n    del g0, g1, g2\r\n\r\n    # Scheduler\r\n    if opt.cos_lr:\r\n        lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']\r\n    else:\r\n        lf = lambda x: (1 - x / epochs) * (1.0 - hyp['lrf']) + hyp['lrf']  # linear\r\n    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)\r\n\r\n    # EMA\r\n    ema = ModelEMA(model) if RANK in [-1, 0] else None\r\n\r\n    # Resume\r\n    start_epoch, best_fitness = 0, 0.0\r\n    if pretrained:\r\n        # Optimizer\r\n        if ckpt['optimizer'] is not None:\r\n            optimizer.load_state_dict(ckpt['optimizer'])\r\n            best_fitness = ckpt['best_fitness']\r\n\r\n        # EMA\r\n        if ema and ckpt.get('ema'):\r\n            ema.ema.load_state_dict(ckpt['ema'].float().state_dict())\r\n            ema.updates = ckpt['updates']\r\n\r\n        # Epochs\r\n        start_epoch = ckpt['epoch'] + 1\r\n        if resume:\r\n            assert start_epoch > 0, f'{weights} training to {epochs} epochs is finished, nothing to resume.'\r\n        if epochs < start_epoch:\r\n            LOGGER.info(f\"{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs.\")\r\n            epochs += ckpt['epoch']  # finetune additional epochs\r\n\r\n        del ckpt, csd\r\n\r\n    # DP mode\r\n    if cuda and RANK == -1 and torch.cuda.device_count() > 1:\r\n        LOGGER.warning('WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n'\r\n                       'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')\r\n        model = torch.nn.DataParallel(model)\r\n\r\n    # SyncBatchNorm\r\n    if opt.sync_bn and cuda and RANK != -1:\r\n        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\r\n        LOGGER.info('Using SyncBatchNorm()')\r\n\r\n    # Trainloader\r\n    train_loader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls,\r\n                                              hyp=hyp, augment=True, cache=None if opt.cache == 'val' else opt.cache,\r\n                                              rect=opt.rect, rank=LOCAL_RANK, workers=workers,\r\n                                              image_weights=opt.image_weights, quad=opt.quad,\r\n                                              prefix=colorstr('train: '), shuffle=True)\r\n    mlc = int(np.concatenate(dataset.labels, 0)[:, 0].max())  # max label class\r\n    nb = len(train_loader)  # number of batches\r\n    assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}'\r\n\r\n    # Process 0\r\n    if RANK in [-1, 0]:\r\n        val_loader = create_dataloader(val_path, imgsz, batch_size // WORLD_SIZE * 2, gs, single_cls,\r\n                                       hyp=hyp, cache=None if noval else opt.cache,\r\n                                       rect=True, rank=-1, workers=workers * 2, pad=0.5,\r\n                                       prefix=colorstr('val: '))[0]\r\n\r\n        if not resume:\r\n            labels = np.concatenate(dataset.labels, 0)\r\n            # c = torch.tensor(labels[:, 0])  # classes\r\n            # cf = torch.bincount(c.long(), minlength=nc) + 1.  # frequency\r\n            # model._initialize_biases(cf.to(device))\r\n            if plots:\r\n                plot_labels(labels, names, save_dir)\r\n\r\n            # Anchors\r\n            if not opt.noautoanchor:\r\n                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\r\n            model.half().float()  # pre-reduce anchor precision\r\n\r\n        callbacks.run('on_pretrain_routine_end')\r\n\r\n    # DDP mode\r\n    if cuda and RANK != -1:\r\n        model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)\r\n\r\n    # Model attributes\r\n    nl = de_parallel(model).model[-1].nl  # number of detection layers (to scale hyps)\r\n    hyp['box'] *= 3 / nl  # scale to layers\r\n    hyp['cls'] *= nc / 80 * 3 / nl  # scale to classes and layers\r\n    hyp['obj'] *= (imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers\r\n    hyp['label_smoothing'] = opt.label_smoothing\r\n    model.nc = nc  # attach number of classes to model\r\n    model.hyp = hyp  # attach hyperparameters to model\r\n    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights\r\n    model.names = names\r\n\r\n    # Start training\r\n    t0 = time.time()\r\n    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # number of warmup iterations, max(3 epochs, 1k iterations)\r\n    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training\r\n    last_opt_step = -1\r\n    maps = np.zeros(nc)  # mAP per class\r\n    results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)\r\n    scheduler.last_epoch = start_epoch - 1  # do not move\r\n    scaler = amp.GradScaler(enabled=cuda)\r\n    stopper = EarlyStopping(patience=opt.patience)\r\n    compute_loss = ComputeLoss(model)  # init loss class\r\n    LOGGER.info(f'Image sizes {imgsz} train, {imgsz} val\\n'\r\n                f'Using {train_loader.num_workers * WORLD_SIZE} dataloader workers\\n'\r\n                f\"Logging results to {colorstr('bold', save_dir)}\\n\"\r\n                f'Starting training for {epochs} epochs...')\r\n    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\r\n        model.train()\r\n\r\n        # Update image weights (optional, single-GPU only)\r\n        if opt.image_weights:\r\n            cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights\r\n            iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights\r\n            dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx\r\n\r\n        # Update mosaic border (optional)\r\n        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\r\n        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\r\n\r\n        mloss = torch.zeros(3, device=device)  # mean losses\r\n        if RANK != -1:\r\n            train_loader.sampler.set_epoch(epoch)\r\n        pbar = enumerate(train_loader)\r\n        LOGGER.info(('\\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))\r\n        if RANK in [-1, 0]:\r\n            pbar = tqdm(pbar, total=nb, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # progress bar\r\n        optimizer.zero_grad()\r\n        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\r\n            ni = i + nb * epoch  # number integrated batches (since train start)\r\n            imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\r\n\r\n            # Warmup\r\n            if ni <= nw:\r\n                xi = [0, nw]  # x interp\r\n                # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\r\n                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\r\n                for j, x in enumerate(optimizer.param_groups):\r\n                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\r\n                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\r\n                    if 'momentum' in x:\r\n                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\r\n\r\n            # Multi-scale\r\n            if opt.multi_scale:\r\n                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\r\n                sf = sz / max(imgs.shape[2:])  # scale factor\r\n                if sf != 1:\r\n                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\r\n                    imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\r\n\r\n            # Forward\r\n            with amp.autocast(enabled=cuda):\r\n                pred = model(imgs)  # forward\r\n                loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\r\n                if RANK != -1:\r\n                    loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode\r\n                if opt.quad:\r\n                    loss *= 4.\r\n\r\n            # Backward\r\n            scaler.scale(loss).backward()\r\n\r\n            # Optimize\r\n            if ni - last_opt_step >= accumulate:\r\n                scaler.step(optimizer)  # optimizer.step\r\n                scaler.update()\r\n                optimizer.zero_grad()\r\n                if ema:\r\n                    ema.update(model)\r\n                last_opt_step = ni\r\n\r\n            # Log\r\n            if RANK in [-1, 0]:\r\n                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\r\n                mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)\r\n                pbar.set_description(('%10s' * 2 + '%10.4g' * 5) % (\r\n                    f'{epoch}/{epochs - 1}', mem, *mloss, targets.shape[0], imgs.shape[-1]))\r\n                callbacks.run('on_train_batch_end', ni, model, imgs, targets, paths, plots, opt.sync_bn)\r\n                if callbacks.stop_training:\r\n                    return\r\n            # end batch ------------------------------------------------------------------------------------------------\r\n\r\n        # Scheduler\r\n        lr = [x['lr'] for x in optimizer.param_groups]  # for loggers\r\n        scheduler.step()\r\n\r\n        if RANK in [-1, 0]:\r\n            # mAP\r\n            callbacks.run('on_train_epoch_end', epoch=epoch)\r\n            ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])\r\n            final_epoch = (epoch + 1 == epochs) or stopper.possible_stop\r\n            if not noval or final_epoch:  # Calculate mAP\r\n                results, maps, _ = val.run(data_dict,\r\n                                           batch_size=batch_size // WORLD_SIZE * 2,\r\n                                           imgsz=imgsz,\r\n                                           model=ema.ema,\r\n                                           single_cls=single_cls,\r\n                                           dataloader=val_loader,\r\n                                           save_dir=save_dir,\r\n                                           plots=False,\r\n                                           callbacks=callbacks,\r\n                                           compute_loss=compute_loss)\r\n\r\n            # Update best mAP\r\n            fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]\r\n            if fi > best_fitness:\r\n                best_fitness = fi\r\n            log_vals = list(mloss) + list(results) + lr\r\n            callbacks.run('on_fit_epoch_end', log_vals, epoch, best_fitness, fi)\r\n\r\n            # Save model\r\n            if (not nosave) or (final_epoch and not evolve):  # if save\r\n                ckpt = {'epoch': epoch,\r\n                        'best_fitness': best_fitness,\r\n                        'model': deepcopy(de_parallel(model)).half(),\r\n                        'ema': deepcopy(ema.ema).half(),\r\n                        'updates': ema.updates,\r\n                        'optimizer': optimizer.state_dict(),\r\n                        'wandb_id': loggers.wandb.wandb_run.id if loggers.wandb else None,\r\n                        'date': datetime.now().isoformat()}\r\n\r\n                # Save last, best and delete\r\n                torch.save(ckpt, last)\r\n                if best_fitness == fi:\r\n                    torch.save(ckpt, best)\r\n                if (epoch > 0) and (opt.save_period > 0) and (epoch % opt.save_period == 0):\r\n                    torch.save(ckpt, w / f'epoch{epoch}.pt')\r\n                del ckpt\r\n                callbacks.run('on_model_save', last, epoch, final_epoch, best_fitness, fi)\r\n\r\n            # Stop Single-GPU\r\n            if RANK == -1 and stopper(epoch=epoch, fitness=fi):\r\n                break\r\n\r\n            # Stop DDP TODO: known issues shttps://github.com/ultralytics/yolov5/pull/4576\r\n            # stop = stopper(epoch=epoch, fitness=fi)\r\n            # if RANK == 0:\r\n            #    dist.broadcast_object_list([stop], 0)  # broadcast 'stop' to all ranks\r\n\r\n        # Stop DPP\r\n        # with torch_distributed_zero_first(RANK):\r\n        # if stop:\r\n        #    break  # must break all DDP ranks\r\n\r\n        # end epoch ----------------------------------------------------------------------------------------------------\r\n    # end training -----------------------------------------------------------------------------------------------------\r\n    if RANK in [-1, 0]:\r\n        LOGGER.info(f'\\n{epoch - start_epoch + 1} epochs completed in {(time.time() - t0) / 3600:.3f} hours.')\r\n        for f in last, best:\r\n            if f.exists():\r\n                strip_optimizer(f)  # strip optimizers\r\n                if f is best:\r\n                    LOGGER.info(f'\\nValidating {f}...')\r\n                    results, _, _ = val.run(data_dict,\r\n                                            batch_size=batch_size // WORLD_SIZE * 2,\r\n                                            imgsz=imgsz,\r\n                                            model=attempt_load(f, device).half(),\r\n                                            iou_thres=0.65 if is_coco else 0.60,  # best pycocotools results at 0.65\r\n                                            single_cls=single_cls,\r\n                                            dataloader=val_loader,\r\n                                            save_dir=save_dir,\r\n                                            save_json=is_coco,\r\n                                            verbose=True,\r\n                                            plots=True,\r\n                                            callbacks=callbacks,\r\n                                            compute_loss=compute_loss)  # val best model with plots\r\n                    if is_coco:\r\n                        callbacks.run('on_fit_epoch_end', list(mloss) + list(results) + lr, epoch, best_fitness, fi)\r\n\r\n        callbacks.run('on_train_end', last, best, plots, epoch, results)\r\n        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}\")\r\n\r\n    torch.cuda.empty_cache()\r\n    return results\r\n\r\n\r\ndef parse_opt(known=False):\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--weights', type=str, default='pt/best.pt', help='initial weights path')\r\n    parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='model.yaml path')\r\n    parser.add_argument('--data', type=str, default='data/connector-new.yaml', help='dataset.yaml path')\r\n    parser.add_argument('--hyp', type=str, default='data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')\r\n    parser.add_argument('--epochs', type=int, default=300)\r\n    parser.add_argument('--batch-size', type=int, default=8, help='total batch size for all GPUs, -1 for autobatch')\r\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')\r\n    parser.add_argument('--rect', action='store_true', help='rectangular training')\r\n    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')\r\n    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\r\n    parser.add_argument('--noval', action='store_true', help='only validate final epoch')\r\n    parser.add_argument('--noautoanchor', action='store_true', help='disable AutoAnchor')\r\n    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')\r\n    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\r\n    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='--cache images in \"ram\" (default) or \"disk\"')\r\n    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')\r\n    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\r\n    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')\r\n    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')\r\n    parser.add_argument('--optimizer', type=str, choices=['SGD', 'Adam', 'AdamW'], default='SGD', help='optimizer')\r\n    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')\r\n    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')\r\n    parser.add_argument('--project', default='runs/train', help='save to project/name')\r\n    parser.add_argument('--name', default='exp', help='save to project/name')\r\n    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\r\n    parser.add_argument('--quad', action='store_true', help='quad dataloader')\r\n    parser.add_argument('--cos-lr', action='store_true', help='cosine LR scheduler')\r\n    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')\r\n    parser.add_argument('--patience', type=int, default=100, help='EarlyStopping patience (epochs without improvement)')\r\n    parser.add_argument('--freeze', nargs='+', type=int, default=[0], help='Freeze layers: backbone=10, first3=0 1 2')\r\n    parser.add_argument('--save-period', type=int, default=-1, help='Save checkpoint every x epochs (disabled if < 1)')\r\n    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')\r\n\r\n    # Weights & Biases arguments\r\n    parser.add_argument('--entity', default=None, help='W&B: Entity')\r\n    parser.add_argument('--upload_dataset', nargs='?', const=True, default=False, help='W&B: Upload data, \"val\" option')\r\n    parser.add_argument('--bbox_interval', type=int, default=-1, help='W&B: Set bounding-box image logging interval')\r\n    parser.add_argument('--artifact_alias', type=str, default='latest', help='W&B: Version of dataset artifact to use')\r\n\r\n    opt = parser.parse_known_args()[0] if known else parser.parse_args()\r\n    return opt\r\n\r\n\r\ndef main(opt, callbacks=Callbacks()):\r\n    # Checks\r\n    if RANK in [-1, 0]:\r\n        print_args(FILE.stem, opt)\r\n        check_git_status()\r\n        check_requirements(exclude=['thop'])\r\n\r\n    # Resume\r\n    if opt.resume and not check_wandb_resume(opt) and not opt.evolve:  # resume an interrupted run\r\n        ckpt = opt.resume if isinstance(opt.resume, str) else get_latest_run()  # specified or most recent path\r\n        assert os.path.isfile(ckpt), 'ERROR: --resume checkpoint does not exist'\r\n        with open(Path(ckpt).parent.parent / 'opt.yaml', errors='ignore') as f:\r\n            opt = argparse.Namespace(**yaml.safe_load(f))  # replace\r\n        opt.cfg, opt.weights, opt.resume = '', ckpt, True  # reinstate\r\n        LOGGER.info(f'Resuming training from {ckpt}')\r\n    else:\r\n        opt.data, opt.cfg, opt.hyp, opt.weights, opt.project = \\\r\n            check_file(opt.data), check_yaml(opt.cfg), check_yaml(opt.hyp), str(opt.weights), str(opt.project)  # checks\r\n        assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'\r\n        if opt.evolve:\r\n            if opt.project == str(ROOT / 'runs/train'):  # if default project name, rename to runs/evolve\r\n                opt.project = str(ROOT / 'runs/evolve')\r\n            opt.exist_ok, opt.resume = opt.resume, False  # pass resume to exist_ok and disable resume\r\n        opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))\r\n\r\n    # DDP mode\r\n    device = select_device(opt.device, batch_size=opt.batch_size)\r\n    if LOCAL_RANK != -1:\r\n        msg = 'is not compatible with YOLOv5 Multi-GPU DDP training'\r\n        assert not opt.image_weights, f'--image-weights {msg}'\r\n        assert not opt.evolve, f'--evolve {msg}'\r\n        assert opt.batch_size != -1, f'AutoBatch with --batch-size -1 {msg}, please pass a valid --batch-size'\r\n        assert opt.batch_size % WORLD_SIZE == 0, f'--batch-size {opt.batch_size} must be multiple of WORLD_SIZE'\r\n        assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'\r\n        torch.cuda.set_device(LOCAL_RANK)\r\n        device = torch.device('cuda', LOCAL_RANK)\r\n        dist.init_process_group(backend=\"nccl\" if dist.is_nccl_available() else \"gloo\")\r\n\r\n    # Train\r\n    if not opt.evolve:\r\n        train(opt.hyp, opt, device, callbacks)\r\n        if WORLD_SIZE > 1 and RANK == 0:\r\n            LOGGER.info('Destroying process group... ')\r\n            dist.destroy_process_group()\r\n\r\n    # Evolve hyperparameters (optional)\r\n    else:\r\n        # Hyperparameter evolution metadata (mutation scale 0-1, lower_limit, upper_limit)\r\n        meta = {'lr0': (1, 1e-5, 1e-1),  # initial learning rate (SGD=1E-2, Adam=1E-3)\r\n                'lrf': (1, 0.01, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)\r\n                'momentum': (0.3, 0.6, 0.98),  # SGD momentum/Adam beta1\r\n                'weight_decay': (1, 0.0, 0.001),  # optimizer weight decay\r\n                'warmup_epochs': (1, 0.0, 5.0),  # warmup epochs (fractions ok)\r\n                'warmup_momentum': (1, 0.0, 0.95),  # warmup initial momentum\r\n                'warmup_bias_lr': (1, 0.0, 0.2),  # warmup initial bias lr\r\n                'box': (1, 0.02, 0.2),  # box loss gain\r\n                'cls': (1, 0.2, 4.0),  # cls loss gain\r\n                'cls_pw': (1, 0.5, 2.0),  # cls BCELoss positive_weight\r\n                'obj': (1, 0.2, 4.0),  # obj loss gain (scale with pixels)\r\n                'obj_pw': (1, 0.5, 2.0),  # obj BCELoss positive_weight\r\n                'iou_t': (0, 0.1, 0.7),  # IoU training threshold\r\n                'anchor_t': (1, 2.0, 8.0),  # anchor-multiple threshold\r\n                'anchors': (2, 2.0, 10.0),  # anchors per output grid (0 to ignore)\r\n                'fl_gamma': (0, 0.0, 2.0),  # focal loss gamma (efficientDet default gamma=1.5)\r\n                'hsv_h': (1, 0.0, 0.1),  # image HSV-Hue augmentation (fraction)\r\n                'hsv_s': (1, 0.0, 0.9),  # image HSV-Saturation augmentation (fraction)\r\n                'hsv_v': (1, 0.0, 0.9),  # image HSV-Value augmentation (fraction)\r\n                'degrees': (1, 0.0, 45.0),  # image rotation (+/- deg)\r\n                'translate': (1, 0.0, 0.9),  # image translation (+/- fraction)\r\n                'scale': (1, 0.0, 0.9),  # image scale (+/- gain)\r\n                'shear': (1, 0.0, 10.0),  # image shear (+/- deg)\r\n                'perspective': (0, 0.0, 0.001),  # image perspective (+/- fraction), range 0-0.001\r\n                'flipud': (1, 0.0, 1.0),  # image flip up-down (probability)\r\n                'fliplr': (0, 0.0, 1.0),  # image flip left-right (probability)\r\n                'mosaic': (1, 0.0, 1.0),  # image mixup (probability)\r\n                'mixup': (1, 0.0, 1.0),  # image mixup (probability)\r\n                'copy_paste': (1, 0.0, 1.0)}  # segment copy-paste (probability)\r\n\r\n        with open(opt.hyp, errors='ignore') as f:\r\n            hyp = yaml.safe_load(f)  # load hyps dict\r\n            if 'anchors' not in hyp:  # anchors commented in hyp.yaml\r\n                hyp['anchors'] = 3\r\n        opt.noval, opt.nosave, save_dir = True, True, Path(opt.save_dir)  # only val/save final epoch\r\n        # ei = [isinstance(x, (int, float)) for x in hyp.values()]  # evolvable indices\r\n        evolve_yaml, evolve_csv = save_dir / 'hyp_evolve.yaml', save_dir / 'evolve.csv'\r\n        if opt.bucket:\r\n            os.system(f'gsutil cp gs://{opt.bucket}/evolve.csv {evolve_csv}')  # download evolve.csv if exists\r\n\r\n        for _ in range(opt.evolve):  # generations to evolve\r\n            if evolve_csv.exists():  # if evolve.csv exists: select best hyps and mutate\r\n                # Select parent(s)\r\n                parent = 'single'  # parent selection method: 'single' or 'weighted'\r\n                x = np.loadtxt(evolve_csv, ndmin=2, delimiter=',', skiprows=1)\r\n                n = min(5, len(x))  # number of previous results to consider\r\n                x = x[np.argsort(-fitness(x))][:n]  # top n mutations\r\n                w = fitness(x) - fitness(x).min() + 1E-6  # weights (sum > 0)\r\n                if parent == 'single' or len(x) == 1:\r\n                    # x = x[random.randint(0, n - 1)]  # random selection\r\n                    x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\r\n                elif parent == 'weighted':\r\n                    x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\r\n\r\n                # Mutate\r\n                mp, s = 0.8, 0.2  # mutation probability, sigma\r\n                npr = np.random\r\n                npr.seed(int(time.time()))\r\n                g = np.array([meta[k][0] for k in hyp.keys()])  # gains 0-1\r\n                ng = len(meta)\r\n                v = np.ones(ng)\r\n                while all(v == 1):  # mutate until a change occurs (prevent duplicates)\r\n                    v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)\r\n                for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\r\n                    hyp[k] = float(x[i + 7] * v[i])  # mutate\r\n\r\n            # Constrain to limits\r\n            for k, v in meta.items():\r\n                hyp[k] = max(hyp[k], v[1])  # lower limit\r\n                hyp[k] = min(hyp[k], v[2])  # upper limit\r\n                hyp[k] = round(hyp[k], 5)  # significant digits\r\n\r\n            # Train mutation\r\n            results = train(hyp.copy(), opt, device, callbacks)\r\n            callbacks = Callbacks()\r\n            # Write mutation results\r\n            print_mutation(results, hyp.copy(), save_dir, opt.bucket)\r\n\r\n        # Plot results\r\n        plot_evolve(evolve_csv)\r\n        LOGGER.info(f'Hyperparameter evolution finished {opt.evolve} generations\\n'\r\n                    f\"Results saved to {colorstr('bold', save_dir)}\\n\"\r\n                    f'Usage example: $ python train.py --hyp {evolve_yaml}')\r\n\r\n\r\ndef run(**kwargs):\r\n    # Usage: import train; train.run(data='coco128.yaml', imgsz=320, weights='yolov5m.pt')\r\n    opt = parse_opt(True)\r\n    for k, v in kwargs.items():\r\n        setattr(opt, k, v)\r\n    main(opt)\r\n    return opt\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    opt = parse_opt()\r\n    main(opt)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/train.py b/train.py
--- a/train.py	(revision b62d1b7be59dfe5fcbb2560404b82e004124288f)
+++ b/train.py	(date 1699254811087)
@@ -453,11 +453,11 @@
 
 def parse_opt(known=False):
     parser = argparse.ArgumentParser()
-    parser.add_argument('--weights', type=str, default='pt/best.pt', help='initial weights path')
+    parser.add_argument('--weights', type=str, default='pt/yolov5s.pt', help='initial weights path')
     parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='model.yaml path')
-    parser.add_argument('--data', type=str, default='data/connector-new.yaml', help='dataset.yaml path')
+    parser.add_argument('--data', type=str, default='data/coco128.yaml', help='dataset.yaml path')
     parser.add_argument('--hyp', type=str, default='data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')
-    parser.add_argument('--epochs', type=int, default=300)
+    parser.add_argument('--epochs', type=int, default=100)
     parser.add_argument('--batch-size', type=int, default=8, help='total batch size for all GPUs, -1 for autobatch')
     parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')
     parser.add_argument('--rect', action='store_true', help='rectangular training')
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"ProjectRootManager\" version=\"2\" project-jdk-name=\"pyqt5-yolov5\" project-jdk-type=\"Python SDK\" />\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/misc.xml b/.idea/misc.xml
--- a/.idea/misc.xml	(revision b62d1b7be59dfe5fcbb2560404b82e004124288f)
+++ b/.idea/misc.xml	(date 1698999268789)
@@ -1,4 +1,7 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
+  <component name="Black">
+    <option name="sdkName" value="pyqt5-yolov5" />
+  </component>
   <component name="ProjectRootManager" version="2" project-jdk-name="pyqt5-yolov5" project-jdk-type="Python SDK" />
 </project>
\ No newline at end of file
Index: data/coco128.yaml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># YOLOv5 \uD83D\uDE80 by Ultralytics, GPL-3.0 license\r\n# COCO128 dataset https://www.kaggle.com/ultralytics/coco128 (first 128 images from COCO train2017) by Ultralytics\r\n# Example usage: python train.py --data coco128.yaml\r\n# parent\r\n# ├── yolov5\r\n# └── datasets\r\n#     └── coco128  ← downloads here\r\n\r\n\r\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\r\npath: E:/code/yolo/scratch-detect/data  # dataset root dir\r\ntrain: images/rubber-new  # train images (relative to 'path') 128 images\r\nval: images/rubber-new  # val images (relative to 'path') 128 images\r\ntest:  # test images (optional)\r\n\r\n# Classes\r\nnc: 4  # number of classes\r\nnames: ['impress','scar','hole','ink']  # class names\r\n\r\n\r\n# Download script/URL (optional)\r\n#download: https://ultralytics.com/assets/coco128.zip\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/data/coco128.yaml b/data/coco128.yaml
--- a/data/coco128.yaml	(revision b62d1b7be59dfe5fcbb2560404b82e004124288f)
+++ b/data/coco128.yaml	(date 1699261686724)
@@ -8,14 +8,14 @@
 
 
 # Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
-path: E:/code/yolo/scratch-detect/data  # dataset root dir
-train: images/rubber-new  # train images (relative to 'path') 128 images
-val: images/rubber-new  # val images (relative to 'path') 128 images
+path: E:/code/scratch-detect/data  # dataset root dir
+train: images/connecter  # train images (relative to 'path') 128 images
+val: images/connecter  # val images (relative to 'path') 128 images
 test:  # test images (optional)
 
 # Classes
-nc: 4  # number of classes
-names: ['impress','scar','hole','ink']  # class names
+nc: 3  # number of classes
+names: ['hole','QA0-128NA','QAG-013']  # class names
 
 
 # Download script/URL (optional)
Index: config/setting.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n  \"iou\": 0.56,\r\n  \"conf\": 0.25,\r\n  \"rate\": 16,\r\n  \"check\": 0,\r\n  \"savecheck\": 0,\r\n  \"device\": 0,\r\n  \"port\": 2,\r\n  \"source\": 1\r\n}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/config/setting.json b/config/setting.json
--- a/config/setting.json	(revision b62d1b7be59dfe5fcbb2560404b82e004124288f)
+++ b/config/setting.json	(date 1700280439619)
@@ -1,10 +1,10 @@
 {
-  "iou": 0.56,
-  "conf": 0.25,
+  "iou": 0.45,
+  "conf": 0.12,
   "rate": 16,
   "check": 0,
   "savecheck": 0,
   "device": 0,
   "port": 2,
-  "source": 1
+  "source": 0
 }
\ No newline at end of file
